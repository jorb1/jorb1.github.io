[
  {
    "objectID": "posts/2025-4-5-robot/index.html",
    "href": "posts/2025-4-5-robot/index.html",
    "title": "Underwater Paleontology, Robotics, and Education",
    "section": "",
    "text": "In the meantime, I’d like to thank the Paleontological Society and the Society of Vertebrate Paleontology for the grant awards that got this project off the ground.\n\n\n\n\n\n\n\n\n\nAnd a thank you to the Webb Schools for providing funding for our students first expedition!\n\n\n\n\n\n\n\n\n\nThe robot and students performed exceptionally! But, more details on that soon…stay tuned."
  },
  {
    "objectID": "posts/2025-4-5-robot/index.html#coming-soon",
    "href": "posts/2025-4-5-robot/index.html#coming-soon",
    "title": "Underwater Paleontology, Robotics, and Education",
    "section": "",
    "text": "In the meantime, I’d like to thank the Paleontological Society and the Society of Vertebrate Paleontology for the grant awards that got this project off the ground.\n\n\n\n\n\n\n\n\n\nAnd a thank you to the Webb Schools for providing funding for our students first expedition!\n\n\n\n\n\n\n\n\n\nThe robot and students performed exceptionally! But, more details on that soon…stay tuned."
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "Image of Fire"
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#background",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#background",
    "title": "Thomas Fire Analysis",
    "section": "Background:",
    "text": "Background:\nThe Thomas Fire, which burned over 280,000 acres in Ventura and Santa Barbara counties in December 2017, was one of California’s largest wildfires at the time. It caused widespread ecological damage, displaced communities, and left lasting environmental impacts.\nIn this analysis, I will find the perimeter of the fire, analyze spatial data of the lasting fire scars through false color images, and visualize the effects that this fire had on air quality in the Santa Barbara area.\nAbout the data\nIn this task I will use historical open-access data about fire perimeters in California. There are several datasets with this information online. The dataset that I found is from data.gov at this link: https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436. It was a particularly useful site, as there were multiple filetypes to choose from.\nThe next dataset I will use is a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. These data were retrieved from the Microsof Planetary Computer data catalogue and pre-processed to remove data outside land and coarsen the spatial resolution. (This data should be used for visualization and educational purposes only.)\nFinally, I will use Air Quality Index (AQI) data from the US Environmental Protection Agency to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County.\n\nFirst up in my analysis:\nFire perimeter data retrieval and selection\nI will find and isolate the perimeter of the Thomas Fire, using open source data. I will then be able to use the Thomas Fire perimeter data in further analysis of the effects of the fire on Santa Barbara ecology. I will save this perimeter data as a file in my repository, for independent use in other notebooks.\nTo begin, I will do some exploratory data analysis to get a sense of the dataset I am using. I will ensure that I know the CRS of the data, for use in further spatial data joining and analysis.\n\n# Load libraries\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rioxarray as rioxr\n\n# Read in data \n\nfp_perimeter = os.path.join('data', 'California_Fire_Perimeters_(all).shp')\nperimeter = gpd.read_file(fp_perimeter)\n\n\nperimeter.head(3)\n\n\n\n\n\n\n\n\nYEAR_\nSTATE\nAGENCY\nUNIT_ID\nFIRE_NAME\nINC_NUM\nALARM_DATE\nCONT_DATE\nCAUSE\nC_METHOD\nOBJECTIVE\nGIS_ACRES\nCOMMENTS\nCOMPLEX_NA\nIRWINID\nFIRE_NUM\nCOMPLEX_ID\nDECADES\ngeometry\n\n\n\n\n0\n2023\nCA\nCDF\nSKU\nWHITWORTH\n00004808\n2023-06-17\n2023-06-17\n5\n1\n1\n5.72913\nNone\nNone\n{7985848C-0AC2-4BA4-8F0E-29F778652E61}\nNone\nNone\n2020\nPOLYGON ((-13682443.000 5091132.739, -13682445...\n\n\n1\n2023\nCA\nLRA\nBTU\nKAISER\n00010225\n2023-06-02\n2023-06-02\n5\n1\n1\n13.60240\nNone\nNone\n{43EBCC88-B3AC-48EB-8EF5-417FE0939CCF}\nNone\nNone\n2020\nPOLYGON ((-13576727.142 4841226.161, -13576726...\n\n\n2\n2023\nCA\nCDF\nAEU\nJACKSON\n00017640\n2023-07-01\n2023-07-02\n2\n1\n1\n27.81450\nNone\nNone\n{B64E1355-BF1D-441A-95D0-BC1FBB93483B}\nNone\nNone\n2020\nPOLYGON ((-13459243.000 4621236.000, -13458968...\n\n\n\n\n\n\n\n\n# Figure out the dimensions of the dataframe\nprint(\"Shape of the data:\", perimeter.shape)\n\n# Figure out if the columns are the expected datatypes\nprint(\"Data types:\", perimeter.dtypes)\n\nShape of the data: (22261, 19)\nData types: YEAR_            int64\nSTATE           object\nAGENCY          object\nUNIT_ID         object\nFIRE_NAME       object\nINC_NUM         object\nALARM_DATE      object\nCONT_DATE       object\nCAUSE            int64\nC_METHOD         int64\nOBJECTIVE        int64\nGIS_ACRES      float64\nCOMMENTS        object\nCOMPLEX_NA      object\nIRWINID         object\nFIRE_NUM        object\nCOMPLEX_ID      object\nDECADES          int64\ngeometry      geometry\ndtype: object\n\n\n\n# Explore data CRS\nperimeter.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# Find out if the CRS is projected\nperimeter.crs.is_projected\n\nTrue\n\n\nFrom this data exploration, I learned that the dataset is much larger than I need, but does contain useful information in addition to the geometries, such as acres burned, cause, etc. I learned that the names of the fires are in all capitals, and that the year numbers are int64, so I can treat them as numeric values. Finally, I leaned that the CRS is WGS 84, and that is is projected data, rather than geographic.\nFrom this fire perimeter data, I will select the Thomas Fire boundary. The fire occurred in 2017.\n\n# Make the column names lower case\nperimeter.columns = perimeter.columns.str.lower()\n\n# Filter data to only include the Thomas Fire boudnary in 2017\nthomas = perimeter[(perimeter['fire_name'] == \"THOMAS\") & (perimeter['year_'] == 2017)]\n\nthomas\n\n\n\n\n\n\n\n\nyear_\nstate\nagency\nunit_id\nfire_name\ninc_num\nalarm_date\ncont_date\ncause\nc_method\nobjective\ngis_acres\ncomments\ncomplex_na\nirwinid\nfire_num\ncomplex_id\ndecades\ngeometry\n\n\n\n\n2654\n2017\nCA\nUSF\nVNC\nTHOMAS\n00003583\n2017-12-04\n2018-01-12\n9\n7\n1\n281791.0\nCONT_DATE based on Inciweb\nNone\nNone\nNone\nNone\n2010\nMULTIPOLYGON (((-13316089.016 4088553.040, -13...\n\n\n\n\n\n\n\nNow I will save only the 2017 Thomas Fire boundary as a GeoJSON file. The file should go into the data/ directory in my repository.\n\n# Save the fire boundary as a file that can go into my repository\n# Save the filtered GeoDataFrame as a GeoJSON file\npath = 'data/thomas.geojson'\nthomas.to_file(path, driver='GeoJSON')\n\nI chose to use a GeoJSON file format for my perimeter boundary because it is a common and useful “open format for encoding vector points and their attributes”. It comes in one file, as compared to .shp files, which have many dependencies. It requires the data be in WGS84, and since I already verified that this data is in that CRS, it seems like the best possible option for this analysis."
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#next-up",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#next-up",
    "title": "Thomas Fire Analysis",
    "section": "Next up:",
    "text": "Next up:\n\nTrue Color Image\nAs I import the raster file to make this true color image, I add the parameter decode_coords=\"all\" to the code. This parameter controls how the coordinate metadata in the NetCDF file are processed. Specifically, the all distinction decodes all coordinates in the file to easily useable xarray coordinate variables.\n\n# Construct a file path to the Landsat data using os and import it\nfp = os.path.join('data', 'landsat8-2018-01-26-sb-simplified.nc')\nsb_rast = rioxr.open_rasterio(fp, decode_coords=\"all\")\nsb_rast\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 25MB\nDimensions:      (band: 1, x: 870, y: 731)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0\nData variables:\n    red          (band, y, x) float64 5MB ...\n    green        (band, y, x) float64 5MB ...\n    blue         (band, y, x) float64 5MB ...\n    nir08        (band, y, x) float64 5MB ...\n    swir22       (band, y, x) float64 5MB ...xarray.DatasetDimensions:band: 1x: 870y: 731Coordinates: (4)band(band)int641array([1])x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([121305.0, 121575.0, 121845.0, 122115.0, 122385.0, 122655.0, 122925.0,\n       123195.0, 123465.0, 123735.0,\n       ...\n       353505.0, 353775.0, 354045.0, 354315.0, 354585.0, 354855.0, 355125.0,\n       355395.0, 355665.0, 355935.0],\n      dtype='float64', name='x', length=870))yPandasIndexPandasIndex(Index([3952395.0, 3952125.0, 3951855.0, 3951585.0, 3951315.0, 3951045.0,\n       3950775.0, 3950505.0, 3950235.0, 3949965.0,\n       ...\n       3757725.0, 3757455.0, 3757185.0, 3756915.0, 3756645.0, 3756375.0,\n       3756105.0, 3755835.0, 3755565.0, 3755295.0],\n      dtype='float64', name='y', length=731))Attributes: (0)\n\n\n\n# Explore the data\nprint('Shape: ', dict(sb_rast.sizes))\nprint(sb_rast.data_vars, '\\n')\n\nShape:  {'band': 1, 'x': 870, 'y': 731}\nData variables:\n    red      (band, y, x) float64 5MB ...\n    green    (band, y, x) float64 5MB ...\n    blue     (band, y, x) float64 5MB ...\n    nir08    (band, y, x) float64 5MB ...\n    swir22   (band, y, x) float64 5MB ... \n\n\n\nThis data exploration shows me that there is only one band on this raster. This means that including it is redundant, and the band will need to be removed. I also learned that each color band is a float64 integer, which is good to know when doing analysis.\n\n# Drop the band dimension of the data\n# Original dimensions and coordinates\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\n# Remove length 1 dimension (band)\nsb_rast = sb_rast.squeeze()\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\n# Drop the coordinates associated to band\nsb_rast = sb_rast.drop_vars('band')\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\nFrozenMappingWarningOnValuesAccess({'band': 1, 'x': 870, 'y': 731}) Coordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\nFrozenMappingWarningOnValuesAccess({'x': 870, 'y': 731}) Coordinates:\n    band         int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\nFrozenMappingWarningOnValuesAccess({'x': 870, 'y': 731}) Coordinates:\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\n\n\nTo get a good look at this raster data, without creating any new variables:\nI will select the red, green, and blue variables (in that order) of the xarray.Dataset holding the Landsat data, convert it to an xarray.DataArray using the to_array() method, and then use .plot.imshow() to create an RGB image with the data. There will be a warning, that’s ok. I will adjust the scale used for plotting the bands to get a true color image.\nThe first plot will have the parameter set to be robust=False.\n\n# Select red, green, and blue variables, stack them, and plot as an RGB image\nsb_rast[['red', 'green', 'blue']].to_array().plot.imshow(robust=False)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nAs we can see, this doesn’t turn out quite as we’d hoped. That’s because, by setting that parameter to False, we are not accounting for cloud cover. Their RGB values are outliers and cause the other values to be squished when plotting.\nTo account for this, I will use the robust = True parameter at the end of my code, in order to deal with the clouds:\n\n# Select red, green, and blue variables, stack them, and plot as an RGB image\nsb_rast[['red', 'green', 'blue']].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nThis true color image gives us a visual that mostly resembles what we would expect to see with our human eyes looking down on Santa Barbara. The colors are what we would expect to see, and this can be useful for identifying landmarks. However, sometimes it is important to get a new perspective. For example, using this map, it is almost impossible to see the area in which the Thomas Fire burned. And that’s when we bring in…\n\n\n4. False color image\nTo continue my analysis, and without creating any new variables, I will create a false color image by plotting the short-wave infrared (swir22), near-infrared, and red variables (in that order).\n\n# Select the swir22, near-infrared, and red variables, stack them, and plot as a false color image\nsb_rast[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nFalse color imagery, created using satellite data from instruments like Landsat, is a useful tool for monitoring wildfire impacts. By assigning infrared bands to visible colors, these images highlight vegetation health, burn severity, and the extent of fire scars. This approach helps researchers and land managers assess recovery efforts, identify high-risk areas, and plan restoration strategies.\n\n\n5. Map\nFinally, I will create a map showing the shortwave infrared/near-infrared/red false color image together with the Thomas Fire perimeter.\n\n# Read in the Thomas fire perimeter we created\nfp2 = os.path.join('data', 'thomas.geojson')\nthomas_perim = gpd.read_file(fp2)\nthomas_perim.plot()\n\n\n\n\n\n\n\n\nThis initial plot shows us that our perimeter file is looking good. It also shows us the perimeter that we will want to clip our raster file to, in order to create our map analyzing the burn area of the Thomas Fire.\nFirst, we will have to ensure that our CRS’s match for these datasets…\n\n# Examine CRss\nprint('Santa Barbara Raster CRS: ', sb_rast.rio.crs)\nprint('Thomas Fire Perimeter CRS: ', thomas_perim.crs)\n\nSanta Barbara Raster CRS:  EPSG:32611\nThomas Fire Perimeter CRS:  EPSG:3857\n\n\n\n# Reproject CRS of the Santa Barbara Raster\nsb_rast = sb_rast.rio.reproject(\"EPSG:3857\")\nprint('Matched CRS?', sb_rast.rio.crs == thomas_perim.crs)\n\nMatched CRS? True\n\n\n\n# Clip the sb_rast map to match the dimensions of the Thomas Fire Perimeter\nsb_fire = sb_rast.rio.clip_box(*thomas_perim.total_bounds)\n\n\n# Map our false color image with the fire boundary overlaid\nfig, ax = plt.subplots(figsize=(10, 10))\nsb_fire[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax=ax, robust=True)\nthomas_perim.boundary.plot(ax=ax, edgecolor=\"firebrick\", linewidth = 2, label=\"Thomas Fire Perimeter\")\nax.set_title(\"Thomas Fire: Burn Scars in False Color Imagery\", fontsize=16)\nax.legend(loc='upper right', fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\nOur true color image was insufficient to reveal plainly the path of the Thomas Fire. However, by simply utilizing a false color composit, the path of the fire and the effect it had on the landscape are much more clear.\nThis final map clearly shows the burn scars from the fire, contained within the perimeter boundary we utilized to help us further identify the exact area effected."
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#conclusion",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#conclusion",
    "title": "Thomas Fire Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nUsing Python in Jupyter Notebooks, I have successfully executed a series of analysis analyzing a fire that had clear and lasting effects on the regions of Santa Barbara and Ventura. This analysis also showcases the variety of data types that Python can handle, from spatial data to data visualization.\n\nCitations:\nAirNow. “Air Quality Index (AQI) Basics.” Accessed December 4, 2024. https://www.airnow.gov/aqi/aqi-basics/.\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nCAL Fire. “California Fire Perimeters (All).” Data.gov. Metadata created March 30, 2024, updated May 14, 2024. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436.\nHamm, Keith. “Closing Schools and Moving Finals Due to Thomas Fire: A Look at Our Education System’s Response to the Wildfire.” Santa Barbara Independent, December 13, 2017. https://www.independent.com/2017/12/13/closing-schools-and-moving-finals-due-thomas-fire/.\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 Atmospherically Corrected Surface Reflectance Data from Landsat 8 [Dataset]. Simplified for visualization and educational purposes. Accessed November 20, 2024. https://planetarycomputer.microsoft.com.\nU.S. Environmental Protection Agency. “Air Data: Air Quality Data Collected at Outdoor Monitors Across the US.” Accessed December 4, 2024. https://www.epa.gov/outdoor-air-quality-data."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-context",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-context",
    "title": "Experimenting with a DEEP SEA CORAL interactions model",
    "section": "THE CONTEXT:",
    "text": "THE CONTEXT:\nIt is estimated that two-thirds of all known coral species can be classified as “deep-sea corals”. These corals exhibit a diversity similar to those of their shallow-water relatives, but they do not form symbiotic relationships with algae, do not obtain energy from sunlight (instead feeding from microorganisms in the water), and are much more resilient to cold temperatures.\nAlso similar to their shallow water counterparts, deep sea corals are hosts to many other forms of biodiversity. They are also extremely slow growing, and as a result, slow to recover from ecological or anthropogenic disturbance[2].\nThis analysis will seek to more deeply understand the environments in which deep sea corals are more likely to appear."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-data",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-data",
    "title": "Experimenting with a DEEP SEA CORAL interactions model",
    "section": "THE DATA:",
    "text": "THE DATA:\nThe data used for this analysis comes from NOAA’s Deep Sea Coral Research and Technology Program (DSCRTP), and is housed at the Smithsonian National Museum of Natural History, Invertebrate Zoology Collection. It contains decades worth of data regarding deep sea corals, their taxa, spatial data, and collecting data. It is an open source data set, that is consistently being updated. (See link in the citations section, to take a look for yourself!)[3]\nHere are some details from the metadata of the dataset:\n\nNumber of records: 30,850\nNumber of coral records: 24,768\nNumber of sponge records: 6,082\nRecords with images: 245\n\nSeems like we have a lot of data points to work with here. Let’s begin by loading our libraries and reading in the data!\nThe deep sea corals dataset I chose also includes deep sea sponges. Since this analysis wants to focus on corals, I’ll have to filter that data out. In addition, there are several columns on the dataframe that I know won’t be helpful for my analysis (for example, collector data, institution data, or columns that contains a lot of NAs.) In addition to filtering out the sponge data, I will filter out those unwanted columns as well.\n\n\nCode\n# Filter data to only include columns I want\ncorals &lt;- corals %&gt;% \n  clean_names(case = \"snake\") %&gt;% # Change column names to snake_case\n  select(scientific_name, phylum, genus, species, individual_count, latitude, longitude, depth_in_meters, country) %&gt;% # Choose columns I want\n  filter(phylum %in% c(\"Cnidaria\")) # Filter to exclude sponges and NA row\n\n\nI know that in order to properly visualize my data, I’m going to want to make some maps. However, I also want the latitude and longitude data to be easily handled during any linear model calculations, so I decide to make a separate dataset variable, that transforms the lat long data into geometric data that I can easily add to tmaps for visualizations.\n\n\nCode\ncorals_transformed &lt;- corals %&gt;% \n  sf::st_as_sf(coords = c(\"longitude\", \"latitude\"),\n               crs = st_crs(4326)) %&gt;% \n              filter(st_is_valid(.))\n\n\nLet’s take a look at the result of our well filtered data! This data cleaning will enable us to run effective and easy to understand data analysis in the next few steps.\n\n\nCode\n# Create a table showing the head of our filtered dataframe\nkable(head(corals), format = \"html\", caption = \"Preview of Filtered Global Coral Data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\nPreview of Filtered Global Coral Data\n\n\nscientific_name\nphylum\ngenus\nspecies\nindividual_count\nlatitude\nlongitude\ndepth_in_meters\ncountry\n\n\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n2\n34.95839\n-75.32464\n146\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n6\n34.95839\n-75.32464\n146\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n6\n29.28357\n-88.26665\n84\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n1\n24.48375\n-80.88314\n191\nUSA\n\n\nDesmophyllum pertusum\nCnidaria\nDesmophyllum\npertusum\n1\n30.96684\n-79.69976\n396\nUSA\n\n\nDesmophyllum pertusum\nCnidaria\nDesmophyllum\npertusum\n1\n27.98361\n-79.3331\n577\nBahamas"
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-exploration",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-exploration",
    "title": "Experimenting with a DEEP SEA CORAL interactions model",
    "section": "THE EXPLORATION:",
    "text": "THE EXPLORATION:\n\nVISUALIZATION\nI want to take a look at the whole scale of my data set. To do this, I decide to make a global map.\n\n\nCode\nworld &lt;- spData::world\n\ntm_shape(world) +\n  tm_fill(col = \"continent\",\n          palette = c(\"slategray\", \"snow3\", \"slategray2\", \"slategray3\", \"slategray4\", \"lightslategrey\", \"lightsteelblue4\", \"snow4\"),\n          title = \"Continents\") +\n  tm_shape(corals_transformed) +\n  tm_dots(col = \"phylum\",\n          palette = \"pink2\",\n          size = 0.05,\n          border.col = \"black\",\n          title = \"Data Point\") +\n  # tm_compass(type = \"4star\",\n  #            size = .05,\n  #            position = c(\"left\", \"top\")) +\n  tm_layout(main.title = \"Global Coral Observations\",\n            title.position = c(\"center\", \"top\"),\n            title.snap.to.legend = FALSE,\n            #frame = TRUE, \n            #legend.frame = TRUE,\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\nFrom this global map, we can see the huge volume to coral observations present in our dataset. In order to get a little more focus in my model, I decide filter the datset to a specific country, and run my analysis on that.\nIt could be interesting to, in the future, run the same analysis on each continent, and then cross-compare them. This would reveal interesting insights about different continents and species. Politically, it could also be an interesting view into how collecting bias might affect the results of such an analysis, since we can see that some continents contain more sample points than others. Unfortunately, this could be due to the fact that certain countries and regions are more studied than others, due to data collecting bias in the environmental sector.[1]\nFor the purpose of this study, however, I will filter the coral data to the Philippines. The Philippines is an area known for its coral diversity and it’s popularity in the diving community. I am curious to see how DEEP SEA corals are represented in the region, rather than the easily dive-able shallow coral systems.\n\n\nCode\nphil_coral &lt;- corals %&gt;% \n  filter(country %in% \"Philippines\") %&gt;% \n  mutate(\n    latitude = as.numeric(latitude),\n    longitude = as.numeric(longitude),\n    depth_in_meters = as.numeric(depth_in_meters),\n    latitude_rounded = round(latitude, 1)\n  ) %&gt;% \n  add_count(latitude_rounded, name = \"obs_count\")\n\n# Transformed dataset for mapping\nphil_coral_transformed &lt;- corals_transformed %&gt;% \n  filter(country %in% \"Philippines\")\n\n# Create a table showing the head of our filtered dataframe\nkable(head(phil_coral), format = \"html\", caption = \"Preview of Filtered Philippines Coral Data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\nPreview of Filtered Philippines Coral Data\n\n\nscientific_name\nphylum\ngenus\nspecies\nindividual_count\nlatitude\nlongitude\ndepth_in_meters\ncountry\nlatitude_rounded\nobs_count\n\n\n\n\nCoralliidae\nCnidaria\nNA\nNA\n1\n15.9708\n119.672\n1719\nPhilippines\n16.0\n3\n\n\nStenohelia tiliata\nCnidaria\nStenohelia\ntiliata\n1\n6.1333\n121.317\n275\nPhilippines\n6.1\n25\n\n\nStylaster multiplex\nCnidaria\nStylaster\nmultiplex\n1\n5.1867\n119.590\n450\nPhilippines\n5.2\n16\n\n\nDistichopora irregularis\nCnidaria\nDistichopora\nirregularis\n1\n7.0853\n125.662\n42\nPhilippines\n7.1\n8\n\n\nDistichopora irregularis\nCnidaria\nDistichopora\nirregularis\n4\n7.0950\n125.662\n38\nPhilippines\n7.1\n8\n\n\nDistichopora\nCnidaria\nDistichopora\nNA\n1\n7.0867\n125.660\n37\nPhilippines\n7.1\n8\n\n\n\n\n\n\n\n\n\nCode\n# Get data for base map of Philippines\nph &lt;- ne_countries(scale = 10, country = \"Philippines\", returnclass = \"sf\")\n\n# Make a plot of corals in the Philippines\ntm_shape(ph) +\n  tm_polygons(col = \"name\",\n          palette = c(\"slategray\", \"snow3\", \"slategray2\", \"slategray3\", \"slategray4\", \"lightslategrey\", \"lightsteelblue4\", \"snow4\"),\n          title = \"Philippines\") +\n  tm_shape(phil_coral_transformed) +\n  tm_dots(col = \"phylum\",\n          palette = \"pink2\",\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Data Point\") +\n   tm_compass(type = \"8star\",\n              size = 2,\n              position = c(\"left\", \"top\")) +\n  tm_layout(main.title = \"Coral Observations \\nin the Philippines\",\n            title.position = c(\"center\", \"top\"),\n            title.snap.to.legend = FALSE,\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nTHE SUMMARIZATION:\nAs we can see from the map above, there have been deep sea coral collections happening all throughout the country of the Philippines. This gives me a lot of data to run analysis on. I decided to make some additional visualizations to summarize the contents of the data, and guide me toward making a null and alternate hypothesis.\nFirst, I want to take a look at the distribution of coral observations along the different latitudes of the Philippines. In the data cleaning I performed above, I decided to create another column on the Philippines coral dataframe that rounded the latitudes to just two decimal places. This will allow me to work with latitiude data more easily, without having too many latitude points obscuring the visualizations of my data. I also added a column that summed the total number of coral observations at each of these rounded latitudes. This number will allow me to analyze the abundance of corals at each rounded latitude, enabling analysis of coral distributions.\nIt is using the rounded latitudes column(latitude_rounded), and the the observations per latitude column(obs_count) that I made the following data visualizations.\nFirst up, I wanted to take an initial look at these two columns:\n\n\nCode\n# Make a plot of coral obs and latitude\nggplot() +\n geom_line(data = phil_coral,\n            aes(x = latitude_rounded,\n                y = obs_count),\n           color = \"pink2\",\n           size = 1) +\n  labs(x = \"Latitude\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Deep Sea Corals at Latitude\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows the general distribution of the corals along the different latitudes, and visualizes a significant spike in abundance around the 14 degrees latitude line.\nI wanted a way to visualize depth fluctuations along these latitude lines, in order to help visualize general depth patters in the area of interest. I decided to do another line graph, this one working almost like a cross section of depth along latitude, though of course only including depths available on our dataframe, so not actually having spatial representation, just data representation:\n\n\nCode\n# Make a plot\nggplot() +\n geom_line(data = phil_coral,\n            aes(x = latitude_rounded,\n                y = depth_in_meters),\n           color = \"slategray\",\n           size = 1) +\n  labs(x = \"Latitude\",\n       y = \"Depth in Meters\",\n       title = \"Depth Fluctuations in the Philippines\") +\n  scale_y_reverse() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis visualization showed some interesting spikes in depths collected from, with another large spike around the 14 degree mark, but several other degrees showing some spikes as well. This helps visualize the depths along latitudes that our corals dwell on, but I want to find a better way of visualizing how these depths might effect coral abundance.\nI decide to create scatterplots that incorporates all three variables, and see how these different visualizations can help me form my hypothesis.\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = latitude_rounded,\n                 y = depth_in_meters,\n                 color = obs_count)) +\n  scale_color_gradient(low = \"black\", high = \"pink2\") +\n  labs(x = \"Latitude\",\n       y = \"Depth in Meters\",\n       title = \"Colored by # of Observations\") +\n    geom_smooth(data = phil_coral,\n              aes(x = latitude_rounded, y = depth_in_meters), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot, with a geom_smooth line with method “lm”, or linear model, shows us that as latitude increases, the depths that corals are found in decreases slightly. This visualization makes it difficult to interpret the number of corals observed, however, though it does show how there are more observations made at the 14 degree latitude.\nI decide to try rearranging what variables go on what axes:\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = latitude_rounded,\n                 y = obs_count,\n                 color = depth_in_meters)) +\n  scale_color_gradient(low = \"skyblue\", high = \"black\") +\n  labs(x = \"Latitude\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Depth, Latitude, and Observation Count\") +\n    geom_smooth(data = phil_coral,\n              aes(x = latitude_rounded, y = obs_count), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows me that as latitude increaases, the number of corals observed also increases. There is an outlier high up around latitude 14 degrees, and I suspect that that outlier is increasing the slope of our lm line by quite a bit. This plot makes it easy to observe the number of observations made, compared to the first plot.\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = depth_in_meters,\n                 y = obs_count,\n                 color = latitude)) +\n  scale_color_gradient(low = \"skyblue\", high = \"black\") +\n  labs(x = \"Depth in Meters\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Depth, Latitude, and Observation Count\") +\n  geom_smooth(data = phil_coral,\n              aes(x = depth_in_meters, y = obs_count), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nFinally, this plot shows us that as depth increases, the number of deep sea corals observed also decreases, with the colors of the points showing us the distribution of the points along latitude, with more observations occurring at higher latitudes.\nAll of these plots seem to show that there is some sort of relationship between all of these variables. Let’s see if we can quantify that relationship!\n\n\nTHE HYPOTHESIS:\nNull: There is no relationship between number of deep sea corals observed, latitude, and depth in the Philippines.\nAlternate: There IS a relationship between number of deep sea corals observed, latitude, and depth in the Philippines."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-analysis",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-analysis",
    "title": "Experimenting with a DEEP SEA CORAL interactions model",
    "section": "THE ANALYSIS:",
    "text": "THE ANALYSIS:\n\nMETHOD:\nSince I have multiple variables to contend with, I decide on using an INTERACTIONS MODEL to understand the relationship between my variables (corals observed, latitude, and depth). My interactions model will take on this format:\n\\[\n\\text{number of deep sea corals observed} \\sim \\text{latitude} + \\text{depth} + \\text{latitude} \\times \\text{depth}\n\\]\nOr, more technically:\n\\[\n\\text{coral observed} = \\beta_0 + \\beta_1 \\cdot \\text{latitude} + \\beta_2 \\cdot \\text{depth} + \\beta_3 \\cdot (\\text{latitude} \\times \\text{depth}) + \\epsilon\n\\]\n\n\nIMPLEMENTATION:\nIn R code, I will create this linear model and then run the summary:\n\n\nCode\n# Create an interactions model\ninteractions &lt;- lm(obs_count ~ latitude_rounded + depth_in_meters + latitude_rounded:depth_in_meters,\n                   data = phil_coral)\nsummary(interactions)\n\n\n\nCall:\nlm(formula = obs_count ~ latitude_rounded + depth_in_meters + \n    latitude_rounded:depth_in_meters, data = phil_coral)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-116.30  -36.38  -11.13   18.18  137.55 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      -81.583936   9.804587  -8.321  2.6e-16 ***\nlatitude_rounded                  12.426692   0.827484  15.017  &lt; 2e-16 ***\ndepth_in_meters                    0.056945   0.024205   2.353 0.018821 *  \nlatitude_rounded:depth_in_meters  -0.007709   0.002110  -3.653 0.000272 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.38 on 1082 degrees of freedom\nMultiple R-squared:  0.2766,    Adjusted R-squared:  0.2746 \nF-statistic: 137.9 on 3 and 1082 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nDIAGNOSTICS AND INTERPRETATION:\nLooking at the p values on the far right of the summary chart, I can see that using latitude and depth together is significantly better than using depth alone to predict coral counts.\nThe p-value of the F-statistic tells me that it is below our threshold of .05, meaning that the relationship between our variables is statistically significant and we can REJECT the null hypothesis. So, there IS a relationship between coral counts, depth, and latitude!\nHowever, our adjusted R squared tells me that that only 27.46% of the variation of coral counts is explained by depth and latitude. Since this isn’t a very high percentage, I am lead to believe that there may be another variable in play that also effects coral counts.\nIf I were to take this test further, I would explore the possibility of omitted variable bias.\nIt is also important to consider how collecting practices might be influencing how and where corals are collected. I would also recommend further analysis of these variables, should I choose to continue this DEEP SEA CORALS analysis."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#citations",
    "href": "posts/2024-12-12-coral-interactions/index.html#citations",
    "title": "Experimenting with a DEEP SEA CORAL interactions model",
    "section": "Citations",
    "text": "Citations\n[1]Konno, K., Gibbons, J., Lewis, R. and Pullin, A.S., 2024. Potential types of bias when estimating causal effects in environmental research and how to interpret them. Environmental Evidence, 13(1), p.1. (https://link.springer.com/article/10.1186/s13750-024-00324-7)\n[2]Roberts, S. and Hirshfield, M. (2004), Deep-sea corals: out of sight, but no longer out of mind. Frontiers in Ecology and the Environment, 2: 123-130. https://doi.org/10.1890/1540-9295(2004)002[0123:DCOOSB]2.0.CO;2\n[3]Smithsonian Institution, National Museum of Natural History. Observation date range: 1860 to 2022. Coral or sponge occurrence observations submitted to the NOAA National Database for Deep Sea Corals and Sponges (www.deepseacoraldata.noaa.gov). DSCRTP Dataset ID: NMNH_IZ. Database version: 20241022-1.\n[4] IMAGE CREDITS: Woods Hole Oceanographic Institution. “Deep-sea Corals.” Know Your Ocean. Accessed December 10, 2024. https://www.whoi.edu/know-your-ocean/ocean-topics/ocean-life/coral/deep-sea-corals/."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#github-repository",
    "href": "posts/2024-12-12-coral-interactions/index.html#github-repository",
    "title": "Experimenting with a DEEP SEA CORAL interactions model",
    "section": "GITHUB REPOSITORY:",
    "text": "GITHUB REPOSITORY:\nhttps://github.com/jorb1/coral-interactions"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html",
    "href": "posts/2024-10-18-my-first-post/index.html",
    "title": "New Records of Eocene and Oligocene Squamates from Southwest Montana",
    "section": "",
    "text": "Recently recovered specimens from the Chadronian Pipestone Springs Main Pocket (PSMP), Whitneyan strata in the Gravelly Range, and Uintan rocks in the Sage Creek area significantly increase the diversity of squamates known from the Tertiary depositional basins of southwest Montana. At PSMP, most common are the glyptosaurine lizard Helodermoides tuberculatus which represents over 60% of dentigerous lizard elements and the boid snake Calamagras which is represented by forty-four vertebrae. Four varanid elements, a specimen tentatively referred to the glyptosaurine Peltosaurus, and Calamagras from PSMP are their first documented occurrences in the Tertiary depositional basins of southwest Montana, as is a miniscule dentary fragment that represents an iguanid smaller than Aciprion formosum. A skull of A. formosum from the Gravelly Range is the first record of the species from Montana and its first confirmed occurrence from Whitneyan strata. Fragmentary elements from the Uintan Dell beds in the Sage Creek depositional basin indicate the presence of a glyptosaurine and an indeterminate iguanid. These new records of Eocene and Oligocene squamates from Montana are important additions to the temporal and/or paleogeographic ranges of Calamagras, Aciprion formosum, varanids, and perhaps Peltosaurus.\n\n\n\n\n\n\n\n\n\n\n\n\nSmash that contact button if you want to learn more!"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#published-in-paludicola-143110-121",
    "href": "posts/2024-10-18-my-first-post/index.html#published-in-paludicola-143110-121",
    "title": "New Records of Eocene and Oligocene Squamates from Southwest Montana",
    "section": "",
    "text": "Recently recovered specimens from the Chadronian Pipestone Springs Main Pocket (PSMP), Whitneyan strata in the Gravelly Range, and Uintan rocks in the Sage Creek area significantly increase the diversity of squamates known from the Tertiary depositional basins of southwest Montana. At PSMP, most common are the glyptosaurine lizard Helodermoides tuberculatus which represents over 60% of dentigerous lizard elements and the boid snake Calamagras which is represented by forty-four vertebrae. Four varanid elements, a specimen tentatively referred to the glyptosaurine Peltosaurus, and Calamagras from PSMP are their first documented occurrences in the Tertiary depositional basins of southwest Montana, as is a miniscule dentary fragment that represents an iguanid smaller than Aciprion formosum. A skull of A. formosum from the Gravelly Range is the first record of the species from Montana and its first confirmed occurrence from Whitneyan strata. Fragmentary elements from the Uintan Dell beds in the Sage Creek depositional basin indicate the presence of a glyptosaurine and an indeterminate iguanid. These new records of Eocene and Oligocene squamates from Montana are important additions to the temporal and/or paleogeographic ranges of Calamagras, Aciprion formosum, varanids, and perhaps Peltosaurus.\n\n\n\n\n\n\n\n\n\n\n\n\nSmash that contact button if you want to learn more!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bailey Jørgensen",
    "section": "",
    "text": "Underwater Paleontology, Robotics, and Education\n\n\nA reflection on the early days of a novel data collection technique\n\n\n\nBailey Jørgensen\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA geospatial look at the relationship between redlining and citizen science\n\n\nUsing HOLC grade and GBIF data from Los Angeles\n\n\n\nBailey Jørgensen\n\n\nJan 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrioritizing Potential Aquaculture Zones on the West Coast of the United States\n\n\nAn Exercise in R and Quarto\n\n\n\nBailey Jørgensen\n\n\nDec 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with a DEEP SEA CORAL interactions model\n\n\nAnalyzing the relationship between deep sea corals, latitude, and ocean depth\n\n\n\nBAILEY JØRGENSEN\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R Analysis of Landcover Classification using Decision Trees\n\n\nUsing multi-spectral imagery on the location of 4 land cover types, with a machine learning twist\n\n\n\nBailey Jørgensen\n\n\nNov 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire Analysis\n\n\nAQI and spatial analysis walkthrough of the 2017 natural disaster from the perspective of a Masters of Environmental Data Science student\n\n\n\nBailey Jørgensen\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiodiversity Intactness Index (BII) change in Phoenix, AZ\n\n\nA geospatial analysis in Python\n\n\n\nBailey Jørgensen\n\n\nSep 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Records of Eocene and Oligocene Squamates from Southwest Montana\n\n\nA new paper published in Paludicola by the Rochester Institute of Vertebrate Paleontology\n\n\n\nDonald Lofgren, Debra Hanneman, Anthony Runkel, Ping Fong, Gabriel Hong, Josephine Burdekin, Michael Chai, Yvonne Kan, and Bailey Jørgensen\n\n\nSep 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bailey Jørgensen",
    "section": "",
    "text": "Nice to e-meet you, I’m Bailey.\n\n\nMapping and data management are my day-passions. Marine secrets and deep time are my life-passions. When I’m not expanding my technical knowledge of coding and geospatial analysis at UC Santa Barbara Bren, caring for an active research collection and building robots for the Alf Museum of Paleontology takes up most of my professional time. Sailing and preserving historic wooden ships for maritime institutes ignited my interest in the heritage sector. Paleontological fieldwork and data collection in the deserts of Utah, California, Wyoming, Montana and Mongolia solidified it. Have a project in the biodiversity heritage sector? (Bonus points if its for marine environments.) Let’s chat!"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "",
    "text": "Purpose: In 2021, Maricopa County —home to the Phoenix metropolitan area— was identified as the U.S. county with the most significant increase in developed land since 2001. This rapid urban sprawl has profound implications for biodiversity and the health of surrounding natural ecosystems.\nIn this notebook, I will investigate the impacts of urban expansion by analyzing a dataset that captures values for the Biodiversity Intactness Index (BII). Apecifically, I will examine changes in BII in the Phoenix county subdivision area between 2017 and 2020, shedding light on how urban growth affects biodiversity over time.\nHighlights: 1. Accessing the Microsoft Planetary Computer to Access their Impact Observatory data.\n\nCreating a plot of the Biodiversity Intactness Index within the Phoenix subdivision polygon.\nCalculate the percentage of area of the Phoenix area with a BII index of at least .07 in 2017 and 2020.\nPlotting the 2020 data, revealing areas that show a loss of biodiversity.\n\nAbout the data: 1. The first data set is is the Biodiversity Intactness Index (BII) Time Series. Access the io-biodiversity collection from the Microsoft Planetary Computer STAC catalog. I will be using the 2017 and 2020 rasters covering the Phoenix subdivision.\n\nThe second data set is the Phoenix Subdivision Shapefile Download the Phoenix subdivision polygon from the Census County Subdivision shapefiles for Arizona. All legal boundaries and names are as of January 1, 2024. The 2024 TIGER/Line Shapefiles were released on September 25, 2024. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions\n\n\n\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nMicrosoft Planetary Computer, STAC Catalog. Biodiversity Intactness (‘io-biodiversity’). [Dataset]. https://planetarycomputer.microsoft.com/dataset/io-biodiversity Accessed 2 December 2024.\nUnited States Census Bureau. (2022). Arizona County Subdivision 2022 TIGER/Line Shapefiles. [Data File]. U.S. Census Bureau, Geography Division. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions Accessed 2 December 2024.\nBoth of these datasets were accessed for this analysis on 12/2/2024.\n\n# Load Libraries\nimport pandas as pd\nimport geopandas as gpd\nimport planetary_computer\nimport pystac_client\nimport rich.table\nfrom geogif import gif\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rioxarray as rioxr\nfrom IPython.display import Image \nfrom shapely.geometry import box\nimport xarray as xr\nimport os\nimport rasterio\nfrom rasterio.windows import from_bounds\nfrom IPython.display import Image\nimport contextily as ctx\nimport matplotlib.patches as mpatches\nfrom matplotlib.lines import Line2D\n\nTo begin the analysis, I will first read in and explore the shapefile data that I have for Arizona, specifically looking for the Phoenix area.\n\n# Read in shapefile data for Arizona\narizona = gpd.read_file('data/tl_2022_04_cousub.shp')\narizona.head(3)\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUSUBFP\nCOUSUBNS\nGEOID\nNAME\nNAMELSAD\nLSAD\nCLASSFP\nMTFCC\nCNECTAFP\nNECTAFP\nNCTADVFP\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n04\n005\n91198\n01934931\n0400591198\nFlagstaff\nFlagstaff CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n12231052883\n44653332\n+35.1066114\n-111.3662497\nPOLYGON ((-112.13370 35.85596, -112.13368 35.8...\n\n\n1\n04\n005\n91838\n01934953\n0400591838\nKaibab Plateau\nKaibab Plateau CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n7228864534\n29327221\n+36.5991097\n-112.1368033\nPOLYGON ((-112.66039 36.53941, -112.66033 36.5...\n\n\n2\n04\n005\n91683\n01934950\n0400591683\nHualapai\nHualapai CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n2342313339\n3772690\n+35.9271665\n-113.1170408\nPOLYGON ((-113.35416 36.04097, -113.35416 36.0...\n\n\n\n\n\n\n\n\n# Clean up Arizona data\narizona.columns = arizona.columns.str.lower()\n\n# Select just the Phoenix area\nphoenix = arizona[arizona.name == \"Phoenix\"]\n\n# Generate a quick plot of the Phoenix shape\nphoenix.plot()\n\n\n\n\n\n\n\n\nWhen making maps it’s important to know what Coordinate Reference System we are dealing with. Let’s check what we have for the Phoenix file:\n\nphoenix.crs\n\n&lt;Geographic 2D CRS: EPSG:4269&gt;\nName: NAD83\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.\n- bounds: (167.65, 14.92, -40.73, 86.45)\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nNow that I have my Arizona data looking how I want it, lets access the Microsoft Planetary Computer!\n\n# Access the MPC catalog\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\ncatalog.get_collections()\ncollections = list(catalog.get_collections())\n\n# Print the number of collections\nprint('Number of collections:', len(collections))\n\n#Pull out the Impact Observatory collection\nio_collection = catalog.get_child('io-biodiversity')\nio_collection\n\nNumber of collections: 124\n\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Collection\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"io-biodiversity\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            description\n            \"Generated by [Impact Observatory](https://www.impactobservatory.com/), in collaboration with [Vizzuality](https://www.vizzuality.com/), these datasets estimate terrestrial Biodiversity Intactness as 100-meter gridded maps for the years 2017-2020.\n\nMaps depicting the intactness of global biodiversity have become a critical tool for spatial planning and management, monitoring the extent of biodiversity across Earth, and identifying critical remaining intact habitat. Yet, these maps are often years out of date by the time they are available to scientists and policy-makers. The datasets in this STAC Collection build on past studies that map Biodiversity Intactness using the [PREDICTS database](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.2579) of spatially referenced observations of biodiversity across 32,000 sites from over 750 studies. The approach differs from previous work by modeling the relationship between observed biodiversity metrics and contemporary, global, geospatial layers of human pressures, with the intention of providing a high resolution monitoring product into the future.\n\nBiodiversity intactness is estimated as a combination of two metrics: Abundance, the quantity of individuals, and Compositional Similarity, how similar the composition of species is to an intact baseline. Linear mixed effects models are fit to estimate the predictive capacity of spatial datasets of human pressures on each of these metrics and project results spatially across the globe. These methods, as well as comparisons to other leading datasets and guidance on interpreting results, are further explained in a methods [white paper](https://ai4edatasetspublicassets.blob.core.windows.net/assets/pdfs/io-biodiversity/Biodiversity_Intactness_whitepaper.pdf) entitled “Global 100m Projections of Biodiversity Intactness for the years 2017-2020.”\n\nAll years are available under a Creative Commons BY-4.0 license.\n\"\n        \n    \n                \n            \n                \n                    \n        links[] 7 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"items\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/io-biodiversity/items\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/geo+json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"license\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://creativecommons.org/licenses/by/4.0/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"CC BY 4.0\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            rel\n            \"about\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pdfs/io-biodiversity/Biodiversity_Intactness_whitepaper.pdf\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/pdf\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Technical White Paper\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            \n        \n            \n                \n        \n            rel\n            \"describedby\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/dataset/io-biodiversity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Human readable dataset overview and reference\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            \n        \n            \n                \n        \n            rel\n            \"self\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/io-biodiversity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            \n        \n            \n                \n        \n            rel\n            \"parent\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        stac_extensions[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/item-assets/v1.0.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"https://stac-extensions.github.io/table/v1.2.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            item_assets\n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff; profile=cloud-optimized\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Terrestrial biodiversity intactness at 100m resolution\"\n        \n    \n            \n        \n            \n                \n        raster:bands[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            sampling\n            \"area\"\n        \n    \n            \n        \n            \n                \n        \n            data_type\n            \"float32\"\n        \n    \n            \n        \n            \n                \n        \n            spatial_resolution\n            100\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            msft:region\n            \"westeurope\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:container\n            \"impact\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:storage_account\n            \"pcdata01euw\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:short_description\n            \"Global terrestrial biodiversity intactness at 100m resolution for years 2017-2020\"\n        \n    \n                \n            \n                \n                    \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n                \n            \n                \n                    \n        \n            extent\n            \n        \n            \n                \n        \n            spatial\n            \n        \n            \n                \n        bbox[] 1 items\n        \n            \n        \n            \n                \n        0[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -180\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            -90\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            180\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            90\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            temporal\n            \n        \n            \n                \n        interval[] 1 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2017-01-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2020-12-31T23:59:59Z\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            license\n            \"CC-BY-4.0\"\n        \n    \n                \n            \n                \n                    \n        keywords[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"Global\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"Biodiversity\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        providers[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"Impact Observatory\"\n        \n    \n            \n        \n            \n                \n        roles[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"producer\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"licensor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.impactobservatory.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            name\n            \"Vizzuality\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.vizzuality.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            name\n            \"Microsoft\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"host\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://planetarycomputer.microsoft.com\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            summaries\n            \n        \n            \n                \n        version[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"v1\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            assets\n            \n        \n            \n                \n        \n            thumbnail\n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pc_thumbnails/io-biodiversity-thumb.png\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n            \n        \n            \n                \n        \n            media_type\n            \"image/png\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            geoparquet-items\n            \n        \n            \n                \n        \n            href\n            \"abfs://items/io-biodiversity.parquet\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/x-parquet\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"GeoParquet STAC items\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Snapshot of the collection's STAC items exported to GeoParquet format.\"\n        \n    \n            \n        \n            \n                \n        \n            msft:partition_info\n            \n        \n            \n                \n        \n            is_partitioned\n            False\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            table:storage_options\n            \n        \n            \n                \n        \n            account_name\n            \"pcstacitems\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"stac-items\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n        \n    \n\n\n\nNow that we have our data, we need to specify the temporal and spatial information we are interested in. I will create a bounding box, that specifies our spatial area of interest.\nAs we can see from the description of the Impact Observatory Collection above, the only dates represented in the data are our time range of interest (2017-2020). As such, there is no need to specify a temporal variable.\n\n# Create bounding box\nbbox_of_interest = [-112.826843, 32.974108, -111.184387, 33.863574]\n\n\n# Catalog search\nsearch = catalog.search(\n    collections = ['io-biodiversity'],\n    bbox = bbox_of_interest)\n\n# Get items from search\nitems = search.item_collection()\n\n# Determine number of items in search\nprint(f'There are {len(items)} items in the search.')\n\nThere are 4 items in the search.\n\n\n\n# Retrieve items\nitem_names = {item.id : item for item in search.items()}\nlist(item_names) # from this list, we can see that our 1st item is 2020 data, and our 4th item is 2017 data\n\n# Select 2017 subset\nphx_2017 = items[3]\n\n# Select 2020 subset\nphx_2020 = items[0]\n\nOur search returned four STAC Items. We can tell from their IDs that that they contain data for the same area but for different times, specifically the years 2017 through 2020. Let’s display the available assets and properties for the 2017 Item.\n\nasset_table = rich.table.Table(\"Asset Key\", \"Asset Title\")\nfor key, value in items[-1].assets.items():\n    asset_table.add_row(key, value.title)\nasset_table\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Asset Key        ┃ Asset Title                     ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ data             │ Biodiversity Intactness         │\n│ tilejson         │ TileJSON with default rendering │\n│ rendered_preview │ Rendered preview                │\n└──────────────────┴─────────────────────────────────┘\n\n\n\n\nproperty_table = rich.table.Table(\"Property Name\", \"Property Value\")\nfor key, value in sorted(items[-1].properties.items()):\n    property_table.add_row(key, str(value))\nproperty_table\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Property Name  ┃ Property Value                                                                                 ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ datetime       │ None                                                                                           │\n│ end_datetime   │ 2017-12-31T23:59:59Z                                                                           │\n│ proj:epsg      │ 4326                                                                                           │\n│ proj:shape     │ [7992, 7992]                                                                                   │\n│ proj:transform │ [0.0008983152841195215, 0.0, -115.38597824385106, 0.0, -0.0008983152841195215,                 │\n│                │ 34.74464974521749, 0.0, 0.0, 1.0]                                                              │\n│ start_datetime │ 2017-01-01T00:00:00Z                                                                           │\n└────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\nWe can see from the tables, that the Planetary Computer includes a “Rendered Preview”. Let’s take a look at the 2017 data:\n\n# Plot rendered preview\nImage(url=phx_2017.assets['rendered_preview'].href, width=500)"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#about",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#about",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "",
    "text": "Purpose: In 2021, Maricopa County —home to the Phoenix metropolitan area— was identified as the U.S. county with the most significant increase in developed land since 2001. This rapid urban sprawl has profound implications for biodiversity and the health of surrounding natural ecosystems.\nIn this notebook, I will investigate the impacts of urban expansion by analyzing a dataset that captures values for the Biodiversity Intactness Index (BII). Apecifically, I will examine changes in BII in the Phoenix county subdivision area between 2017 and 2020, shedding light on how urban growth affects biodiversity over time.\nHighlights: 1. Accessing the Microsoft Planetary Computer to Access their Impact Observatory data.\n\nCreating a plot of the Biodiversity Intactness Index within the Phoenix subdivision polygon.\nCalculate the percentage of area of the Phoenix area with a BII index of at least .07 in 2017 and 2020.\nPlotting the 2020 data, revealing areas that show a loss of biodiversity.\n\nAbout the data: 1. The first data set is is the Biodiversity Intactness Index (BII) Time Series. Access the io-biodiversity collection from the Microsoft Planetary Computer STAC catalog. I will be using the 2017 and 2020 rasters covering the Phoenix subdivision.\n\nThe second data set is the Phoenix Subdivision Shapefile Download the Phoenix subdivision polygon from the Census County Subdivision shapefiles for Arizona. All legal boundaries and names are as of January 1, 2024. The 2024 TIGER/Line Shapefiles were released on September 25, 2024. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions\n\n\n\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nMicrosoft Planetary Computer, STAC Catalog. Biodiversity Intactness (‘io-biodiversity’). [Dataset]. https://planetarycomputer.microsoft.com/dataset/io-biodiversity Accessed 2 December 2024.\nUnited States Census Bureau. (2022). Arizona County Subdivision 2022 TIGER/Line Shapefiles. [Data File]. U.S. Census Bureau, Geography Division. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions Accessed 2 December 2024.\nBoth of these datasets were accessed for this analysis on 12/2/2024.\n\n# Load Libraries\nimport pandas as pd\nimport geopandas as gpd\nimport planetary_computer\nimport pystac_client\nimport rich.table\nfrom geogif import gif\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rioxarray as rioxr\nfrom IPython.display import Image \nfrom shapely.geometry import box\nimport xarray as xr\nimport os\nimport rasterio\nfrom rasterio.windows import from_bounds\nfrom IPython.display import Image\nimport contextily as ctx\nimport matplotlib.patches as mpatches\nfrom matplotlib.lines import Line2D\n\nTo begin the analysis, I will first read in and explore the shapefile data that I have for Arizona, specifically looking for the Phoenix area.\n\n# Read in shapefile data for Arizona\narizona = gpd.read_file('data/tl_2022_04_cousub.shp')\narizona.head(3)\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUSUBFP\nCOUSUBNS\nGEOID\nNAME\nNAMELSAD\nLSAD\nCLASSFP\nMTFCC\nCNECTAFP\nNECTAFP\nNCTADVFP\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n04\n005\n91198\n01934931\n0400591198\nFlagstaff\nFlagstaff CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n12231052883\n44653332\n+35.1066114\n-111.3662497\nPOLYGON ((-112.13370 35.85596, -112.13368 35.8...\n\n\n1\n04\n005\n91838\n01934953\n0400591838\nKaibab Plateau\nKaibab Plateau CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n7228864534\n29327221\n+36.5991097\n-112.1368033\nPOLYGON ((-112.66039 36.53941, -112.66033 36.5...\n\n\n2\n04\n005\n91683\n01934950\n0400591683\nHualapai\nHualapai CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n2342313339\n3772690\n+35.9271665\n-113.1170408\nPOLYGON ((-113.35416 36.04097, -113.35416 36.0...\n\n\n\n\n\n\n\n\n# Clean up Arizona data\narizona.columns = arizona.columns.str.lower()\n\n# Select just the Phoenix area\nphoenix = arizona[arizona.name == \"Phoenix\"]\n\n# Generate a quick plot of the Phoenix shape\nphoenix.plot()\n\n\n\n\n\n\n\n\nWhen making maps it’s important to know what Coordinate Reference System we are dealing with. Let’s check what we have for the Phoenix file:\n\nphoenix.crs\n\n&lt;Geographic 2D CRS: EPSG:4269&gt;\nName: NAD83\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.\n- bounds: (167.65, 14.92, -40.73, 86.45)\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nNow that I have my Arizona data looking how I want it, lets access the Microsoft Planetary Computer!\n\n# Access the MPC catalog\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\ncatalog.get_collections()\ncollections = list(catalog.get_collections())\n\n# Print the number of collections\nprint('Number of collections:', len(collections))\n\n#Pull out the Impact Observatory collection\nio_collection = catalog.get_child('io-biodiversity')\nio_collection\n\nNumber of collections: 124\n\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Collection\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"io-biodiversity\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            description\n            \"Generated by [Impact Observatory](https://www.impactobservatory.com/), in collaboration with [Vizzuality](https://www.vizzuality.com/), these datasets estimate terrestrial Biodiversity Intactness as 100-meter gridded maps for the years 2017-2020.\n\nMaps depicting the intactness of global biodiversity have become a critical tool for spatial planning and management, monitoring the extent of biodiversity across Earth, and identifying critical remaining intact habitat. Yet, these maps are often years out of date by the time they are available to scientists and policy-makers. The datasets in this STAC Collection build on past studies that map Biodiversity Intactness using the [PREDICTS database](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.2579) of spatially referenced observations of biodiversity across 32,000 sites from over 750 studies. The approach differs from previous work by modeling the relationship between observed biodiversity metrics and contemporary, global, geospatial layers of human pressures, with the intention of providing a high resolution monitoring product into the future.\n\nBiodiversity intactness is estimated as a combination of two metrics: Abundance, the quantity of individuals, and Compositional Similarity, how similar the composition of species is to an intact baseline. Linear mixed effects models are fit to estimate the predictive capacity of spatial datasets of human pressures on each of these metrics and project results spatially across the globe. These methods, as well as comparisons to other leading datasets and guidance on interpreting results, are further explained in a methods [white paper](https://ai4edatasetspublicassets.blob.core.windows.net/assets/pdfs/io-biodiversity/Biodiversity_Intactness_whitepaper.pdf) entitled “Global 100m Projections of Biodiversity Intactness for the years 2017-2020.”\n\nAll years are available under a Creative Commons BY-4.0 license.\n\"\n        \n    \n                \n            \n                \n                    \n        links[] 7 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"items\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/io-biodiversity/items\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/geo+json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"license\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://creativecommons.org/licenses/by/4.0/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"CC BY 4.0\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            rel\n            \"about\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pdfs/io-biodiversity/Biodiversity_Intactness_whitepaper.pdf\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/pdf\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Technical White Paper\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            \n        \n            \n                \n        \n            rel\n            \"describedby\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/dataset/io-biodiversity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Human readable dataset overview and reference\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            \n        \n            \n                \n        \n            rel\n            \"self\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/io-biodiversity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            \n        \n            \n                \n        \n            rel\n            \"parent\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        stac_extensions[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/item-assets/v1.0.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"https://stac-extensions.github.io/table/v1.2.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            item_assets\n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff; profile=cloud-optimized\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Terrestrial biodiversity intactness at 100m resolution\"\n        \n    \n            \n        \n            \n                \n        raster:bands[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            sampling\n            \"area\"\n        \n    \n            \n        \n            \n                \n        \n            data_type\n            \"float32\"\n        \n    \n            \n        \n            \n                \n        \n            spatial_resolution\n            100\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            msft:region\n            \"westeurope\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:container\n            \"impact\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:storage_account\n            \"pcdata01euw\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:short_description\n            \"Global terrestrial biodiversity intactness at 100m resolution for years 2017-2020\"\n        \n    \n                \n            \n                \n                    \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n                \n            \n                \n                    \n        \n            extent\n            \n        \n            \n                \n        \n            spatial\n            \n        \n            \n                \n        bbox[] 1 items\n        \n            \n        \n            \n                \n        0[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -180\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            -90\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            180\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            90\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            temporal\n            \n        \n            \n                \n        interval[] 1 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2017-01-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2020-12-31T23:59:59Z\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            license\n            \"CC-BY-4.0\"\n        \n    \n                \n            \n                \n                    \n        keywords[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"Global\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"Biodiversity\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        providers[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"Impact Observatory\"\n        \n    \n            \n        \n            \n                \n        roles[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"producer\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"licensor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.impactobservatory.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            name\n            \"Vizzuality\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.vizzuality.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            name\n            \"Microsoft\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"host\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://planetarycomputer.microsoft.com\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            summaries\n            \n        \n            \n                \n        version[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"v1\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            assets\n            \n        \n            \n                \n        \n            thumbnail\n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pc_thumbnails/io-biodiversity-thumb.png\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n            \n        \n            \n                \n        \n            media_type\n            \"image/png\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            geoparquet-items\n            \n        \n            \n                \n        \n            href\n            \"abfs://items/io-biodiversity.parquet\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/x-parquet\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"GeoParquet STAC items\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Snapshot of the collection's STAC items exported to GeoParquet format.\"\n        \n    \n            \n        \n            \n                \n        \n            msft:partition_info\n            \n        \n            \n                \n        \n            is_partitioned\n            False\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            table:storage_options\n            \n        \n            \n                \n        \n            account_name\n            \"pcstacitems\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"stac-items\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n        \n    \n\n\n\nNow that we have our data, we need to specify the temporal and spatial information we are interested in. I will create a bounding box, that specifies our spatial area of interest.\nAs we can see from the description of the Impact Observatory Collection above, the only dates represented in the data are our time range of interest (2017-2020). As such, there is no need to specify a temporal variable.\n\n# Create bounding box\nbbox_of_interest = [-112.826843, 32.974108, -111.184387, 33.863574]\n\n\n# Catalog search\nsearch = catalog.search(\n    collections = ['io-biodiversity'],\n    bbox = bbox_of_interest)\n\n# Get items from search\nitems = search.item_collection()\n\n# Determine number of items in search\nprint(f'There are {len(items)} items in the search.')\n\nThere are 4 items in the search.\n\n\n\n# Retrieve items\nitem_names = {item.id : item for item in search.items()}\nlist(item_names) # from this list, we can see that our 1st item is 2020 data, and our 4th item is 2017 data\n\n# Select 2017 subset\nphx_2017 = items[3]\n\n# Select 2020 subset\nphx_2020 = items[0]\n\nOur search returned four STAC Items. We can tell from their IDs that that they contain data for the same area but for different times, specifically the years 2017 through 2020. Let’s display the available assets and properties for the 2017 Item.\n\nasset_table = rich.table.Table(\"Asset Key\", \"Asset Title\")\nfor key, value in items[-1].assets.items():\n    asset_table.add_row(key, value.title)\nasset_table\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Asset Key        ┃ Asset Title                     ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ data             │ Biodiversity Intactness         │\n│ tilejson         │ TileJSON with default rendering │\n│ rendered_preview │ Rendered preview                │\n└──────────────────┴─────────────────────────────────┘\n\n\n\n\nproperty_table = rich.table.Table(\"Property Name\", \"Property Value\")\nfor key, value in sorted(items[-1].properties.items()):\n    property_table.add_row(key, str(value))\nproperty_table\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Property Name  ┃ Property Value                                                                                 ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ datetime       │ None                                                                                           │\n│ end_datetime   │ 2017-12-31T23:59:59Z                                                                           │\n│ proj:epsg      │ 4326                                                                                           │\n│ proj:shape     │ [7992, 7992]                                                                                   │\n│ proj:transform │ [0.0008983152841195215, 0.0, -115.38597824385106, 0.0, -0.0008983152841195215,                 │\n│                │ 34.74464974521749, 0.0, 0.0, 1.0]                                                              │\n│ start_datetime │ 2017-01-01T00:00:00Z                                                                           │\n└────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\nWe can see from the tables, that the Planetary Computer includes a “Rendered Preview”. Let’s take a look at the 2017 data:\n\n# Plot rendered preview\nImage(url=phx_2017.assets['rendered_preview'].href, width=500)"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#visualization-map-of-phoenix-county-az",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#visualization-map-of-phoenix-county-az",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Visualization: Map of Phoenix County AZ",
    "text": "Visualization: Map of Phoenix County AZ\nIn order to better visualize the Phoenix area, I will make a map, and add in a basemap using the Contextily package. This map will give us a good visual of the greater Phoenix area, and will better help us understand our biodiveristy analysis later.\n\n# Set up the figure\nfig, ax = plt.subplots()\n\n# Create the axis with plot\nphoenix_merc = phoenix.to_crs(epsg=3857)\nphoenix_merc.plot(ax=ax, figsize=(11, 10), alpha=0.45, edgecolor=\"k\", label=\"Phoenix Boundary\")\n\n# Add NatGeo basemap from contextily\nctx.add_basemap(ax=ax, source=ctx.providers.Esri.NatGeoWorldMap)\n\n# Add legend\nlegend_elements = [Line2D([0], [0], color='k', alpha=0.45, lw=3, label='Phoenix County Boundary')]\nax.legend(handles=legend_elements, loc=\"upper left\", fontsize=12)\n\n# Update axes\nax.set_title(\"Phoenix County, Arizona\", fontdict={\"fontsize\": 20})\nax.set_axis_off()\n\nplt.show()"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#biodiversity-intactness-index",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#biodiversity-intactness-index",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Biodiversity Intactness Index",
    "text": "Biodiversity Intactness Index\nNow that we have a good basemap, showing roads and ciites for context, we can work on also plotting our Plantary Computer data, to see biodiversity intactness in this highly urban area.\nRemembering the data tables created above, I know that I am interested the “data” asset, which should contain the information that I need to continue my comparative analysis. Since my analysis will involve directly comparing the years 2017 and 2020, I will create variables defining the assets of those years specifically.\n\n# Retrieve 2017 biodiversity data \nphx_2017_asset = phx_2017.assets[\"data\"]\nphx_2017_data = rioxr.open_rasterio(phx_2017_asset.href)\n\n# Retrieve 2020 biodiversity data \nphx_2020_asset = phx_2020.assets[\"data\"]\nphx_2020_data = rioxr.open_rasterio(phx_2020_asset.href)\n\nphx_2017_data\nphx_2020_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 7992, x: 7992)&gt; Size: 255MB\n[63872064 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 64kB -115.4 -115.4 -115.4 ... -108.2 -108.2 -108.2\n  * y            (y) float64 64kB 34.74 34.74 34.74 34.74 ... 27.57 27.57 27.57\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 7992x: 7992...[63872064 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-115.4 -115.4 ... -108.2 -108.2array([-115.385529, -115.384631, -115.383732, ..., -108.208888, -108.20799 ,\n       -108.207092])y(y)float6434.74 34.74 34.74 ... 27.57 27.57array([34.744201, 34.743302, 34.742404, ..., 27.56756 , 27.566661, 27.565763])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-115.38597824385106 0.0008983152841195215 0.0 34.74464974521749 0.0 -0.0008983152841195215array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([-115.38552908620899, -115.38463077092487, -115.38373245564075,\n       -115.38283414035664, -115.38193582507252,  -115.3810375097884,\n       -115.38013919450427, -115.37924087922015, -115.37834256393603,\n       -115.37744424865191,\n       ...\n       -108.21517648836696, -108.21427817308285, -108.21337985779873,\n       -108.21248154251461, -108.21158322723049, -108.21068491194637,\n       -108.20978659666225, -108.20888828137814, -108.20798996609402,\n        -108.2070916508099],\n      dtype='float64', name='x', length=7992))yPandasIndexPandasIndex(Index([ 34.74420058757543,  34.74330227229131,  34.74240395700719,\n       34.741505641723066,  34.74060732643895,  34.73970901115483,\n        34.73881069587071,  34.73791238058659, 34.737014065302475,\n        34.73611575001835,\n       ...\n        27.57384798973341, 27.572949674449287,  27.57205135916517,\n       27.571153043881047,  27.57025472859693,  27.56935641331281,\n        27.56845809802869,  27.56755978274457,  27.56666146746045,\n        27.56576315217633],\n      dtype='float64', name='y', length=7992))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\nSince there is only 1 band, we can remove that dimension for a more easily managed dataset.\n\nphx_2017_data = phx_2017_data.squeeze().drop_vars('band')\nphx_2020_data = phx_2020_data.squeeze().drop_vars('band')\n\nNow, I need to clip the Planetary Computer data to match the Phoenix polygon I created earlier. I will make sure that my Coordinate Reference Systems match, before clipping and plotting.\n\n# Match CRSs\nphoenix = phoenix.to_crs(phx_2017_data.rio.crs)\n\n# Assert check to ensure it worked\nassert phoenix.crs == phx_2017_data.rio.crs\n\n# Clip 2017 BII raster to the Phoenix polygon\nphx_clip_2017 = phx_2017_data.rio.clip(phoenix[\"geometry\"])\n\n# And again for 2020\nphx_clip_2020 =  phx_2020_data.rio.clip(phoenix[\"geometry\"])\n\n# Let's see how that looks for 2017\nphx_clip_2017.plot()"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#calculating-the-percentage-of-area-of-the-phoenix-subdivision-with-a-bii-of-at-least-0.75-in-2017-and-2020.",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#calculating-the-percentage-of-area-of-the-phoenix-subdivision-with-a-bii-of-at-least-0.75-in-2017-and-2020.",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Calculating the percentage of area of the Phoenix subdivision with a BII of at least 0.75 in 2017 and 2020.",
    "text": "Calculating the percentage of area of the Phoenix subdivision with a BII of at least 0.75 in 2017 and 2020.\nIn order to do this calculation, I need to do a bit of “map algebra”.\n\nI need to find the total number of pixels in the Phoenix subdivision polygon, for both years.\nThen, I need to find the numbers of pixels with the BII at 0.75 or higher, again for both years.\nFinally, I can use those pixel numbers I get to calculate the desired percentage.\n\n\n# Calculate total area for 2017\ntotal_2017 = phx_clip_2017.count().item() \n# Calculate total area for 2020\ntotal_2020 = phx_clip_2020.count().item()\n\n# Calculate BII % for 2017, Make the data binary for easy math\nphx_bii_2017 = (phx_clip_2017 &gt;= 0.75).astype(int)\npixels_2017 = phx_bii_2017.sum().item()\n\n# Calculate BII % for 2020\nphx_bii_2020 = (phx_clip_2020 &gt;= 0.75).astype(int)\npixels_2020 = phx_bii_2020.sum().item()\n\n\n\n# Calcualte the percentage area for 2017 data:\npct_bii_2017 =  (pixels_2017 / total_2017) * 100\n\n# Calculate the percentage area for 2020 data:\npct_bii_2020 = (pixels_2020 / total_2020) * 100\n\n# Print output\npct_text = \"The percentage of area in Phoenix County with a BII over 0.75 in\"\nprint(pct_text, \"2017 is: \", round(pct_bii_2017, 2), \"%\")\n\nprint(pct_text, \"2020 is: \", round(pct_bii_2020, 2), \"%\")\n\nThe percentage of area in Phoenix County with a BII over 0.75 in 2017 is:  7.13 %\nThe percentage of area in Phoenix County with a BII over 0.75 in 2020 is:  6.49 %"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#visualization-biodiversity-loss",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#visualization-biodiversity-loss",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Visualization: Biodiversity Loss",
    "text": "Visualization: Biodiversity Loss\nNow that we have calculated the differences in biodiversity density between the two years, we can visualize this difference with a map.\n\n# Find areas where there was a loss in BII from 2017 to 2020\nphx_bii_diff = phx_bii_2017 - phx_bii_2020\n\n# Make a mask - like in 223!\nphx_mask = phx_bii_diff == 1\n\n# Convert to int type\nphx_mask = phx_mask.astype(int)\n\n# Make a color gradient\ncolor_id = [\"none\", \"red\"]\n\n# Make a color map object\ncmap = plt.cm.colors.ListedColormap(color_id)\n\n# Plot!\nphx_mask.plot(cmap = cmap, add_colorbar = False)\n\n\n\n\n\n\n\n\nNow that we have applied a color map visualization to our biodiversity loss, we can overlay this with our Planetary Computer raster, and our Phoenix subdivision polygon, to create a final visualization that shows us the area, in red, where Phoenix county experienced significant biodiversity losee."
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#final-visualization",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#final-visualization",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Final Visualization:",
    "text": "Final Visualization:\n\n# Initialize plot\nfig, ax = plt.subplots(figsize = (8, 7))\n\n# Remove axes\nax.axis(\"off\")\n\n# Begin with clipped raster layer\nphx_clip_2020.plot(ax = ax,\n                  cbar_kwargs = {\"location\": \"bottom\",\n                                \"label\": \"BII in 2020\"})\n# Add mask layer\nphx_mask.plot(ax = ax,\n             cmap = cmap,\n             add_colorbar = False)\nphx_mask_patch = mpatches.Patch(color = \"red\",\n                      label = \"Area with 75% Biodiversity Loss between 2017 and 2020\")\n# Add polygon layer\nphoenix.plot(ax = ax, \n            color = \"none\",\n            edgecolor = \"black\",\n            linewidth = 2)\n\n# update legend, add in BII loss patch\nax.legend(handles = [phx_mask_patch],\n          frameon = False,\n          bbox_to_anchor = (0.85, -0.07))\nax.set_title(\"Biodiversity Intactness Index (BII) and Biodiversity Loss in Phoenix, AZ\")\n\nplt.show()"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#conclusion",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#conclusion",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Conclusion",
    "text": "Conclusion\nPython and Jupyter Notebooks, as we can see, are powerful tools that allow us to conduct high level analyis with large amounts of spatial data.\nIn this analysis, we can use these tools to visualize the sharp decrease in biodiversity in this area, perhaps due to urbanization and human impact. Such an analysis can be used to further understand the overall biodiversity crisis, and can help inform decision makers about the needs and priorities of conservation efforts."
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#citations-and-data-access",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#citations-and-data-access",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Citations and Data Access:",
    "text": "Citations and Data Access:\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nMicrosoft Planetary Computer, STAC Catalog. Biodiversity Intactness (‘io-biodiversity’). [Dataset]. https://planetarycomputer.microsoft.com/dataset/io-biodiversity Accessed 2 December 2024.\nUnited States Census Bureau. (2022). Arizona County Subdivision 2022 TIGER/Line Shapefiles. [Data File]. U.S. Census Bureau, Geography Division. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions Accessed 2 December 2024."
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#background",
    "href": "posts/2024-11-8-landcover-classification/index.html#background",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Background",
    "text": "Background\nMonitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance.\nClassifying remotely sensed imagery into land cover classes enables us to understand the distribution and change in land cover types over large areas.\nThere are many approaches for performing land cover classification:\n\nSupervised approaches use training data labeled by the user\nUnsupervised approaches use algorithms to create groups which are identified by the user afterward"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#game-plan",
    "href": "posts/2024-11-8-landcover-classification/index.html#game-plan",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Game Plan",
    "text": "Game Plan\nIn this exercise, I am using a form of supervised classification – a decision tree classifier.\nDecision trees classify pixels using a series of conditions based on values in spectral bands. These conditions (or decisions) are developed based on training data.\nI will create a land cover classification for southern Santa Barbara County based on multi-spectral imagery and data on the location of 4 land cover types:\n\ngreen vegetation\ndry grass or soil\nurban\nwater\n\nTo do so, I will need to:\n\nLoad and process Landsat scene\nCrop and mask Landsat data to study area\nExtract spectral data at training sites\nTrain and apply decision tree classifier\nPlot results"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#data-details",
    "href": "posts/2024-11-8-landcover-classification/index.html#data-details",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Data Details",
    "text": "Data Details\nTo conduct this analysis, I will use the Landsat 5 Thematic Mapper data. More information about these data can be found at this link: https://www.usgs.gov/landsat-missions/landsat-5.\nSpecifically, I will be using 1 scene from September 25, 2007 (my birthday!), on bands 1,2, 3, 4, 5, 7, with collection 2 surface reflectance product.\nData files:\n\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B1.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B2.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B3.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B4.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B5.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B7.tif\n\nStudy area:\nI will be using a polygon representing southern Santa Barbara county, the county in which I am currently attending school.\nData file:\n\nSB_county_south.shp\n\nTraining data:\nAnd finally, I will be using a data file with polygons representing sites with training data. Specifically, I will be using the data character string with land cover type.\nData file:\n\ntrainingdata.shp\n\nAll of the data used in this study were accessed on November 25th, 2024."
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#workflow",
    "href": "posts/2024-11-8-landcover-classification/index.html#workflow",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Workflow",
    "text": "Workflow\n\n1. Set up\nTo train our classification algorithm and plot the results, I will use the rpart and rpart.plot packages.\n\n\nCode\n# install.packages(\"rpart\")\n# install.packages(\"rpart.plot\")\n\n\nLet’s load all necessary packages:\n\n\n2. Load Landsat data\nLet’s create a raster stack. Each file name ends with the band number (e.g. B1.tif).\nI am missing a file for band 6, but, this is intentional. Band 6 corresponds to thermal data, which I will not be working with during this exercise.\nTo create a raster stack, I will create a list of the files that I would like to work with and read them all in at once using the terra::rast() function. I will then update the names of the layers to match the spectral bands and plot a true color image to see what we’re working with.\n\n\nCode\n# list files for each band, including the full file path\nfilelist &lt;- list.files(here::here(\"posts\", \"2024-11-8-landcover-classification\", \"data\", \"landsat-data\"), full.names = TRUE)\n\n# read in and store as a raster stack\nlandsat &lt;- rast(filelist)\n\n# update layer names to match band\nnames(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\n# plot true color image\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\n\n\n3. Load study area\nI want to constrain our analysis to the southern portion of the county where we have training data, so I’ll read in a file that defines the area I would like to study.\n\n\nCode\n# read in shapefile for southern portion of SB county\nSB_county_south &lt;- st_read(here::here(\"posts\", \"2024-11-8-landcover-classification\", \"data\", \"SB_county_south.shp\")) %&gt;%\n      st_transform(SB_county_south, crs = crs(landsat))\n\n\nReading layer `SB_county_south' from data source \n  `C:\\MEDS\\jorb1.github.io\\posts\\2024-11-8-landcover-classification\\data\\SB_county_south.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -120.2327 ymin: 34.33603 xmax: -119.5757 ymax: 34.53716\nGeodetic CRS:  NAD83\n\n\nCode\n# Plot the shapefile\ntm_shape(SB_county_south) +\n  tm_borders()\n\n\n\n\n\n\n\n\n\n\n\n4. Crop and mask Landsat data to study area\nNow, I can crop and mask the Landsat data to the study area.\n\nWhy? This reduces the amount of data we’ll be working with and therefore saves computational time\nBonus: We can also remove any objects we’re no longer working with to save space\n\n\n\nCode\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped &lt;- crop(landsat, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked &lt;- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat, SB_county_south, landsat_cropped)\n\n# Plot!\nplotRGB(landsat_masked, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\n\n\n5. Convert Landsat values to reflectance\nNow I need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any scaling factors to convert to reflectance.\nIn this case, I are working with Landsat Collection 2.\nThe valid range of pixel values for this collection goes from 7,273 to 43,636… - with a multiplicative scale factor of 0.0000275 - with an additive scale factor of -0.2\nLet’s reclassify any erroneous values as NA and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%!\n\n\nCode\n# reclassify erroneous values as NA\nrcl &lt;- matrix(c(-Inf, 7273, NA,\n                 43636, Inf, NA), ncol = 3, byrow = TRUE)\n\nlandsat &lt;- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\nlandsat &lt;- (landsat * 0.0000275 - 0.2) * 100\n\n# check values are 0 - 100\nsummary(landsat)\n\n\n      blue           green            red             NIR       \n Min.   : 1.11   Min.   : 0.74   Min.   : 0.00   Min.   : 0.23  \n 1st Qu.: 2.49   1st Qu.: 2.17   1st Qu.: 1.08   1st Qu.: 0.75  \n Median : 3.06   Median : 4.59   Median : 4.45   Median :14.39  \n Mean   : 3.83   Mean   : 5.02   Mean   : 4.92   Mean   :11.52  \n 3rd Qu.: 4.63   3rd Qu.: 6.76   3rd Qu.: 7.40   3rd Qu.:19.34  \n Max.   :39.42   Max.   :53.32   Max.   :56.68   Max.   :57.08  \n NA's   :39856   NA's   :39855   NA's   :39855   NA's   :39856  \n     SWIR1           SWIR2      \n Min.   : 0.10   Min.   : 0.20  \n 1st Qu.: 0.41   1st Qu.: 0.60  \n Median :13.43   Median : 8.15  \n Mean   :11.88   Mean   : 8.52  \n 3rd Qu.:18.70   3rd Qu.:13.07  \n Max.   :49.13   Max.   :48.07  \n NA's   :42892   NA's   :46809  \n\n\n\n\n6. Training classifier\nLet’s begin by extracting reflectance values for training data!\nWe will load the shapefile identifying locations within our study area as containing one of our 4 land cover types.\n\n\nCode\n# read in and transform training data\ntraining_data &lt;- st_read(here::here( \"posts\", \"2024-11-8-landcover-classification\", \"data\", \"trainingdata.shp\")) %&gt;%\n  st_transform(., crs = crs(landsat))\n\n\nReading layer `trainingdata' from data source \n  `C:\\MEDS\\jorb1.github.io\\posts\\2024-11-8-landcover-classification\\data\\trainingdata.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 40 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 215539.2 ymin: 3808948 xmax: 259927.3 ymax: 3823134\nProjected CRS: WGS 84 / UTM zone 11N\n\n\nNow, we can extract the spectral reflectance values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\nCode\n# extract reflectance values at training sites\ntraining_data_values &lt;- terra::extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes &lt;- training_data %&gt;%\n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\nSB_training_data &lt;- left_join(training_data_values, training_data_attributes,\n                              by = c(\"ID\" = \"id\")) %&gt;%\n                    mutate(type = as.factor(type)) # convert landcover type to factor\n\n\nNext, let’s train the decision tree classifier!\nTo train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are).\n\nThe rpart() function implements the CART algorithm\nThe rpart() function needs to know the model formula and training data you would like to use\nBecause we are performing a classification, we set method = “class”\nWe also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\n\n\nCode\n# Establish model formula\nSB_formula &lt;- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# Train decision tree\nSB_decision_tree &lt;- rpart(formula = SB_formula,\n                          data = SB_training_data, \n                          method = \"class\",\n                          na.action = na.omit)\n\n\nTo understand how the decision tree will classify pixels, I can plot the results!\n\n\nCode\n# plot decision tree\nprp(SB_decision_tree)\n\n\n\n\n\n\n\n\n\n\n\n7. Classify image\nNow that I have a rule set for classifying spectral reflectance values into landcover types, I can apply the classifier to identify the landcover type in each pixel.\nThe terra package includes a predict() function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The predict() function will return a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data.\n\n\nCode\n# classify image based on decision tree\nSB_classification &lt;- terra::predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nlevels(SB_training_data$type)\n\n\n[1] \"green_vegetation\" \"soil_dead_grass\"  \"urban\"            \"water\"           \n\n\n\n\n8. Plot results\nFinally, I can plot the results and check out our land cover map!\n\n\nCode\n# Plot results\ntm_shape(SB_classification) + \n  tm_raster(palette = c(\"#8DB580\", \"#F2DDA4\", \"grey\", \"cornflowerblue\"),\n            labels = c(\"green vegetation\",\n                       \"soil/dead grass\",\n                       \"urban\",\n                       \"water\"),\n            title = \"Land cover type\") +\n  tm_layout(legend.position = c(\"left\", \"bottom\"),\n            main.title = \"Santa Barbara Land Cover\")"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#conclusion",
    "href": "posts/2024-11-8-landcover-classification/index.html#conclusion",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Conclusion:",
    "text": "Conclusion:\nWorking with Landsat data is fun! And it allows us to run analysis regarding landcover types, with the magic of R."
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#acknowledements",
    "href": "posts/2024-11-8-landcover-classification/index.html#acknowledements",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Acknowledements:",
    "text": "Acknowledements:\nMaterial for this exercise was taken from Ruth Oliver’s EDS 223: Geospatial Analysis Course at the University of Santa Barbara’s Masters of Environmental Data Science program. Thank you, Ruth!"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#citations",
    "href": "posts/2024-11-8-landcover-classification/index.html#citations",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Citations:",
    "text": "Citations:\nR. Oliver, EDS 223 - Geospatial Analysis and Remote Sensing, Course Notes. 2024. [Online]. Available: https://eds-223-geospatial.github.io/\nU.S. Geological Survey. (n.d.). Landsat 5. Retrieved December 7, 2024, from https://www.usgs.gov/landsat-missions/landsat-5"
  },
  {
    "objectID": "posts/2024-12-20-aquaculture/index.html",
    "href": "posts/2024-12-20-aquaculture/index.html",
    "title": "Prioritizing Potential Aquaculture Zones on the West Coast of the United States",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production. Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, and bottom depth. They found that global seafood demand could be met using less than 0.015% of the global ocean area.\nFor this analysis, I will determine which Exclusive Economic Zones (EEZ) on the West Coast of the United States are best suited to developing marine aquaculture for several species of Oysters and White Shrimp. Suitable locations will be determined based on a range of suitable sea surface temperatures (SST) and depth values for the species.\nData on species depth and temperature requirements came from SeaLifeBase.\nData on sea surface temperatures came from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\nBathymetry data came from the General Bathymetric Chart of the Oceans (GEBCO).\nData on Exclusive Economic Zones came from Marineregions.org.\n\n\nCode\n# Transform the CRS of the files to match\nbathymetry &lt;- terra::project(bathymetry, crs(sst_stack))\n\nmaritime_boundaries &lt;- maritime_boundaries %&gt;% \n  st_transform(crs = st_crs(bathymetry))\n\n# Confirm that the CRS transformed as expected\ntest_that(\"The CRS of all data sets match\", {\n  expect_true(crs(sst_stack) == crs(bathymetry) && crs(bathymetry) == crs(maritime_boundaries))\n})\n\n\nTest passed 🥇"
  },
  {
    "objectID": "posts/2024-12-20-aquaculture/index.html#background",
    "href": "posts/2024-12-20-aquaculture/index.html#background",
    "title": "Prioritizing Potential Aquaculture Zones on the West Coast of the United States",
    "section": "",
    "text": "Marine aquaculture has the potential to play an important role in the global food supply as a more sustainable protein option than land-based meat production. Gentry et al. mapped the potential for marine aquaculture globally based on multiple constraints, including ship traffic, dissolved oxygen, and bottom depth. They found that global seafood demand could be met using less than 0.015% of the global ocean area.\nFor this analysis, I will determine which Exclusive Economic Zones (EEZ) on the West Coast of the United States are best suited to developing marine aquaculture for several species of Oysters and White Shrimp. Suitable locations will be determined based on a range of suitable sea surface temperatures (SST) and depth values for the species.\nData on species depth and temperature requirements came from SeaLifeBase.\nData on sea surface temperatures came from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\nBathymetry data came from the General Bathymetric Chart of the Oceans (GEBCO).\nData on Exclusive Economic Zones came from Marineregions.org.\n\n\nCode\n# Transform the CRS of the files to match\nbathymetry &lt;- terra::project(bathymetry, crs(sst_stack))\n\nmaritime_boundaries &lt;- maritime_boundaries %&gt;% \n  st_transform(crs = st_crs(bathymetry))\n\n# Confirm that the CRS transformed as expected\ntest_that(\"The CRS of all data sets match\", {\n  expect_true(crs(sst_stack) == crs(bathymetry) && crs(bathymetry) == crs(maritime_boundaries))\n})\n\n\nTest passed 🥇"
  },
  {
    "objectID": "posts/2024-12-20-aquaculture/index.html#data-processing",
    "href": "posts/2024-12-20-aquaculture/index.html#data-processing",
    "title": "Prioritizing Potential Aquaculture Zones on the West Coast of the United States",
    "section": "Data Processing",
    "text": "Data Processing\nIn order to perform this analysis, the data must be processed. Specifically, I must ensure that the Sea Surface Temperature data and Depth data can be combined, since the data have different resolutions, extents, and positions.\n\n\nCode\n# Find the mean SST from 2008-2012 (eg create single raster of average SST...)\nsst_average &lt;- mean(sst_stack)\n\n# Convert average SST from Kelvin to Celsius by subtracting 273.15\nsst_average_celsius &lt;- sst_average - 273.15\n\n\nAfter some initial processing of the SST raster stack, I want to take a quick look at the the bathymetry and SST data, to see what further processing might need to be done.\n\n\nCode\n# Plot bathymetry raster\nplot_bathymetry &lt;- ggplot() +\n  geom_spatraster(data = bathymetry) +\n  scale_fill_viridis_c(name = \"Depth\") +\n  ggtitle(\"Bathymetry\") +\n  theme_minimal()\n\n# Plot SST raster\nplot_sst &lt;- ggplot() +\n  geom_spatraster(data = sst_average_celsius) +\n  scale_fill_viridis_c(name = \"Temperature (°C)\") +\n  ggtitle(\"Average SST (°C)\") +\n  theme_minimal()\n\n# Combine the plots\nplot_bathymetry + plot_sst\n\n\n\n\n\n\n\n\n\nI know from the metadata that the resolutions of these raster objects do not match, so I decide to resample the bathymetry data to match the resolution of the SST data, using the nearest neighbor approach. Once the resolutions and CRS of the data matches, I can crop the bathymetry raster to match the extent of the SST raster.\n\nCode\n# Resample the bathymetry data to match the resolution of the SST data using the nearest neighbor approach\nbathymetry_resampled &lt;- resample(bathymetry, sst_stack, method = \"near\")\n\n# Verify the results\ntest_that(\"The Resolutions of the Rasters match\", {\n    expect_true(res(bathymetry_resampled)[1] == res(sst_average_celsius)[1]) && \n      (res(bathymetry_resampled)[2] == res(sst_average_celsius)[2])\n  })\n\nTest passed 🌈\n\nCode\n# Crop the bathymetry raster at last\nbathymetry_cropped &lt;- crop(bathymetry_resampled, sst_average_celsius)\n\nNow, to verify that my processing thus far has been successful, I will run a series of tests. One way to know that the rasters match, is if they are able to be successfully stacked together. This code will test for stackability, and then I will finally stack the two together.\n\nCode\ntest_that(\"The resolutions of the cropped rasters match\", {\n  expect_true(all(res(bathymetry_cropped) == res(sst_average_celsius)))\n})\n\nTest passed 🌈\n\nCode\ntest_that(\"The rasters have the same extent\", {\n  expect_true(ext(bathymetry_cropped) == ext(sst_average_celsius))\n})\n\nTest passed 🥇\n\nCode\ntest_that(\"The rasters have the same CRS\", {\n  expect_true(crs(bathymetry_cropped) == crs(sst_average_celsius))\n})\n\nTest passed 🎊\n\nCode\n# Finally, lets do it! Stack that raster!\nbathy_sst &lt;- c(bathymetry_cropped, sst_average_celsius)"
  },
  {
    "objectID": "posts/2024-12-20-aquaculture/index.html#oysters-a-pearl-fect-match",
    "href": "posts/2024-12-20-aquaculture/index.html#oysters-a-pearl-fect-match",
    "title": "Prioritizing Potential Aquaculture Zones on the West Coast of the United States",
    "section": "Oysters: A “Pearl-fect” Match",
    "text": "Oysters: A “Pearl-fect” Match\nAfter the data is properly prepared and processed, I can begin the analysis to find suitable locations for marine aquaculture. This means finding locations that are suitable in terms of both SST and depth.\nI will begin my analysis with oysters. Research has shown that oysters need the following conditions for optimal growth:\n\nsea surface temperature: 11-30°C\ndepth: 0-70 meters below sea level\n\nWith this in mind, I will first reclassify the SST data to only include raster values that are between 11 and 30 degrees Celsius. Using a similar process, I will reclassify the depth data to only include values that are above 70 meters below sea level. Let’s see what that looks like:\n\n\nCode\n# Turn unneeded values to NA\noptimal_sst &lt;- sst_average_celsius\noptimal_sst[optimal_sst &lt; 11 | optimal_sst &gt; 30] &lt;- NA\n\n# Do it again for the bathymetry data\noptimal_bath &lt;- bathymetry_cropped\noptimal_bath[optimal_bath &lt; -70 | optimal_bath &gt; 0] &lt;- NA\n\n# Plot optimal bathymetry raster\nplot_optimal_bath &lt;- ggplot() +\n  geom_spatraster(data = optimal_bath) +\n  scale_fill_viridis_c(name = \"Depth (m)\") +\n  ggtitle(\"Optimal Depth (m) for Oysters\") +\n  theme_minimal()\n\n# Plot optimal SST raster\nplot_optimal_sst &lt;- ggplot() +\n  geom_spatraster(data = optimal_sst) +\n  scale_fill_viridis_c(name = \"Temperature (°C)\") +\n  ggtitle(\"Optimal SST (°C) for Oysters\") +\n  theme_minimal()\n\n# Combine the plots\nplot_optimal_bath + plot_optimal_sst\n\n\n\n\n\n\n\n\n\nTo find suitable locations for oyster marine aquaculture, we’ll need to find locations that are suitable in terms of both SST and depth. Let’s do it with code!\n\n\nCode\n# Use map algebra to find areas of overlap\noverlap &lt;- !is.na(optimal_sst) & !is.na(optimal_bath)\n\nbig_optimal &lt;- optimal_sst * overlap + optimal_bath * overlap\nplot(big_optimal, main = \"Area Optimal for Oyster Growth\")\n\n\n\n\n\n\n\n\n\nThe result we get is a map showing a relatively thin line representing the areas of the West Coast of the United States that have the most optimal conditions for oyster aquaculture.\n\nDetermine the most suitable EEZ (Exclusive Economic Zone) best suited for developing marine aquaculture for Oysters.\nNow, its time to bring in our EEZ data, to determine which EEZs best overlap with our zones of optimal conditions for oysters.\nFirst, lets take a look at the maritime boundaries data that we have.\n\n\nCode\n# Map the EEZs\ntm_shape(maritime_boundaries) +\n  tm_polygons(col = \"rgn\",\n              palette = \"RdYlBu\",\n              title = \"Region Name\") +\n  tm_layout(main.title = \"West Coast EEZs\",\n            main.title.position = \"center\",\n            legend.outside = TRUE) +\n  tm_shape(us_states) +\n    tm_polygons(alpha = .20) \n\n\n\n\n\n\n\n\n\nNow the question remains: what percent of each EEZ region contains suitable area for oyster cultivation? Using the optimal oyster conditions we already calculated, as well as the EEZ information we have in our dataframe, we can calculate this.\nOur final product is a map of visualizations thus far, and a nice table that presents the final numbers from our analysis.\n\n\nCode\n# Let's make a plot!\n\n# Map both raster data sets\nfinal_map &lt;- tm_shape(maritime_boundaries) +\n  tm_polygons(col = \"rgn\",\n              palette = \"Blues\",\n              alpha = .60,\n              title = \"Region Name\") +\n  tm_layout() +\n  tm_shape(us_states) +\n    tm_polygons(alpha = .20) +  \n   tm_shape(big_optimal) +\n     tm_raster(legend.show = TRUE,\n               title = \"Total Suitable\\nArea (km^2)\",\n               palette = \"red\",\n               breaks = seq(0, 5, by = 5),\n               labels = \" \") +\n     tm_layout(main.title = \"EEZs with Optimal\\nOyster Habitat\",\n               main.title.position = \"center\",\n               legend.outside = TRUE)\nprint(final_map)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Rasterize the eez dataframe using the 'rgn' column\neez_rast &lt;- terra::rasterize(maritime_boundaries, big_optimal, field = \"rgn\")\n\n# Create dataframe that displays suitable area of each zone\nsuitable_area &lt;- terra::expanse(big_optimal, unit = \"km\", zones = eez_rast)\n\n# Rename columns\nsuitable_area &lt;- suitable_area %&gt;% \n  rename(\"suitable_area_km2\" = \"area\",\n         \"rgn\" = \"zone\")\n\n# Calculate percent of each zone that contains suitable area for oyster aquaculture \neez_oyster &lt;- maritime_boundaries %&gt;% \n  left_join(suitable_area, by= \"rgn\") %&gt;% \n  mutate(zone_pct = suitable_area_km2 / area_km2 * 100) %&gt;% \n  select(\"rgn\", \"area_km2\", \"suitable_area_km2\", \"zone_pct\") %&gt;% \n  st_drop_geometry()\n\n# Make a fun table that shows this estimate\nfinal_table &lt;- eez_oyster |&gt;\n  gt() |&gt;\n  gt_theme_nytimes() |&gt;\n  tab_header(title = \"EEZ Aquaculture Calculations\") |&gt;\n  cols_label(\n    rgn = \"EEZ REgion\",\n    area_km2 = \"Square Kilometers of EEZ\",\n    suitable_area_km2 = \"Square Kilometers of Optimal Area\",\n    zone_pct = \"Percentage of EEZ Suitable for Aquaculture\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body()\n  )\n\nfinal_table\n\n\n\n\n\n\n\n\nEEZ Aquaculture Calculations\n\n\nEEZ REgion\nSquare Kilometers of EEZ\nSquare Kilometers of Optimal Area\nPercentage of EEZ Suitable for Aquaculture\n\n\n\n\nOregon\n179994.06\n1120.146\n0.6223238\n\n\nNorthern California\n164378.81\n194.169\n0.1181229\n\n\nCentral California\n202738.33\n4086.765\n2.0157832\n\n\nSouthern California\n206860.78\n3882.136\n1.8766904\n\n\nWashington\n66898.31\n2452.143\n3.6654784"
  },
  {
    "objectID": "posts/2024-12-20-aquaculture/index.html#aquaculture-made-shrimple",
    "href": "posts/2024-12-20-aquaculture/index.html#aquaculture-made-shrimple",
    "title": "Prioritizing Potential Aquaculture Zones on the West Coast of the United States",
    "section": "Aquaculture made “Shrimple”",
    "text": "Aquaculture made “Shrimple”\nAccording to Boyd et. all, one of the most common species of shrimp harvested using aquaculture is Penaeus setiferus, or White Shrimp. According to SeaLifeBase, this shrimp species requires the following conditions for optimal growth:\n\nsea surface temperature: 15.3-27.3°C\ndepth: 0-119 meters below sea level\n\nI want to run the same analysis for shrimps, that I did for oysters. In order to do that, I will create a function that utilizes the same steps above, but would only require me to input the parameters of the analysis, and result in a comprehensive map and table.\nFirst I will create the function, called optimal_aquaculture:\n\n\nCode\noptimal_aquaculture &lt;- function(sst_data, bathy_data, eez_data, species, min_temp, max_temp, min_depth, max_depth) {\n  # Transform the CRS of the files to match\n  bathymetry &lt;- terra::project(bathy_data, crs(sst_data))\n\n  maritime_boundaries &lt;- eez_data %&gt;% \n    st_transform(crs = st_crs(bathymetry))\n\n  # Confirm that the CRS transformed as expected\n  test_that(\"The CRS of all data sets match\", {\n    expect_true(crs(sst_data) == crs(bathymetry) && crs(bathymetry) == crs(maritime_boundaries))\n  })\n\n  # Find the mean SST from 2008-2012 (e.g., create single raster of average SST)\n  sst_average &lt;- mean(sst_data)\n\n  # Convert average SST from Kelvin to Celsius by subtracting 273.15\n  sst_average_celsius &lt;- sst_average - 273.15\n\n  # Resample the bathymetry data to match the resolution of the SST data using the nearest neighbor approach\n  bathymetry_resampled &lt;- resample(bathymetry, sst_average_celsius, method = \"near\")\n\n  test_that(\"The Resolutions of the Rasters match\", {\n    expect_true(res(bathymetry_resampled)[1] == res(sst_average_celsius)[1]) && \n      (res(bathymetry_resampled)[2] == res(sst_average_celsius)[2])\n  })\n\n  # Crop the bathymetry raster\n  bathymetry_cropped &lt;- crop(bathymetry_resampled, sst_average_celsius)\n\n  test_that(\"The resolutions of the cropped rasters match\", {\n    expect_true(all(res(bathymetry_cropped) == res(sst_average_celsius)))\n  })\n\n  test_that(\"The rasters have the same extent\", {\n    expect_true(ext(bathymetry_cropped) == ext(sst_average_celsius))\n  })\n\n  test_that(\"The rasters have the same CRS\", {\n    expect_true(crs(bathymetry_cropped) == crs(sst_average_celsius))\n  })\n\n  # As a final check, stack the rasters\n  bathy_sst &lt;- c(bathymetry_cropped, sst_average_celsius)\n\n  # Turn unneeded values to NA\n  optimal_sst &lt;- sst_average_celsius\n  optimal_sst[optimal_sst &lt; min_temp | optimal_sst &gt; max_temp] &lt;- NA\n\n  # Do it again for the bathymetry data\n  optimal_bath &lt;- bathymetry_cropped\n  optimal_bath[optimal_bath &lt; max_depth | optimal_bath &gt; min_depth] &lt;- NA\n\n  # Use map algebra to find areas of overlap\n  overlap &lt;- !is.na(optimal_sst) & !is.na(optimal_bath)\n  big_optimal &lt;- optimal_sst * overlap + optimal_bath * overlap\n\n  # Map both raster data sets\n  final_map &lt;- tm_shape(eez_data) +\n    tm_polygons(col = \"rgn\",\n                palette = \"Blues\",\n                alpha = .60,\n                title = \"Region Name\") +\n    tm_layout() +\n    tm_shape(us_states) +\n      tm_polygons(alpha = .20) +  \n    tm_shape(big_optimal) +\n      tm_raster(legend.show = TRUE,\n                title = \"Total Suitable\\nArea (km^2)\",\n                palette = \"red\",\n                breaks = seq(0, 5, by = 5),\n                labels = \" \") +\n      tm_layout(main.title = paste(\"EEZs with Optimal\\n\", species, \"\\nHabitat\"),\n                main.title.size = .90,\n                main.title.position = \"center\",\n                legend.outside = TRUE)\n  print(final_map)\n\n  # Rasterize the EEZ dataframe using the 'rgn' column\n  eez_rast &lt;- terra::rasterize(eez_data, big_optimal, field = \"rgn\")\n\n  # Create a dataframe that displays the suitable area of each zone\n  suitable_area &lt;- terra::expanse(big_optimal, unit = \"km\", zones = eez_rast)\n\n  # Rename columns\n  suitable_area &lt;- suitable_area %&gt;% \n    rename(\"suitable_area_km2\" = \"area\",\n           \"rgn\" = \"zone\")\n\n  # Calculate the percent of each zone that contains suitable area for oyster aquaculture \n  eez_species &lt;- eez_data %&gt;% \n    left_join(suitable_area, by = \"rgn\") %&gt;% \n    mutate(zone_pct = suitable_area_km2 / area_km2 * 100) %&gt;% \n    select(\"rgn\", \"area_km2\", \"suitable_area_km2\", \"zone_pct\") %&gt;% \n    st_drop_geometry()\n\n  # Create a table to show this estimate\n  final_table &lt;- eez_species |&gt;\n    gt() |&gt;\n    gt_theme_nytimes() |&gt;\n    tab_header(title = paste(\"EEZ Aquaculture Calculations for\", species)) |&gt;\n    cols_label(\n      rgn = \"EEZ Region\",\n      area_km2 = \"Square Kilometers of EEZ\",\n      suitable_area_km2 = \"Square Kilometers of Optimal Area\",\n      zone_pct = \"Percentage of EEZ Suitable for Aquaculture\"\n    ) |&gt;\n    tab_style(\n      style = cell_text(weight = \"bold\"),\n      locations = cells_body()\n    )\n\n  return(final_table)\n}\n\n\nAnd then I will use it to run an analysis of optimal conditions for White Shrimp aquaculture:\n\n\nCode\noptimal_aquaculture(sst_data = sst_stack, bathy_data = bathymetry, eez_data = maritime_boundaries, species = \"White Shrimp\", min_temp = 15.3, max_temp = 27.3, min_depth = 0, max_depth = -119)\n\n\nTest passed 🎉\nTest passed 🥳\nTest passed 😀\nTest passed 🥳\nTest passed 🥇\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEEZ Aquaculture Calculations for White Shrimp\n\n\nEEZ Region\nSquare Kilometers of EEZ\nSquare Kilometers of Optimal Area\nPercentage of EEZ Suitable for Aquaculture\n\n\n\n\nOregon\n179994.06\nNA\nNA\n\n\nNorthern California\n164378.81\nNA\nNA\n\n\nCentral California\n202738.33\nNA\nNA\n\n\nSouthern California\n206860.78\n3292.186\n1.591498\n\n\nWashington\n66898.31\nNA\nNA\n\n\n\n\n\n\n\nWhile obviously useful to analyzing spatial data for shrimp, this function can be used for any variety of cultivated seafoods, so long as the parameters are known and the data is available. This White Shrimp analysis shows us that optimal conditions for the cultivation of this food can only happen in Southern California. Information such as this can lead to the prioritization and higher likelihood of success for aquaculture planning and projects."
  },
  {
    "objectID": "posts/2024-12-20-aquaculture/index.html#conclusion",
    "href": "posts/2024-12-20-aquaculture/index.html#conclusion",
    "title": "Prioritizing Potential Aquaculture Zones on the West Coast of the United States",
    "section": "Conclusion",
    "text": "Conclusion\nAquaculture will continue to have a vital role to play in food cultivation, especially in the face of modern climate stressors and human population demands. Analysis such as this one showcase the important role that data science and spatial data analysis have to play in understanding this important food growing practice."
  },
  {
    "objectID": "posts/2024-12-20-aquaculture/index.html#citations",
    "href": "posts/2024-12-20-aquaculture/index.html#citations",
    "title": "Prioritizing Potential Aquaculture Zones on the West Coast of the United States",
    "section": "Citations",
    "text": "Citations\nBoyd, Claude E., and Jason W. Clay. “Shrimp Aquaculture and the Environment.” Scientific American 278, no. 6 (1998): 58–65. http://www.jstor.org/stable/26057855.\nHall, S. J., Delaporte, A., Phillips, M. J., Beveridge, M. & O’Keefe, M. Blue Frontiers: Managing the Environmental Costs of Aquaculture (The WorldFish Center, Penang, Malaysia, 2011).\nGentry, R. R., Froehlich, H. E., Grimm, D., Kareiva, P., Parke, M., Rust, M., Gaines, S. D., & Halpern, B. S. Mapping the global potential for marine aquaculture. Nature Ecology & Evolution, 1, 1317-1324 (2017).\nGEBCO Compilation Group (2022) GEBCO_2022 Grid (doi:10.5285/e0f0bb80-ab44-2739-e053-6c86abc0289c).\nNOAA Coral Reef Watch. “Daily Global 5km Satellite Sea Surface Temperature Anomaly (Version 3.1, Released August 1, 2018).” Accessed December 3, 2024. https://coralreefwatch.noaa.gov/product/5km/index_5km_ssta.php.\nR. Oliver, EDS 223 - Geospatial Analysis and Remote Sensing, Course Notes. 2024. [Online]. Available: https://eds-223-geospatial.github.io/\nSeaLifeBase. “Penaeus setiferus (Linnaeus, 1767) - Northern White Shrimp.” Accessed December 3, 2024. https://sealifebase.org.\nSeaLifeBase. “Ostrea edulis (Linnaeus, 1758) - Edible Oyster.” Accessed December 3, 2024. https://sealifebase.org.\n\nAcknowledgements\nSpecial shoutout to Jordan Sibley, who helped me debug a particularly fraught piece of code in this analysis.\n\n\nCode\nknitr::include_graphics(\"data/IMG_20241203_193746.jpg\")"
  },
  {
    "objectID": "posts/2025-1-1-redlining/index.html",
    "href": "posts/2025-1-1-redlining/index.html",
    "title": "A geospatial look at the relationship between redlining and citizen science",
    "section": "",
    "text": "Present-day environmental justice may reflect legacies of injustice in the past. The United States has a long history of racial segregation which is still visible. During the 1930’s the Home Owners’ Loan Corporation (HOLC), as part of the New Deal, rated neighborhoods based on their perceived safety for real estate investment. Their ranking system, (A (green), B (blue), C (yellow), D (red)) was then used to block access to loans for home ownership. Colloquially known as “redlining”, this practice has had widely-documented consequences not only for community wealth, but also health.Redlined neighborhoods have less greenery, and are hotter than other neighborhoods.\nThis post will seek to shed light on how these redlining practices have effected environmental health in Los Angeles today, using citizen science data.\n\n\nCode\nrm(list = ls())\n# Import libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(here)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(patchwork)\nlibrary(testthat)\n\n\n\n\nCode\n# Read in data as sf objects\nepa_block_level &lt;- st_read(here(\"posts\", \"2025-1-1-redlining\",\"data\", \"ejscreen/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\"))\nredlining &lt;- st_read(here(\"posts\", \"2025-1-1-redlining\",\"data\", \"mapping-inequality/mapping-inequality-los-angeles.json\"))\nbird_obs &lt;- st_read(here(\"posts\", \"2025-1-1-redlining\",\"data\", \"gbif-birds-LA/gbif-birds-LA.shp\"))\n\n\n\n\nCode\n# Confirm that all data sets are using WGS 84\nst_crs(redlining)\nst_crs(bird_obs)\nst_crs(epa_block_level) # This data does not have geometry, so the CRS is irrelevant\n\n\n\nif(st_crs(redlining) != st_crs(bird_obs) || st_crs(redlining) != st_crs(epa_block_level)) {\n  warning(\"Coordinate systems do not match\")\n} else {\n  print(\"All coordinate systems match\")\n}\n\n# Ah-oh, gotta fix one coordinate system, and then run the test again\n\nepa_block_level &lt;- st_transform(epa_block_level, crs = st_crs(redlining))\n\nif(st_crs(redlining) != st_crs(bird_obs) || st_crs(redlining) != st_crs(epa_block_level)) {\n  warning(\"Coordinate systems do not match\")\n} else {\n  print(\"All coordinate systems match\")\n}\n\n\n\n\nCode\n# Filter the EPA data to only contain data from Los Angeles County\n\nepa_la &lt;- epa_block_level |&gt;\n  filter(epa_block_level$CNTY_NAME == \"Los Angeles County\")"
  },
  {
    "objectID": "posts/2025-1-1-redlining/index.html#background",
    "href": "posts/2025-1-1-redlining/index.html#background",
    "title": "A geospatial look at the relationship between redlining and citizen science",
    "section": "",
    "text": "Present-day environmental justice may reflect legacies of injustice in the past. The United States has a long history of racial segregation which is still visible. During the 1930’s the Home Owners’ Loan Corporation (HOLC), as part of the New Deal, rated neighborhoods based on their perceived safety for real estate investment. Their ranking system, (A (green), B (blue), C (yellow), D (red)) was then used to block access to loans for home ownership. Colloquially known as “redlining”, this practice has had widely-documented consequences not only for community wealth, but also health.Redlined neighborhoods have less greenery, and are hotter than other neighborhoods.\nThis post will seek to shed light on how these redlining practices have effected environmental health in Los Angeles today, using citizen science data.\n\n\nCode\nrm(list = ls())\n# Import libraries\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tmap)\nlibrary(here)\nlibrary(gt)\nlibrary(gtExtras)\nlibrary(patchwork)\nlibrary(testthat)\n\n\n\n\nCode\n# Read in data as sf objects\nepa_block_level &lt;- st_read(here(\"posts\", \"2025-1-1-redlining\",\"data\", \"ejscreen/EJSCREEN_2023_BG_StatePct_with_AS_CNMI_GU_VI.gdb\"))\nredlining &lt;- st_read(here(\"posts\", \"2025-1-1-redlining\",\"data\", \"mapping-inequality/mapping-inequality-los-angeles.json\"))\nbird_obs &lt;- st_read(here(\"posts\", \"2025-1-1-redlining\",\"data\", \"gbif-birds-LA/gbif-birds-LA.shp\"))\n\n\n\n\nCode\n# Confirm that all data sets are using WGS 84\nst_crs(redlining)\nst_crs(bird_obs)\nst_crs(epa_block_level) # This data does not have geometry, so the CRS is irrelevant\n\n\n\nif(st_crs(redlining) != st_crs(bird_obs) || st_crs(redlining) != st_crs(epa_block_level)) {\n  warning(\"Coordinate systems do not match\")\n} else {\n  print(\"All coordinate systems match\")\n}\n\n# Ah-oh, gotta fix one coordinate system, and then run the test again\n\nepa_block_level &lt;- st_transform(epa_block_level, crs = st_crs(redlining))\n\nif(st_crs(redlining) != st_crs(bird_obs) || st_crs(redlining) != st_crs(epa_block_level)) {\n  warning(\"Coordinate systems do not match\")\n} else {\n  print(\"All coordinate systems match\")\n}\n\n\n\n\nCode\n# Filter the EPA data to only contain data from Los Angeles County\n\nepa_la &lt;- epa_block_level |&gt;\n  filter(epa_block_level$CNTY_NAME == \"Los Angeles County\")"
  },
  {
    "objectID": "posts/2025-1-1-redlining/index.html#part-1-legacy-of-redlining-in-current-environmental-injustice",
    "href": "posts/2025-1-1-redlining/index.html#part-1-legacy-of-redlining-in-current-environmental-injustice",
    "title": "A geospatial look at the relationship between redlining and citizen science",
    "section": "Part 1: Legacy of redlining in current environmental (in)justice",
    "text": "Part 1: Legacy of redlining in current environmental (in)justice\nFirst up, we have a map that shows the historically redlined districts in Los Angeles.\n\n\nCode\n# To solve for errors involving invalid polygons, I used the filter function\nredlining &lt;- redlining |&gt;\n  filter(st_is_valid(redlining))\n\n\n\n\nCode\n# Finally, on to mapping!\n\nbbox_la &lt;- st_bbox(redlining)\n\ntm_shape(epa_la, bbox = bbox_la) +\n  tm_polygons(col = \"white\") +\ntm_shape(redlining) +\n  tm_polygons(title = \"HOLC\\nGrade\",\n              col = \"grade\",\n              palette = (\"YlOrRd\")) +\n  tm_compass(type = \"rose\", size = 4, position = c(\"right\", \"top\"), text.size = 0.60) +\n  tm_scale_bar(text.size = 0.50, position = c(\"left\", \"bottom\")) +\n  tm_layout(main.title = \"Historically Redlined Districts in Los Angeles\",\n            bg.color = \"linen\",\n            legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n\n\n\nLet’s see a table that summarizes the percent of current census block groups within each HOLC grade (or none).\n\n\nCode\n# To do this, I will need to perform a st_join on the datasets in order to get both census block group data and HOLC grade data onto the same dataframe\n\ncensus_grades &lt;- st_join(x = epa_la, y = redlining, join = st_intersects, left = TRUE) |&gt;\n  group_by(grade) |&gt;\n  summarise(count_blocks = n(), # The number of blocks in each HOLC grade\n            percentage = (n() /nrow(epa_la)) * 100) |&gt; # Math to calculate %\n  st_drop_geometry() # This will make it a table without the geometry data in the way\n\ntest_that(\"All percentage values are greater than 0\", {\n  expect_true(all(census_grades$percentage &gt; 0))\n})\n\n\nTest passed 😸\n\n\n\n\nCode\n# Now, lets make a table using the gt package that makes the table look like its from the New York Times!\n\nnyt_tab &lt;- census_grades |&gt;\n  gt() |&gt;\n  gt_theme_nytimes() |&gt;\n  tab_header(title = \"Percentage of Current Census Blocks within HOLC Grades\") |&gt;\n  cols_label(\n    grade = \"HOLC Grade\",\n    count_blocks = \"Block Count\",\n    percentage = \"% within Grade\"\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body()\n  )\n\nnyt_tab\n\n\n\n\n\n\n\n\nPercentage of Current Census Blocks within HOLC Grades\n\n\nHOLC Grade\nBlock Count\n% within Grade\n\n\n\n\nA\n449\n6.81232\n\n\nB\n1206\n18.29768\n\n\nC\n2748\n41.69322\n\n\nD\n1300\n19.72387\n\n\nNA\n3097\n46.98832\n\n\n\n\n\n\n\nThis table illustrates the percentages of census blocks that fall within historic HOLC graded areas. Interestingly, excluding NAs, most census blocks fall within the C grade.\nThe effects of this redlining has had an effect on these neighborhoods to this day. Using data from the Environmental Protection Agency Environmental Justice Screening and Mapping tool from Los Angeles county, we can make figures that show these long lasting effects.\n\n\nCode\n# Perform spatial join and group by HOLC grade\nsummary_df &lt;- st_join(x = epa_la, y = redlining, join = st_intersects, left = TRUE) |&gt;\n  group_by(grade) |&gt;\n  summarise(\n    mean_low_income = mean(LOWINCPCT, na.rm = TRUE),  \n    mean_pm25 = mean(P_D2_PM25, na.rm = TRUE),   \n    mean_life_expectancy = mean(P_LIFEEXPPCT, na.rm = TRUE) \n  ) |&gt;\n  st_drop_geometry()  # Remove spatial data\n\n\n\n\nCode\n# Bar plot for % low income\nincome_plot &lt;- ggplot(summary_df, aes(x = grade, y = mean_low_income)) +\n  geom_bar(stat = \"identity\", fill = \"indianred3\") +\n  labs(title = \"Average of Low Income\\nHouseholds by\\nHOLC Grade\", x = \"HOLC Grade\", y = \"Mean % Low Income Households\") +\n  theme_bw()\n\nlife_plot &lt;- ggplot(summary_df, aes(x = grade, y = mean_life_expectancy)) +\n  geom_bar(stat = \"identity\", fill = \"tan1\") +\n  labs(title = \"Average Life\\nExpectancy\\nby HOLC Grade\", x = \"HOLC Grade\", y = \"Mean % Life Expectancy\") +\n  theme_bw()\n\nparticulate_plot &lt;- ggplot(summary_df, aes(x = grade, y = mean_pm25)) +\n  geom_bar(stat = \"identity\", fill = \"peachpuff\") +\n  labs(title = \"Average of Particulate\\nMatter 2.5 in Air\\nby HOLC Grade\", x = \"HOLC Grade\", y = \"Mean % Particulate Matter 2.5\") +\n  theme_bw()\n\n(income_plot | particulate_plot | life_plot) & theme_bw()\n\n\n\n\n\n\n\n\n\nThis bar graph shows the average number of low income households per HOLC grade.As we can see, the lower the grade becomes, the higher the percentage of low income households. This goes to show that poverty may be directly linked with these HOLC grades, even though these are modern statistics about income.\nSimilar results are reflected in both the average of particulate matter, and the average life expectancy. This would suggest a correlation between all of these variables. In summary, even though these HOLC grades were given over a century ago, they are still having an effect today on the quality of life for people in these grades. Specifically, the lower grade areas have been held in poverty and detrimental environmental conditions."
  },
  {
    "objectID": "posts/2025-1-1-redlining/index.html#part-2-legacy-of-redlining-in-biodiversity-observations",
    "href": "posts/2025-1-1-redlining/index.html#part-2-legacy-of-redlining-in-biodiversity-observations",
    "title": "A geospatial look at the relationship between redlining and citizen science",
    "section": "Part 2: Legacy of redlining in biodiversity observations",
    "text": "Part 2: Legacy of redlining in biodiversity observations\nA recent study found that redlining has not only affected the environments communities are exposed to, it has also shaped our observations of biodiversity.4 Community or citizen science, whereby individuals share observations of species, is generating an enormous volume of data. Ellis-Soto and co-authors found that redlined neighborhoods remain the most undersampled areas across 195 US cities. This gap is highly concerning, because conservation decisions are made based on these data.\nThe below chart and bar plot show the total number of bird observations from 2022, per HOLC grade.\n\n\nCode\n# First, filter birds data so that it only contains the year we are interested in - in this case, 2022\nbird_obs_2022 &lt;- bird_obs |&gt;\n  filter(year == 2022)\n\n\n\n\nCode\n# Then, do a join of the HOLC and bird data, grouping by grade\ngrade_birds &lt;- st_join(x = redlining, y = bird_obs_2022, join = st_intersects, left = TRUE) |&gt;\n  group_by(grade) |&gt;\n  summarise(numberof_birds = n()) |&gt; # Count the number of bird obs per grade\n  st_drop_geometry() # This will make it a table without the geometry data in the way\n\n\n\n\nCode\n# Create a fancy table with the dataframe created above\nbird_tab1 &lt;- grade_birds |&gt;\n  gt() |&gt;\n  gt_theme_nytimes() |&gt;\n  tab_header(title = \"Number of GBIF Bird Observations\\nwithin HOLC Grades\") |&gt;\n  cols_label(\n    grade = \"HOLC Grade\",\n    numberof_birds = \"Number of Bird Observations\",\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body()\n  )\n\nbird_tab1\n\n\n\n\n\n\n\n\nNumber of GBIF Bird Observations within HOLC Grades\n\n\nHOLC Grade\nNumber of Bird Observations\n\n\n\n\nA\n1078\n\n\nB\n1323\n\n\nC\n2360\n\n\nD\n2000\n\n\nNA\n273\n\n\n\n\n\n\n\n\n\nCode\n# Make a bar plot of the above tables data\n#| eval: true\n#| echo: true\nbird_plot1 &lt;- ggplot(grade_birds, aes(x = grade, y = numberof_birds)) +\n  geom_bar(stat = \"identity\", fill = \"indianred3\") +\n  labs(title = \"Number of GBIF Bird Observations by HOLC Grade\", x = \"HOLC Grade\", y = \"Number of Bird Observations\") +\n  theme_bw()\n\nbird_plot1\n\n\n\n\n\n\n\n\n\nNow, as we can see, this data does not line up with the findings from Soto et al 2023. However, this is likely because we have not yet accounted for the fact that not very many of our geospatial points contain higher grades. This can be accounted for with code, by summarizing the total observations by area.\nUsing this summarized data, we can make the following graph and corresponding bar chart:\n\n\nCode\n# I had already joined my datasets, leaving out some area data. I did a rejoin to regain that data\n  \nactual_birds &lt;- st_join(x = redlining, y = bird_obs_2022, join = st_intersects, left = TRUE) |&gt;\n  group_by(grade) |&gt;\n  summarize(total_area = sum(area, na.rm = TRUE),\n            grade_count = n()) |&gt;\n  mutate(bird_count_area = grade_count/total_area) |&gt;\n  select(grade, bird_count_area) |&gt;\n  st_drop_geometry()\n\n\n\n\nCode\nbird_tab2 &lt;- actual_birds |&gt;\n  gt() |&gt;\n  gt_theme_nytimes() |&gt;\n  tab_header(title = \"Number of GBIF Bird Observations within HOLC Grades, accounting for Scale Differences\") |&gt;\n  cols_label(\n    grade = \"HOLC Grade\",\n    bird_count_area = \"Number of Bird Observations\",\n  ) |&gt;\n  tab_style(\n    style = cell_text(weight = \"bold\"),\n    locations = cells_body()\n  )\n\nbird_plot2 &lt;- ggplot(actual_birds, aes(x = grade, y = bird_count_area)) +\n  geom_bar(stat = \"identity\", fill = \"tan1\") +\n  labs(title = \"Number of GBIF Bird Observations within HOLC Grades\\naccounting for Scale Differences\", x = \"HOLC Grade\", y = \"Number of Bird Observations\") +\n  theme_bw()\n\nbird_tab2\n\n\n\n\n\n\n\n\nNumber of GBIF Bird Observations within HOLC Grades, accounting for Scale Differences\n\n\nHOLC Grade\nNumber of Bird Observations\n\n\n\n\nA\n2980.5113\n\n\nB\n5213.2797\n\n\nC\n3102.9699\n\n\nD\n2064.0089\n\n\nNA\n392.0854\n\n\n\n\n\n\n\nCode\nbird_plot2\n\n\n\n\n\n\n\n\n\nThis data shows us that when accounting for differences in number of areas that contain certain grades, the results match more the findings revealed in the 2023 study. This supports the hypothesis that less observations are recorded in areas with a low HOLC grade, which could lead to bias when making biodiversity and environmental health decisions."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Bailey Jørgensen",
    "section": "",
    "text": "Underwater Paleontology, Robotics, and Education\n\n\nA reflection on the early days of a novel data collection technique\n\n\n\nBailey Jørgensen\n\n\nApr 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA geospatial look at the relationship between redlining and citizen science\n\n\nUsing HOLC grade and GBIF data from Los Angeles\n\n\n\nBailey Jørgensen\n\n\nJan 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrioritizing Potential Aquaculture Zones on the West Coast of the United States\n\n\nAn Exercise in R and Quarto\n\n\n\nBailey Jørgensen\n\n\nDec 20, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperimenting with a DEEP SEA CORAL interactions model\n\n\nAnalyzing the relationship between deep sea corals, latitude, and ocean depth\n\n\n\nBAILEY JØRGENSEN\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R Analysis of Landcover Classification using Decision Trees\n\n\nUsing multi-spectral imagery on the location of 4 land cover types, with a machine learning twist\n\n\n\nBailey Jørgensen\n\n\nNov 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire Analysis\n\n\nAQI and spatial analysis walkthrough of the 2017 natural disaster from the perspective of a Masters of Environmental Data Science student\n\n\n\nBailey Jørgensen\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiodiversity Intactness Index (BII) change in Phoenix, AZ\n\n\nA geospatial analysis in Python\n\n\n\nBailey Jørgensen\n\n\nSep 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNew Records of Eocene and Oligocene Squamates from Southwest Montana\n\n\nA new paper published in Paludicola by the Rochester Institute of Vertebrate Paleontology\n\n\n\nDonald Lofgren, Debra Hanneman, Anthony Runkel, Ping Fong, Gabriel Hong, Josephine Burdekin, Michael Chai, Yvonne Kan, and Bailey Jørgensen\n\n\nSep 5, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]