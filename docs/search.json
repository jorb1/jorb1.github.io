[
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "Image of Fire"
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#background",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#background",
    "title": "Thomas Fire Analysis",
    "section": "Background:",
    "text": "Background:\nThe Thomas Fire, which burned over 280,000 acres in Ventura and Santa Barbara counties in December 2017, was one of California’s largest wildfires at the time. It caused widespread ecological damage, displaced communities, and left lasting environmental impacts.\nIn this analysis, I will find the perimeter of the fire, analyze spatial data of the lasting fire scars through false color images, and visualize the effects that this fire had on air quality in the Santa Barbara area.\nAbout the data\nIn this task I will use historical open-access data about fire perimeters in California. There are several datasets with this information online. The dataset that I found is from data.gov at this link: https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436. It was a particularly useful site, as there were multiple filetypes to choose from.\nThe next dataset I will use is a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. These data were retrieved from the Microsof Planetary Computer data catalogue and pre-processed to remove data outside land and coarsen the spatial resolution. (This data should be used for visualization and educational purposes only.)\nFinally, I will use Air Quality Index (AQI) data from the US Environmental Protection Agency to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County.\n\nFirst up in my analysis:\nFire perimeter data retrieval and selection\nI will find and isolate the perimeter of the Thomas Fire, using open source data. I will then be able to use the Thomas Fire perimeter data in further analysis of the effects of the fire on Santa Barbara ecology. I will save this perimeter data as a file in my repository, for independent use in other notebooks.\nTo begin, I will do some exploratory data analysis to get a sense of the dataset I am using. I will ensure that I know the CRS of the data, for use in further spatial data joining and analysis.\n\n# Load libraries\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rioxarray as rioxr\n\n# Read in data \n\nfp_perimeter = os.path.join('data', 'California_Fire_Perimeters_(all).shp')\nperimeter = gpd.read_file(fp_perimeter)\n\n\nperimeter.head(3)\n\n\n\n\n\n\n\n\nYEAR_\nSTATE\nAGENCY\nUNIT_ID\nFIRE_NAME\nINC_NUM\nALARM_DATE\nCONT_DATE\nCAUSE\nC_METHOD\nOBJECTIVE\nGIS_ACRES\nCOMMENTS\nCOMPLEX_NA\nIRWINID\nFIRE_NUM\nCOMPLEX_ID\nDECADES\ngeometry\n\n\n\n\n0\n2023\nCA\nCDF\nSKU\nWHITWORTH\n00004808\n2023-06-17\n2023-06-17\n5\n1\n1\n5.72913\nNone\nNone\n{7985848C-0AC2-4BA4-8F0E-29F778652E61}\nNone\nNone\n2020\nPOLYGON ((-13682443.000 5091132.739, -13682445...\n\n\n1\n2023\nCA\nLRA\nBTU\nKAISER\n00010225\n2023-06-02\n2023-06-02\n5\n1\n1\n13.60240\nNone\nNone\n{43EBCC88-B3AC-48EB-8EF5-417FE0939CCF}\nNone\nNone\n2020\nPOLYGON ((-13576727.142 4841226.161, -13576726...\n\n\n2\n2023\nCA\nCDF\nAEU\nJACKSON\n00017640\n2023-07-01\n2023-07-02\n2\n1\n1\n27.81450\nNone\nNone\n{B64E1355-BF1D-441A-95D0-BC1FBB93483B}\nNone\nNone\n2020\nPOLYGON ((-13459243.000 4621236.000, -13458968...\n\n\n\n\n\n\n\n\n# Figure out the dimensions of the dataframe\nprint(\"Shape of the data:\", perimeter.shape)\n\n# Figure out if the columns are the expected datatypes\nprint(\"Data types:\", perimeter.dtypes)\n\nShape of the data: (22261, 19)\nData types: YEAR_            int64\nSTATE           object\nAGENCY          object\nUNIT_ID         object\nFIRE_NAME       object\nINC_NUM         object\nALARM_DATE      object\nCONT_DATE       object\nCAUSE            int64\nC_METHOD         int64\nOBJECTIVE        int64\nGIS_ACRES      float64\nCOMMENTS        object\nCOMPLEX_NA      object\nIRWINID         object\nFIRE_NUM        object\nCOMPLEX_ID      object\nDECADES          int64\ngeometry      geometry\ndtype: object\n\n\n\n# Explore data CRS\nperimeter.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# Find out if the CRS is projected\nperimeter.crs.is_projected\n\nTrue\n\n\nFrom this data exploration, I learned that the dataset is much larger than I need, but does contain useful information in addition to the geometries, such as acres burned, cause, etc. I learned that the names of the fires are in all capitals, and that the year numbers are int64, so I can treat them as numeric values. Finally, I leaned that the CRS is WGS 84, and that is is projected data, rather than geographic.\nFrom this fire perimeter data, I will select the Thomas Fire boundary. The fire occurred in 2017.\n\n# Make the column names lower case\nperimeter.columns = perimeter.columns.str.lower()\n\n# Filter data to only include the Thomas Fire boudnary in 2017\nthomas = perimeter[(perimeter['fire_name'] == \"THOMAS\") & (perimeter['year_'] == 2017)]\n\nthomas\n\n\n\n\n\n\n\n\nyear_\nstate\nagency\nunit_id\nfire_name\ninc_num\nalarm_date\ncont_date\ncause\nc_method\nobjective\ngis_acres\ncomments\ncomplex_na\nirwinid\nfire_num\ncomplex_id\ndecades\ngeometry\n\n\n\n\n2654\n2017\nCA\nUSF\nVNC\nTHOMAS\n00003583\n2017-12-04\n2018-01-12\n9\n7\n1\n281791.0\nCONT_DATE based on Inciweb\nNone\nNone\nNone\nNone\n2010\nMULTIPOLYGON (((-13316089.016 4088553.040, -13...\n\n\n\n\n\n\n\nNow I will save only the 2017 Thomas Fire boundary as a GeoJSON file. The file should go into the data/ directory in my repository.\n\n# Save the fire boundary as a file that can go into my repository\n# Save the filtered GeoDataFrame as a GeoJSON file\npath = 'data/thomas.geojson'\nthomas.to_file(path, driver='GeoJSON')\n\nI chose to use a GeoJSON file format for my perimeter boundary because it is a common and useful “open format for encoding vector points and their attributes”. It comes in one file, as compared to .shp files, which have many dependencies. It requires the data be in WGS84, and since I already verified that this data is in that CRS, it seems like the best possible option for this analysis."
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#next-up",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#next-up",
    "title": "Thomas Fire Analysis",
    "section": "Next up:",
    "text": "Next up:\n\nTrue Color Image\nAs I import the raster file to make this true color image, I add the parameter decode_coords=\"all\" to the code. This parameter controls how the coordinate metadata in the NetCDF file are processed. Specifically, the all distinction decodes all coordinates in the file to easily useable xarray coordinate variables.\n\n# Construct a file path to the Landsat data using os and import it\nfp = os.path.join('data', 'landsat8-2018-01-26-sb-simplified.nc')\nsb_rast = rioxr.open_rasterio(fp, decode_coords=\"all\")\nsb_rast\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 25MB\nDimensions:      (band: 1, x: 870, y: 731)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0\nData variables:\n    red          (band, y, x) float64 5MB ...\n    green        (band, y, x) float64 5MB ...\n    blue         (band, y, x) float64 5MB ...\n    nir08        (band, y, x) float64 5MB ...\n    swir22       (band, y, x) float64 5MB ...xarray.DatasetDimensions:band: 1x: 870y: 731Coordinates: (4)band(band)int641array([1])x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([121305.0, 121575.0, 121845.0, 122115.0, 122385.0, 122655.0, 122925.0,\n       123195.0, 123465.0, 123735.0,\n       ...\n       353505.0, 353775.0, 354045.0, 354315.0, 354585.0, 354855.0, 355125.0,\n       355395.0, 355665.0, 355935.0],\n      dtype='float64', name='x', length=870))yPandasIndexPandasIndex(Index([3952395.0, 3952125.0, 3951855.0, 3951585.0, 3951315.0, 3951045.0,\n       3950775.0, 3950505.0, 3950235.0, 3949965.0,\n       ...\n       3757725.0, 3757455.0, 3757185.0, 3756915.0, 3756645.0, 3756375.0,\n       3756105.0, 3755835.0, 3755565.0, 3755295.0],\n      dtype='float64', name='y', length=731))Attributes: (0)\n\n\n\n# Explore the data\nprint('Shape: ', dict(sb_rast.sizes))\nprint(sb_rast.data_vars, '\\n')\n\nShape:  {'band': 1, 'x': 870, 'y': 731}\nData variables:\n    red      (band, y, x) float64 5MB ...\n    green    (band, y, x) float64 5MB ...\n    blue     (band, y, x) float64 5MB ...\n    nir08    (band, y, x) float64 5MB ...\n    swir22   (band, y, x) float64 5MB ... \n\n\n\nThis data exploration shows me that there is only one band on this raster. This means that including it is redundant, and the band will need to be removed. I also learned that each color band is a float64 integer, which is good to know when doing analysis.\n\n# Drop the band dimension of the data\n# Original dimensions and coordinates\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\n# Remove length 1 dimension (band)\nsb_rast = sb_rast.squeeze()\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\n# Drop the coordinates associated to band\nsb_rast = sb_rast.drop_vars('band')\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\nFrozenMappingWarningOnValuesAccess({'band': 1, 'x': 870, 'y': 731}) Coordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\nFrozenMappingWarningOnValuesAccess({'x': 870, 'y': 731}) Coordinates:\n    band         int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\nFrozenMappingWarningOnValuesAccess({'x': 870, 'y': 731}) Coordinates:\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\n\n\nTo get a good look at this raster data, without creating any new variables:\nI will select the red, green, and blue variables (in that order) of the xarray.Dataset holding the Landsat data, convert it to an xarray.DataArray using the to_array() method, and then use .plot.imshow() to create an RGB image with the data. There will be a warning, that’s ok. I will adjust the scale used for plotting the bands to get a true color image.\nThe first plot will have the parameter set to be robust=False.\n\n# Select red, green, and blue variables, stack them, and plot as an RGB image\nsb_rast[['red', 'green', 'blue']].to_array().plot.imshow(robust=False)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nAs we can see, this doesn’t turn out quite as we’d hoped. That’s because, by setting that parameter to False, we are not accounting for cloud cover. Their RGB values are outliers and cause the other values to be squished when plotting.\nTo account for this, I will use the robust = True parameter at the end of my code, in order to deal with the clouds:\n\n# Select red, green, and blue variables, stack them, and plot as an RGB image\nsb_rast[['red', 'green', 'blue']].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nThis true color image gives us a visual that mostly resembles what we would expect to see with our human eyes looking down on Santa Barbara. The colors are what we would expect to see, and this can be useful for identifying landmarks. However, sometimes it is important to get a new perspective. For example, using this map, it is almost impossible to see the area in which the Thomas Fire burned. And that’s when we bring in…\n\n\n4. False color image\nTo continue my analysis, and without creating any new variables, I will create a false color image by plotting the short-wave infrared (swir22), near-infrared, and red variables (in that order).\n\n# Select the swir22, near-infrared, and red variables, stack them, and plot as a false color image\nsb_rast[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nFalse color imagery, created using satellite data from instruments like Landsat, is a useful tool for monitoring wildfire impacts. By assigning infrared bands to visible colors, these images highlight vegetation health, burn severity, and the extent of fire scars. This approach helps researchers and land managers assess recovery efforts, identify high-risk areas, and plan restoration strategies.\n\n\n5. Map\nFinally, I will create a map showing the shortwave infrared/near-infrared/red false color image together with the Thomas Fire perimeter.\n\n# Read in the Thomas fire perimeter we created\nfp2 = os.path.join('data', 'thomas.geojson')\nthomas_perim = gpd.read_file(fp2)\nthomas_perim.plot()\n\n\n\n\n\n\n\n\nThis initial plot shows us that our perimeter file is looking good. It also shows us the perimeter that we will want to clip our raster file to, in order to create our map analyzing the burn area of the Thomas Fire.\nFirst, we will have to ensure that our CRS’s match for these datasets…\n\n# Examine CRss\nprint('Santa Barbara Raster CRS: ', sb_rast.rio.crs)\nprint('Thomas Fire Perimeter CRS: ', thomas_perim.crs)\n\nSanta Barbara Raster CRS:  EPSG:32611\nThomas Fire Perimeter CRS:  EPSG:3857\n\n\n\n# Reproject CRS of the Santa Barbara Raster\nsb_rast = sb_rast.rio.reproject(\"EPSG:3857\")\nprint('Matched CRS?', sb_rast.rio.crs == thomas_perim.crs)\n\nMatched CRS? True\n\n\n\n# Clip the sb_rast map to match the dimensions of the Thomas Fire Perimeter\nsb_fire = sb_rast.rio.clip_box(*thomas_perim.total_bounds)\n\n\n# Map our false color image with the fire boundary overlaid\nfig, ax = plt.subplots(figsize=(10, 10))\nsb_fire[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax=ax, robust=True)\nthomas_perim.boundary.plot(ax=ax, edgecolor=\"firebrick\", linewidth = 2, label=\"Thomas Fire Perimeter\")\nax.set_title(\"Thomas Fire: Burn Scars in False Color Imagery\", fontsize=16)\nax.legend(loc='upper right', fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\nOur true color image was insufficient to reveal plainly the path of the Thomas Fire. However, by simply utilizing a false color composit, the path of the fire and the effect it had on the landscape are much more clear.\nThis final map clearly shows the burn scars from the fire, contained within the perimeter boundary we utilized to help us further identify the exact area effected."
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#conclusion",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis (1).html#conclusion",
    "title": "Thomas Fire Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nUsing Python in Jupyter Notebooks, I have successfully executed a series of analysis analyzing a fire that had clear and lasting effects on the regions of Santa Barbara and Ventura. This analysis also showcases the variety of data types that Python can handle, from spatial data to data visualization.\n\nCitations:\nAirNow. “Air Quality Index (AQI) Basics.” Accessed December 4, 2024. https://www.airnow.gov/aqi/aqi-basics/.\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nCAL Fire. “California Fire Perimeters (All).” Data.gov. Metadata created March 30, 2024, updated May 14, 2024. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436.\nHamm, Keith. “Closing Schools and Moving Finals Due to Thomas Fire: A Look at Our Education System’s Response to the Wildfire.” Santa Barbara Independent, December 13, 2017. https://www.independent.com/2017/12/13/closing-schools-and-moving-finals-due-thomas-fire/.\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 Atmospherically Corrected Surface Reflectance Data from Landsat 8 [Dataset]. Simplified for visualization and educational purposes. Accessed November 20, 2024. https://planetarycomputer.microsoft.com.\nU.S. Environmental Protection Agency. “Air Data: Air Quality Data Collected at Outdoor Monitors Across the US.” Accessed December 4, 2024. https://www.epa.gov/outdoor-air-quality-data."
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html",
    "href": "posts/2024-11-8-landcover-classification/index.html",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "",
    "text": "Monitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance.\nClassifying remotely sensed imagery into land cover classes enables us to understand the distribution and change in land cover types over large areas.\nThere are many approaches for performing land cover classification:\n\nSupervised approaches use training data labeled by the user\nUnsupervised approaches use algorithms to create groups which are identified by the user afterward"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#background",
    "href": "posts/2024-11-8-landcover-classification/index.html#background",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "",
    "text": "Monitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance.\nClassifying remotely sensed imagery into land cover classes enables us to understand the distribution and change in land cover types over large areas.\nThere are many approaches for performing land cover classification:\n\nSupervised approaches use training data labeled by the user\nUnsupervised approaches use algorithms to create groups which are identified by the user afterward"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#game-plan",
    "href": "posts/2024-11-8-landcover-classification/index.html#game-plan",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Game Plan",
    "text": "Game Plan\nIn this exercise, I am using a form of supervised classification – a decision tree classifier.\nDecision trees classify pixels using a series of conditions based on values in spectral bands. These conditions (or decisions) are developed based on training data.\nI will create a land cover classification for southern Santa Barbara County based on multi-spectral imagery and data on the location of 4 land cover types:\n\ngreen vegetation\ndry grass or soil\nurban\nwater\n\nTo do so, I will need to:\n\nLoad and process Landsat scene\nCrop and mask Landsat data to study area\nExtract spectral data at training sites\nTrain and apply decision tree classifier\nPlot results"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#data-details",
    "href": "posts/2024-11-8-landcover-classification/index.html#data-details",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Data Details",
    "text": "Data Details\nTo conduct this analysis, I will use the Landsat 5 Thematic Mapper data. More information about these data can be found at this link: https://www.usgs.gov/landsat-missions/landsat-5.\nSpecifically, I will be using 1 scene from September 25, 2007 (my birthday!), on bands 1,2, 3, 4, 5, 7, with collection 2 surface reflectance product.\nData files:\n\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B1.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B2.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B3.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B4.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B5.tif\nlandsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B7.tif\n\nStudy area:\nI will be using a polygon representing southern Santa Barbara county, the county in which I am currently attending school.\nData file:\n\nSB_county_south.shp\n\nTraining data:\nAnd finally, I will be using a data file with polygons representing sites with training data. Specifically, I will be using the data character string with land cover type.\nData file:\n\ntrainingdata.shp\n\nAll of the data used in this study were accessed on November 25th, 2024."
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#workflow",
    "href": "posts/2024-11-8-landcover-classification/index.html#workflow",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Workflow",
    "text": "Workflow\n\n1. Set up\nTo train our classification algorithm and plot the results, I will use the rpart and rpart.plot packages.\n\n\nCode\n# install.packages(\"rpart\")\n# install.packages(\"rpart.plot\")\n\n\nLet’s load all necessary packages:\n\n\n2. Load Landsat data\nLet’s create a raster stack. Each file name ends with the band number (e.g. B1.tif).\nI am missing a file for band 6, but, this is intentional. Band 6 corresponds to thermal data, which I will not be working with during this exercise.\nTo create a raster stack, I will create a list of the files that I would like to work with and read them all in at once using the terra::rast() function. I will then update the names of the layers to match the spectral bands and plot a true color image to see what we’re working with.\n\n\nCode\n# list files for each band, including the full file path\nfilelist &lt;- list.files(here::here(\"posts\", \"2024-11-8-landcover-classification\", \"data\", \"landsat-data\"), full.names = TRUE)\n\n# read in and store as a raster stack\nlandsat &lt;- rast(filelist)\n\n# update layer names to match band\nnames(landsat) &lt;- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\n# plot true color image\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\n\n\n3. Load study area\nI want to constrain our analysis to the southern portion of the county where we have training data, so I’ll read in a file that defines the area I would like to study.\n\n\nCode\n# read in shapefile for southern portion of SB county\nSB_county_south &lt;- st_read(here::here(\"posts\", \"2024-11-8-landcover-classification\", \"data\", \"SB_county_south.shp\")) %&gt;%\n      st_transform(SB_county_south, crs = crs(landsat))\n\n\nReading layer `SB_county_south' from data source \n  `C:\\MEDS\\jorb1.github.io\\posts\\2024-11-8-landcover-classification\\data\\SB_county_south.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -120.2327 ymin: 34.33603 xmax: -119.5757 ymax: 34.53716\nGeodetic CRS:  NAD83\n\n\nCode\n# Plot the shapefile\ntm_shape(SB_county_south) +\n  tm_borders()\n\n\n\n\n\n\n\n\n\n\n\n4. Crop and mask Landsat data to study area\nNow, I can crop and mask the Landsat data to the study area.\n\nWhy? This reduces the amount of data we’ll be working with and therefore saves computational time\nBonus: We can also remove any objects we’re no longer working with to save space\n\n\n\nCode\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped &lt;- crop(landsat, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked &lt;- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat, SB_county_south, landsat_cropped)\n\n# Plot!\nplotRGB(landsat_masked, r = 3, g = 2, b = 1, stretch = \"lin\")\n\n\n\n\n\n\n\n\n\n\n\n5. Convert Landsat values to reflectance\nNow I need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any scaling factors to convert to reflectance.\nIn this case, I are working with Landsat Collection 2.\nThe valid range of pixel values for this collection goes from 7,273 to 43,636… - with a multiplicative scale factor of 0.0000275 - with an additive scale factor of -0.2\nLet’s reclassify any erroneous values as NA and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%!\n\n\nCode\n# reclassify erroneous values as NA\nrcl &lt;- matrix(c(-Inf, 7273, NA,\n                 43636, Inf, NA), ncol = 3, byrow = TRUE)\n\nlandsat &lt;- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\nlandsat &lt;- (landsat * 0.0000275 - 0.2) * 100\n\n# check values are 0 - 100\nsummary(landsat)\n\n\n      blue           green            red             NIR       \n Min.   : 1.11   Min.   : 0.74   Min.   : 0.00   Min.   : 0.23  \n 1st Qu.: 2.49   1st Qu.: 2.17   1st Qu.: 1.08   1st Qu.: 0.75  \n Median : 3.06   Median : 4.59   Median : 4.45   Median :14.39  \n Mean   : 3.83   Mean   : 5.02   Mean   : 4.92   Mean   :11.52  \n 3rd Qu.: 4.63   3rd Qu.: 6.76   3rd Qu.: 7.40   3rd Qu.:19.34  \n Max.   :39.42   Max.   :53.32   Max.   :56.68   Max.   :57.08  \n NA's   :39856   NA's   :39855   NA's   :39855   NA's   :39856  \n     SWIR1           SWIR2      \n Min.   : 0.10   Min.   : 0.20  \n 1st Qu.: 0.41   1st Qu.: 0.60  \n Median :13.43   Median : 8.15  \n Mean   :11.88   Mean   : 8.52  \n 3rd Qu.:18.70   3rd Qu.:13.07  \n Max.   :49.13   Max.   :48.07  \n NA's   :42892   NA's   :46809  \n\n\n\n\n6. Training classifier\nLet’s begin by extracting reflectance values for training data!\nWe will load the shapefile identifying locations within our study area as containing one of our 4 land cover types.\n\n\nCode\n# read in and transform training data\ntraining_data &lt;- st_read(here::here( \"posts\", \"2024-11-8-landcover-classification\", \"data\", \"trainingdata.shp\")) %&gt;%\n  st_transform(., crs = crs(landsat))\n\n\nReading layer `trainingdata' from data source \n  `C:\\MEDS\\jorb1.github.io\\posts\\2024-11-8-landcover-classification\\data\\trainingdata.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 40 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 215539.2 ymin: 3808948 xmax: 259927.3 ymax: 3823134\nProjected CRS: WGS 84 / UTM zone 11N\n\n\nNow, we can extract the spectral reflectance values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\nCode\n# extract reflectance values at training sites\ntraining_data_values &lt;- terra::extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes &lt;- training_data %&gt;%\n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\nSB_training_data &lt;- left_join(training_data_values, training_data_attributes,\n                              by = c(\"ID\" = \"id\")) %&gt;%\n                    mutate(type = as.factor(type)) # convert landcover type to factor\n\n\nNext, let’s train the decision tree classifier!\nTo train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are).\n\nThe rpart() function implements the CART algorithm\nThe rpart() function needs to know the model formula and training data you would like to use\nBecause we are performing a classification, we set method = “class”\nWe also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\n\n\nCode\n# Establish model formula\nSB_formula &lt;- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# Train decision tree\nSB_decision_tree &lt;- rpart(formula = SB_formula,\n                          data = SB_training_data, \n                          method = \"class\",\n                          na.action = na.omit)\n\n\nTo understand how the decision tree will classify pixels, I can plot the results!\n\n\nCode\n# plot decision tree\nprp(SB_decision_tree)\n\n\n\n\n\n\n\n\n\n\n\n7. Classify image\nNow that I have a rule set for classifying spectral reflectance values into landcover types, I can apply the classifier to identify the landcover type in each pixel.\nThe terra package includes a predict() function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The predict() function will return a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data.\n\n\nCode\n# classify image based on decision tree\nSB_classification &lt;- terra::predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nlevels(SB_training_data$type)\n\n\n[1] \"green_vegetation\" \"soil_dead_grass\"  \"urban\"            \"water\"           \n\n\n\n\n8. Plot results\nFinally, I can plot the results and check out our land cover map!\n\n\nCode\n# Plot results\ntm_shape(SB_classification) + \n  tm_raster(palette = c(\"#8DB580\", \"#F2DDA4\", \"grey\", \"cornflowerblue\"),\n            labels = c(\"green vegetation\",\n                       \"soil/dead grass\",\n                       \"urban\",\n                       \"water\"),\n            title = \"Land cover type\") +\n  tm_layout(legend.position = c(\"left\", \"bottom\"),\n            main.title = \"Santa Barbara Land Cover\")"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#conclusion",
    "href": "posts/2024-11-8-landcover-classification/index.html#conclusion",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Conclusion:",
    "text": "Conclusion:\nWorking with Landsat data is fun! And it allows us to run analysis regarding landcover types, with the magic of R."
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#acknowledements",
    "href": "posts/2024-11-8-landcover-classification/index.html#acknowledements",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Acknowledements:",
    "text": "Acknowledements:\nMaterial for this exercise was taken from Ruth Oliver’s EDS 223: Geospatial Analysis Course at the University of Santa Barbara’s Masters of Environmental Data Science program. Thank you, Ruth!"
  },
  {
    "objectID": "posts/2024-11-8-landcover-classification/index.html#citations",
    "href": "posts/2024-11-8-landcover-classification/index.html#citations",
    "title": "An R Analysis of Landcover Classification using Decision Trees",
    "section": "Citations:",
    "text": "Citations:\nR. Oliver, EDS 223 - Geospatial Analysis and Remote Sensing, Course Notes. 2024. [Online]. Available: https://eds-223-geospatial.github.io/\nU.S. Geological Survey. (n.d.). Landsat 5. Retrieved December 7, 2024, from https://www.usgs.gov/landsat-missions/landsat-5"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "",
    "text": "Purpose: In 2021, Maricopa County —home to the Phoenix metropolitan area— was identified as the U.S. county with the most significant increase in developed land since 2001. This rapid urban sprawl has profound implications for biodiversity and the health of surrounding natural ecosystems.\nIn this notebook, I will investigate the impacts of urban expansion by analyzing a dataset that captures values for the Biodiversity Intactness Index (BII). Apecifically, I will examine changes in BII in the Phoenix county subdivision area between 2017 and 2020, shedding light on how urban growth affects biodiversity over time.\nHighlights: 1. Accessing the Microsoft Planetary Computer to Access their Impact Observatory data.\n\nCreating a plot of the Biodiversity Intactness Index within the Phoenix subdivision polygon.\nCalculate the percentage of area of the Phoenix area with a BII index of at least .07 in 2017 and 2020.\nPlotting the 2020 data, revealing areas that show a loss of biodiversity.\n\nAbout the data: 1. The first data set is is the Biodiversity Intactness Index (BII) Time Series. Access the io-biodiversity collection from the Microsoft Planetary Computer STAC catalog. I will be using the 2017 and 2020 rasters covering the Phoenix subdivision.\n\nThe second data set is the Phoenix Subdivision Shapefile Download the Phoenix subdivision polygon from the Census County Subdivision shapefiles for Arizona. All legal boundaries and names are as of January 1, 2024. The 2024 TIGER/Line Shapefiles were released on September 25, 2024. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions\n\n\n\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nMicrosoft Planetary Computer, STAC Catalog. Biodiversity Intactness (‘io-biodiversity’). [Dataset]. https://planetarycomputer.microsoft.com/dataset/io-biodiversity Accessed 2 December 2024.\nUnited States Census Bureau. (2022). Arizona County Subdivision 2022 TIGER/Line Shapefiles. [Data File]. U.S. Census Bureau, Geography Division. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions Accessed 2 December 2024.\nBoth of these datasets were accessed for this analysis on 12/2/2024.\n\n# Load Libraries\nimport pandas as pd\nimport geopandas as gpd\nimport planetary_computer\nimport pystac_client\nimport rich.table\nfrom geogif import gif\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rioxarray as rioxr\nfrom IPython.display import Image \nfrom shapely.geometry import box\nimport xarray as xr\nimport os\nimport rasterio\nfrom rasterio.windows import from_bounds\nfrom IPython.display import Image\nimport contextily as ctx\nimport matplotlib.patches as mpatches\nfrom matplotlib.lines import Line2D\n\nTo begin the analysis, I will first read in and explore the shapefile data that I have for Arizona, specifically looking for the Phoenix area.\n\n# Read in shapefile data for Arizona\narizona = gpd.read_file('data/tl_2022_04_cousub.shp')\narizona.head(3)\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUSUBFP\nCOUSUBNS\nGEOID\nNAME\nNAMELSAD\nLSAD\nCLASSFP\nMTFCC\nCNECTAFP\nNECTAFP\nNCTADVFP\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n04\n005\n91198\n01934931\n0400591198\nFlagstaff\nFlagstaff CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n12231052883\n44653332\n+35.1066114\n-111.3662497\nPOLYGON ((-112.13370 35.85596, -112.13368 35.8...\n\n\n1\n04\n005\n91838\n01934953\n0400591838\nKaibab Plateau\nKaibab Plateau CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n7228864534\n29327221\n+36.5991097\n-112.1368033\nPOLYGON ((-112.66039 36.53941, -112.66033 36.5...\n\n\n2\n04\n005\n91683\n01934950\n0400591683\nHualapai\nHualapai CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n2342313339\n3772690\n+35.9271665\n-113.1170408\nPOLYGON ((-113.35416 36.04097, -113.35416 36.0...\n\n\n\n\n\n\n\n\n# Clean up Arizona data\narizona.columns = arizona.columns.str.lower()\n\n# Select just the Phoenix area\nphoenix = arizona[arizona.name == \"Phoenix\"]\n\n# Generate a quick plot of the Phoenix shape\nphoenix.plot()\n\n\n\n\n\n\n\n\nWhen making maps it’s important to know what Coordinate Reference System we are dealing with. Let’s check what we have for the Phoenix file:\n\nphoenix.crs\n\n&lt;Geographic 2D CRS: EPSG:4269&gt;\nName: NAD83\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.\n- bounds: (167.65, 14.92, -40.73, 86.45)\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nNow that I have my Arizona data looking how I want it, lets access the Microsoft Planetary Computer!\n\n# Access the MPC catalog\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\ncatalog.get_collections()\ncollections = list(catalog.get_collections())\n\n# Print the number of collections\nprint('Number of collections:', len(collections))\n\n#Pull out the Impact Observatory collection\nio_collection = catalog.get_child('io-biodiversity')\nio_collection\n\nNumber of collections: 124\n\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Collection\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"io-biodiversity\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            description\n            \"Generated by [Impact Observatory](https://www.impactobservatory.com/), in collaboration with [Vizzuality](https://www.vizzuality.com/), these datasets estimate terrestrial Biodiversity Intactness as 100-meter gridded maps for the years 2017-2020.\n\nMaps depicting the intactness of global biodiversity have become a critical tool for spatial planning and management, monitoring the extent of biodiversity across Earth, and identifying critical remaining intact habitat. Yet, these maps are often years out of date by the time they are available to scientists and policy-makers. The datasets in this STAC Collection build on past studies that map Biodiversity Intactness using the [PREDICTS database](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.2579) of spatially referenced observations of biodiversity across 32,000 sites from over 750 studies. The approach differs from previous work by modeling the relationship between observed biodiversity metrics and contemporary, global, geospatial layers of human pressures, with the intention of providing a high resolution monitoring product into the future.\n\nBiodiversity intactness is estimated as a combination of two metrics: Abundance, the quantity of individuals, and Compositional Similarity, how similar the composition of species is to an intact baseline. Linear mixed effects models are fit to estimate the predictive capacity of spatial datasets of human pressures on each of these metrics and project results spatially across the globe. These methods, as well as comparisons to other leading datasets and guidance on interpreting results, are further explained in a methods [white paper](https://ai4edatasetspublicassets.blob.core.windows.net/assets/pdfs/io-biodiversity/Biodiversity_Intactness_whitepaper.pdf) entitled “Global 100m Projections of Biodiversity Intactness for the years 2017-2020.”\n\nAll years are available under a Creative Commons BY-4.0 license.\n\"\n        \n    \n                \n            \n                \n                    \n        links[] 7 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"items\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/io-biodiversity/items\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/geo+json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"license\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://creativecommons.org/licenses/by/4.0/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"CC BY 4.0\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            rel\n            \"about\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pdfs/io-biodiversity/Biodiversity_Intactness_whitepaper.pdf\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/pdf\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Technical White Paper\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            \n        \n            \n                \n        \n            rel\n            \"describedby\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/dataset/io-biodiversity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Human readable dataset overview and reference\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            \n        \n            \n                \n        \n            rel\n            \"self\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/io-biodiversity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            \n        \n            \n                \n        \n            rel\n            \"parent\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        stac_extensions[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/item-assets/v1.0.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"https://stac-extensions.github.io/table/v1.2.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            item_assets\n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff; profile=cloud-optimized\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Terrestrial biodiversity intactness at 100m resolution\"\n        \n    \n            \n        \n            \n                \n        raster:bands[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            sampling\n            \"area\"\n        \n    \n            \n        \n            \n                \n        \n            data_type\n            \"float32\"\n        \n    \n            \n        \n            \n                \n        \n            spatial_resolution\n            100\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            msft:region\n            \"westeurope\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:container\n            \"impact\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:storage_account\n            \"pcdata01euw\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:short_description\n            \"Global terrestrial biodiversity intactness at 100m resolution for years 2017-2020\"\n        \n    \n                \n            \n                \n                    \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n                \n            \n                \n                    \n        \n            extent\n            \n        \n            \n                \n        \n            spatial\n            \n        \n            \n                \n        bbox[] 1 items\n        \n            \n        \n            \n                \n        0[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -180\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            -90\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            180\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            90\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            temporal\n            \n        \n            \n                \n        interval[] 1 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2017-01-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2020-12-31T23:59:59Z\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            license\n            \"CC-BY-4.0\"\n        \n    \n                \n            \n                \n                    \n        keywords[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"Global\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"Biodiversity\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        providers[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"Impact Observatory\"\n        \n    \n            \n        \n            \n                \n        roles[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"producer\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"licensor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.impactobservatory.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            name\n            \"Vizzuality\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.vizzuality.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            name\n            \"Microsoft\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"host\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://planetarycomputer.microsoft.com\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            summaries\n            \n        \n            \n                \n        version[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"v1\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            assets\n            \n        \n            \n                \n        \n            thumbnail\n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pc_thumbnails/io-biodiversity-thumb.png\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n            \n        \n            \n                \n        \n            media_type\n            \"image/png\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            geoparquet-items\n            \n        \n            \n                \n        \n            href\n            \"abfs://items/io-biodiversity.parquet\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/x-parquet\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"GeoParquet STAC items\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Snapshot of the collection's STAC items exported to GeoParquet format.\"\n        \n    \n            \n        \n            \n                \n        \n            msft:partition_info\n            \n        \n            \n                \n        \n            is_partitioned\n            False\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            table:storage_options\n            \n        \n            \n                \n        \n            account_name\n            \"pcstacitems\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"stac-items\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n        \n    \n\n\n\nNow that we have our data, we need to specify the temporal and spatial information we are interested in. I will create a bounding box, that specifies our spatial area of interest.\nAs we can see from the description of the Impact Observatory Collection above, the only dates represented in the data are our time range of interest (2017-2020). As such, there is no need to specify a temporal variable.\n\n# Create bounding box\nbbox_of_interest = [-112.826843, 32.974108, -111.184387, 33.863574]\n\n\n# Catalog search\nsearch = catalog.search(\n    collections = ['io-biodiversity'],\n    bbox = bbox_of_interest)\n\n# Get items from search\nitems = search.item_collection()\n\n# Determine number of items in search\nprint(f'There are {len(items)} items in the search.')\n\nThere are 4 items in the search.\n\n\n\n# Retrieve items\nitem_names = {item.id : item for item in search.items()}\nlist(item_names) # from this list, we can see that our 1st item is 2020 data, and our 4th item is 2017 data\n\n# Select 2017 subset\nphx_2017 = items[3]\n\n# Select 2020 subset\nphx_2020 = items[0]\n\nOur search returned four STAC Items. We can tell from their IDs that that they contain data for the same area but for different times, specifically the years 2017 through 2020. Let’s display the available assets and properties for the 2017 Item.\n\nasset_table = rich.table.Table(\"Asset Key\", \"Asset Title\")\nfor key, value in items[-1].assets.items():\n    asset_table.add_row(key, value.title)\nasset_table\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Asset Key        ┃ Asset Title                     ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ data             │ Biodiversity Intactness         │\n│ tilejson         │ TileJSON with default rendering │\n│ rendered_preview │ Rendered preview                │\n└──────────────────┴─────────────────────────────────┘\n\n\n\n\nproperty_table = rich.table.Table(\"Property Name\", \"Property Value\")\nfor key, value in sorted(items[-1].properties.items()):\n    property_table.add_row(key, str(value))\nproperty_table\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Property Name  ┃ Property Value                                                                                 ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ datetime       │ None                                                                                           │\n│ end_datetime   │ 2017-12-31T23:59:59Z                                                                           │\n│ proj:epsg      │ 4326                                                                                           │\n│ proj:shape     │ [7992, 7992]                                                                                   │\n│ proj:transform │ [0.0008983152841195215, 0.0, -115.38597824385106, 0.0, -0.0008983152841195215,                 │\n│                │ 34.74464974521749, 0.0, 0.0, 1.0]                                                              │\n│ start_datetime │ 2017-01-01T00:00:00Z                                                                           │\n└────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\nWe can see from the tables, that the Planetary Computer includes a “Rendered Preview”. Let’s take a look at the 2017 data:\n\n# Plot rendered preview\nImage(url=phx_2017.assets['rendered_preview'].href, width=500)"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#about",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#about",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "",
    "text": "Purpose: In 2021, Maricopa County —home to the Phoenix metropolitan area— was identified as the U.S. county with the most significant increase in developed land since 2001. This rapid urban sprawl has profound implications for biodiversity and the health of surrounding natural ecosystems.\nIn this notebook, I will investigate the impacts of urban expansion by analyzing a dataset that captures values for the Biodiversity Intactness Index (BII). Apecifically, I will examine changes in BII in the Phoenix county subdivision area between 2017 and 2020, shedding light on how urban growth affects biodiversity over time.\nHighlights: 1. Accessing the Microsoft Planetary Computer to Access their Impact Observatory data.\n\nCreating a plot of the Biodiversity Intactness Index within the Phoenix subdivision polygon.\nCalculate the percentage of area of the Phoenix area with a BII index of at least .07 in 2017 and 2020.\nPlotting the 2020 data, revealing areas that show a loss of biodiversity.\n\nAbout the data: 1. The first data set is is the Biodiversity Intactness Index (BII) Time Series. Access the io-biodiversity collection from the Microsoft Planetary Computer STAC catalog. I will be using the 2017 and 2020 rasters covering the Phoenix subdivision.\n\nThe second data set is the Phoenix Subdivision Shapefile Download the Phoenix subdivision polygon from the Census County Subdivision shapefiles for Arizona. All legal boundaries and names are as of January 1, 2024. The 2024 TIGER/Line Shapefiles were released on September 25, 2024. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions\n\n\n\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nMicrosoft Planetary Computer, STAC Catalog. Biodiversity Intactness (‘io-biodiversity’). [Dataset]. https://planetarycomputer.microsoft.com/dataset/io-biodiversity Accessed 2 December 2024.\nUnited States Census Bureau. (2022). Arizona County Subdivision 2022 TIGER/Line Shapefiles. [Data File]. U.S. Census Bureau, Geography Division. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions Accessed 2 December 2024.\nBoth of these datasets were accessed for this analysis on 12/2/2024.\n\n# Load Libraries\nimport pandas as pd\nimport geopandas as gpd\nimport planetary_computer\nimport pystac_client\nimport rich.table\nfrom geogif import gif\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport rioxarray as rioxr\nfrom IPython.display import Image \nfrom shapely.geometry import box\nimport xarray as xr\nimport os\nimport rasterio\nfrom rasterio.windows import from_bounds\nfrom IPython.display import Image\nimport contextily as ctx\nimport matplotlib.patches as mpatches\nfrom matplotlib.lines import Line2D\n\nTo begin the analysis, I will first read in and explore the shapefile data that I have for Arizona, specifically looking for the Phoenix area.\n\n# Read in shapefile data for Arizona\narizona = gpd.read_file('data/tl_2022_04_cousub.shp')\narizona.head(3)\n\n\n\n\n\n\n\n\nSTATEFP\nCOUNTYFP\nCOUSUBFP\nCOUSUBNS\nGEOID\nNAME\nNAMELSAD\nLSAD\nCLASSFP\nMTFCC\nCNECTAFP\nNECTAFP\nNCTADVFP\nFUNCSTAT\nALAND\nAWATER\nINTPTLAT\nINTPTLON\ngeometry\n\n\n\n\n0\n04\n005\n91198\n01934931\n0400591198\nFlagstaff\nFlagstaff CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n12231052883\n44653332\n+35.1066114\n-111.3662497\nPOLYGON ((-112.13370 35.85596, -112.13368 35.8...\n\n\n1\n04\n005\n91838\n01934953\n0400591838\nKaibab Plateau\nKaibab Plateau CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n7228864534\n29327221\n+36.5991097\n-112.1368033\nPOLYGON ((-112.66039 36.53941, -112.66033 36.5...\n\n\n2\n04\n005\n91683\n01934950\n0400591683\nHualapai\nHualapai CCD\n22\nZ5\nG4040\nNone\nNone\nNone\nS\n2342313339\n3772690\n+35.9271665\n-113.1170408\nPOLYGON ((-113.35416 36.04097, -113.35416 36.0...\n\n\n\n\n\n\n\n\n# Clean up Arizona data\narizona.columns = arizona.columns.str.lower()\n\n# Select just the Phoenix area\nphoenix = arizona[arizona.name == \"Phoenix\"]\n\n# Generate a quick plot of the Phoenix shape\nphoenix.plot()\n\n\n\n\n\n\n\n\nWhen making maps it’s important to know what Coordinate Reference System we are dealing with. Let’s check what we have for the Phoenix file:\n\nphoenix.crs\n\n&lt;Geographic 2D CRS: EPSG:4269&gt;\nName: NAD83\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.\n- bounds: (167.65, 14.92, -40.73, 86.45)\nDatum: North American Datum 1983\n- Ellipsoid: GRS 1980\n- Prime Meridian: Greenwich\n\n\nNow that I have my Arizona data looking how I want it, lets access the Microsoft Planetary Computer!\n\n# Access the MPC catalog\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\n\ncatalog.get_collections()\ncollections = list(catalog.get_collections())\n\n# Print the number of collections\nprint('Number of collections:', len(collections))\n\n#Pull out the Impact Observatory collection\nio_collection = catalog.get_child('io-biodiversity')\nio_collection\n\nNumber of collections: 124\n\n\n\n\n\n\n    \n        \n            \n                \n                    \n        \n            type\n            \"Collection\"\n        \n    \n                \n            \n                \n                    \n        \n            id\n            \"io-biodiversity\"\n        \n    \n                \n            \n                \n                    \n        \n            stac_version\n            \"1.0.0\"\n        \n    \n                \n            \n                \n                    \n        \n            description\n            \"Generated by [Impact Observatory](https://www.impactobservatory.com/), in collaboration with [Vizzuality](https://www.vizzuality.com/), these datasets estimate terrestrial Biodiversity Intactness as 100-meter gridded maps for the years 2017-2020.\n\nMaps depicting the intactness of global biodiversity have become a critical tool for spatial planning and management, monitoring the extent of biodiversity across Earth, and identifying critical remaining intact habitat. Yet, these maps are often years out of date by the time they are available to scientists and policy-makers. The datasets in this STAC Collection build on past studies that map Biodiversity Intactness using the [PREDICTS database](https://onlinelibrary.wiley.com/doi/full/10.1002/ece3.2579) of spatially referenced observations of biodiversity across 32,000 sites from over 750 studies. The approach differs from previous work by modeling the relationship between observed biodiversity metrics and contemporary, global, geospatial layers of human pressures, with the intention of providing a high resolution monitoring product into the future.\n\nBiodiversity intactness is estimated as a combination of two metrics: Abundance, the quantity of individuals, and Compositional Similarity, how similar the composition of species is to an intact baseline. Linear mixed effects models are fit to estimate the predictive capacity of spatial datasets of human pressures on each of these metrics and project results spatially across the globe. These methods, as well as comparisons to other leading datasets and guidance on interpreting results, are further explained in a methods [white paper](https://ai4edatasetspublicassets.blob.core.windows.net/assets/pdfs/io-biodiversity/Biodiversity_Intactness_whitepaper.pdf) entitled “Global 100m Projections of Biodiversity Intactness for the years 2017-2020.”\n\nAll years are available under a Creative Commons BY-4.0 license.\n\"\n        \n    \n                \n            \n                \n                    \n        links[] 7 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            rel\n            \"items\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/io-biodiversity/items\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/geo+json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            rel\n            \"root\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            rel\n            \"license\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://creativecommons.org/licenses/by/4.0/\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"CC BY 4.0\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            \n        \n            \n                \n        \n            rel\n            \"about\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pdfs/io-biodiversity/Biodiversity_Intactness_whitepaper.pdf\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/pdf\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Technical White Paper\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            4\n            \n        \n            \n                \n        \n            rel\n            \"describedby\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/dataset/io-biodiversity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"text/html\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Human readable dataset overview and reference\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            5\n            \n        \n            \n                \n        \n            rel\n            \"self\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1/collections/io-biodiversity\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            6\n            \n        \n            \n                \n        \n            rel\n            \"parent\"\n        \n    \n            \n        \n            \n                \n        \n            href\n            \"https://planetarycomputer.microsoft.com/api/stac/v1\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/json\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Microsoft Planetary Computer STAC API\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        stac_extensions[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"https://stac-extensions.github.io/item-assets/v1.0.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"https://stac-extensions.github.io/raster/v1.1.0/schema.json\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"https://stac-extensions.github.io/table/v1.2.0/schema.json\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            item_assets\n            \n        \n            \n                \n        \n            data\n            \n        \n            \n                \n        \n            type\n            \"image/tiff; application=geotiff; profile=cloud-optimized\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"data\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Terrestrial biodiversity intactness at 100m resolution\"\n        \n    \n            \n        \n            \n                \n        raster:bands[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            sampling\n            \"area\"\n        \n    \n            \n        \n            \n                \n        \n            data_type\n            \"float32\"\n        \n    \n            \n        \n            \n                \n        \n            spatial_resolution\n            100\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            msft:region\n            \"westeurope\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:container\n            \"impact\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:storage_account\n            \"pcdata01euw\"\n        \n    \n                \n            \n                \n                    \n        \n            msft:short_description\n            \"Global terrestrial biodiversity intactness at 100m resolution for years 2017-2020\"\n        \n    \n                \n            \n                \n                    \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n                \n            \n                \n                    \n        \n            extent\n            \n        \n            \n                \n        \n            spatial\n            \n        \n            \n                \n        bbox[] 1 items\n        \n            \n        \n            \n                \n        0[] 4 items\n        \n            \n        \n            \n                \n        \n            0\n            -180\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            -90\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            180\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            3\n            90\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            temporal\n            \n        \n            \n                \n        interval[] 1 items\n        \n            \n        \n            \n                \n        0[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"2017-01-01T00:00:00Z\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"2020-12-31T23:59:59Z\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            license\n            \"CC-BY-4.0\"\n        \n    \n                \n            \n                \n                    \n        keywords[] 2 items\n        \n            \n        \n            \n                \n        \n            0\n            \"Global\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"Biodiversity\"\n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        providers[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \n        \n            \n                \n        \n            name\n            \"Impact Observatory\"\n        \n    \n            \n        \n            \n                \n        roles[] 3 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \"producer\"\n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \"licensor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.impactobservatory.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            1\n            \n        \n            \n                \n        \n            name\n            \"Vizzuality\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"processor\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://www.vizzuality.com/\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n            \n        \n            \n                \n        \n            2\n            \n        \n            \n                \n        \n            name\n            \"Microsoft\"\n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"host\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            url\n            \"https://planetarycomputer.microsoft.com\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            summaries\n            \n        \n            \n                \n        version[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"v1\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n                \n                    \n        \n            assets\n            \n        \n            \n                \n        \n            thumbnail\n            \n        \n            \n                \n        \n            href\n            \"https://ai4edatasetspublicassets.blob.core.windows.net/assets/pc_thumbnails/io-biodiversity-thumb.png\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"Biodiversity Intactness\"\n        \n    \n            \n        \n            \n                \n        \n            media_type\n            \"image/png\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            geoparquet-items\n            \n        \n            \n                \n        \n            href\n            \"abfs://items/io-biodiversity.parquet\"\n        \n    \n            \n        \n            \n                \n        \n            type\n            \"application/x-parquet\"\n        \n    \n            \n        \n            \n                \n        \n            title\n            \"GeoParquet STAC items\"\n        \n    \n            \n        \n            \n                \n        \n            description\n            \"Snapshot of the collection's STAC items exported to GeoParquet format.\"\n        \n    \n            \n        \n            \n                \n        \n            msft:partition_info\n            \n        \n            \n                \n        \n            is_partitioned\n            False\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        \n            table:storage_options\n            \n        \n            \n                \n        \n            account_name\n            \"pcstacitems\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n            \n                \n        roles[] 1 items\n        \n            \n        \n            \n                \n        \n            0\n            \"stac-items\"\n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n            \n        \n    \n        \n    \n                \n            \n        \n    \n\n\n\nNow that we have our data, we need to specify the temporal and spatial information we are interested in. I will create a bounding box, that specifies our spatial area of interest.\nAs we can see from the description of the Impact Observatory Collection above, the only dates represented in the data are our time range of interest (2017-2020). As such, there is no need to specify a temporal variable.\n\n# Create bounding box\nbbox_of_interest = [-112.826843, 32.974108, -111.184387, 33.863574]\n\n\n# Catalog search\nsearch = catalog.search(\n    collections = ['io-biodiversity'],\n    bbox = bbox_of_interest)\n\n# Get items from search\nitems = search.item_collection()\n\n# Determine number of items in search\nprint(f'There are {len(items)} items in the search.')\n\nThere are 4 items in the search.\n\n\n\n# Retrieve items\nitem_names = {item.id : item for item in search.items()}\nlist(item_names) # from this list, we can see that our 1st item is 2020 data, and our 4th item is 2017 data\n\n# Select 2017 subset\nphx_2017 = items[3]\n\n# Select 2020 subset\nphx_2020 = items[0]\n\nOur search returned four STAC Items. We can tell from their IDs that that they contain data for the same area but for different times, specifically the years 2017 through 2020. Let’s display the available assets and properties for the 2017 Item.\n\nasset_table = rich.table.Table(\"Asset Key\", \"Asset Title\")\nfor key, value in items[-1].assets.items():\n    asset_table.add_row(key, value.title)\nasset_table\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Asset Key        ┃ Asset Title                     ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ data             │ Biodiversity Intactness         │\n│ tilejson         │ TileJSON with default rendering │\n│ rendered_preview │ Rendered preview                │\n└──────────────────┴─────────────────────────────────┘\n\n\n\n\nproperty_table = rich.table.Table(\"Property Name\", \"Property Value\")\nfor key, value in sorted(items[-1].properties.items()):\n    property_table.add_row(key, str(value))\nproperty_table\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Property Name  ┃ Property Value                                                                                 ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ datetime       │ None                                                                                           │\n│ end_datetime   │ 2017-12-31T23:59:59Z                                                                           │\n│ proj:epsg      │ 4326                                                                                           │\n│ proj:shape     │ [7992, 7992]                                                                                   │\n│ proj:transform │ [0.0008983152841195215, 0.0, -115.38597824385106, 0.0, -0.0008983152841195215,                 │\n│                │ 34.74464974521749, 0.0, 0.0, 1.0]                                                              │\n│ start_datetime │ 2017-01-01T00:00:00Z                                                                           │\n└────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\nWe can see from the tables, that the Planetary Computer includes a “Rendered Preview”. Let’s take a look at the 2017 data:\n\n# Plot rendered preview\nImage(url=phx_2017.assets['rendered_preview'].href, width=500)"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#visualization-map-of-phoenix-county-az",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#visualization-map-of-phoenix-county-az",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Visualization: Map of Phoenix County AZ",
    "text": "Visualization: Map of Phoenix County AZ\nIn order to better visualize the Phoenix area, I will make a map, and add in a basemap using the Contextily package. This map will give us a good visual of the greater Phoenix area, and will better help us understand our biodiveristy analysis later.\n\n# Set up the figure\nfig, ax = plt.subplots()\n\n# Create the axis with plot\nphoenix_merc = phoenix.to_crs(epsg=3857)\nphoenix_merc.plot(ax=ax, figsize=(11, 10), alpha=0.45, edgecolor=\"k\", label=\"Phoenix Boundary\")\n\n# Add NatGeo basemap from contextily\nctx.add_basemap(ax=ax, source=ctx.providers.Esri.NatGeoWorldMap)\n\n# Add legend\nlegend_elements = [Line2D([0], [0], color='k', alpha=0.45, lw=3, label='Phoenix County Boundary')]\nax.legend(handles=legend_elements, loc=\"upper left\", fontsize=12)\n\n# Update axes\nax.set_title(\"Phoenix County, Arizona\", fontdict={\"fontsize\": 20})\nax.set_axis_off()\n\nplt.show()"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#biodiversity-intactness-index",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#biodiversity-intactness-index",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Biodiversity Intactness Index",
    "text": "Biodiversity Intactness Index\nNow that we have a good basemap, showing roads and ciites for context, we can work on also plotting our Plantary Computer data, to see biodiversity intactness in this highly urban area.\nRemembering the data tables created above, I know that I am interested the “data” asset, which should contain the information that I need to continue my comparative analysis. Since my analysis will involve directly comparing the years 2017 and 2020, I will create variables defining the assets of those years specifically.\n\n# Retrieve 2017 biodiversity data \nphx_2017_asset = phx_2017.assets[\"data\"]\nphx_2017_data = rioxr.open_rasterio(phx_2017_asset.href)\n\n# Retrieve 2020 biodiversity data \nphx_2020_asset = phx_2020.assets[\"data\"]\nphx_2020_data = rioxr.open_rasterio(phx_2020_asset.href)\n\nphx_2017_data\nphx_2020_data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray (band: 1, y: 7992, x: 7992)&gt; Size: 255MB\n[63872064 values with dtype=float32]\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 64kB -115.4 -115.4 -115.4 ... -108.2 -108.2 -108.2\n  * y            (y) float64 64kB 34.74 34.74 34.74 34.74 ... 27.57 27.57 27.57\n    spatial_ref  int64 8B 0\nAttributes:\n    AREA_OR_POINT:  Area\n    scale_factor:   1.0\n    add_offset:     0.0xarray.DataArrayband: 1y: 7992x: 7992...[63872064 values with dtype=float32]Coordinates: (4)band(band)int641array([1])x(x)float64-115.4 -115.4 ... -108.2 -108.2array([-115.385529, -115.384631, -115.383732, ..., -108.208888, -108.20799 ,\n       -108.207092])y(y)float6434.74 34.74 34.74 ... 27.57 27.57array([34.744201, 34.743302, 34.742404, ..., 27.56756 , 27.566661, 27.565763])spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-115.38597824385106 0.0008983152841195215 0.0 34.74464974521749 0.0 -0.0008983152841195215array(0)Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([-115.38552908620899, -115.38463077092487, -115.38373245564075,\n       -115.38283414035664, -115.38193582507252,  -115.3810375097884,\n       -115.38013919450427, -115.37924087922015, -115.37834256393603,\n       -115.37744424865191,\n       ...\n       -108.21517648836696, -108.21427817308285, -108.21337985779873,\n       -108.21248154251461, -108.21158322723049, -108.21068491194637,\n       -108.20978659666225, -108.20888828137814, -108.20798996609402,\n        -108.2070916508099],\n      dtype='float64', name='x', length=7992))yPandasIndexPandasIndex(Index([ 34.74420058757543,  34.74330227229131,  34.74240395700719,\n       34.741505641723066,  34.74060732643895,  34.73970901115483,\n        34.73881069587071,  34.73791238058659, 34.737014065302475,\n        34.73611575001835,\n       ...\n        27.57384798973341, 27.572949674449287,  27.57205135916517,\n       27.571153043881047,  27.57025472859693,  27.56935641331281,\n        27.56845809802869,  27.56755978274457,  27.56666146746045,\n        27.56576315217633],\n      dtype='float64', name='y', length=7992))Attributes: (3)AREA_OR_POINT :Areascale_factor :1.0add_offset :0.0\n\n\nSince there is only 1 band, we can remove that dimension for a more easily managed dataset.\n\nphx_2017_data = phx_2017_data.squeeze().drop_vars('band')\nphx_2020_data = phx_2020_data.squeeze().drop_vars('band')\n\nNow, I need to clip the Planetary Computer data to match the Phoenix polygon I created earlier. I will make sure that my Coordinate Reference Systems match, before clipping and plotting.\n\n# Match CRSs\nphoenix = phoenix.to_crs(phx_2017_data.rio.crs)\n\n# Assert check to ensure it worked\nassert phoenix.crs == phx_2017_data.rio.crs\n\n# Clip 2017 BII raster to the Phoenix polygon\nphx_clip_2017 = phx_2017_data.rio.clip(phoenix[\"geometry\"])\n\n# And again for 2020\nphx_clip_2020 =  phx_2020_data.rio.clip(phoenix[\"geometry\"])\n\n# Let's see how that looks for 2017\nphx_clip_2017.plot()"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#calculating-the-percentage-of-area-of-the-phoenix-subdivision-with-a-bii-of-at-least-0.75-in-2017-and-2020.",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#calculating-the-percentage-of-area-of-the-phoenix-subdivision-with-a-bii-of-at-least-0.75-in-2017-and-2020.",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Calculating the percentage of area of the Phoenix subdivision with a BII of at least 0.75 in 2017 and 2020.",
    "text": "Calculating the percentage of area of the Phoenix subdivision with a BII of at least 0.75 in 2017 and 2020.\nIn order to do this calculation, I need to do a bit of “map algebra”.\n\nI need to find the total number of pixels in the Phoenix subdivision polygon, for both years.\nThen, I need to find the numbers of pixels with the BII at 0.75 or higher, again for both years.\nFinally, I can use those pixel numbers I get to calculate the desired percentage.\n\n\n# Calculate total area for 2017\ntotal_2017 = phx_clip_2017.count().item() \n# Calculate total area for 2020\ntotal_2020 = phx_clip_2020.count().item()\n\n# Calculate BII % for 2017, Make the data binary for easy math\nphx_bii_2017 = (phx_clip_2017 &gt;= 0.75).astype(int)\npixels_2017 = phx_bii_2017.sum().item()\n\n# Calculate BII % for 2020\nphx_bii_2020 = (phx_clip_2020 &gt;= 0.75).astype(int)\npixels_2020 = phx_bii_2020.sum().item()\n\n\n\n# Calcualte the percentage area for 2017 data:\npct_bii_2017 =  (pixels_2017 / total_2017) * 100\n\n# Calculate the percentage area for 2020 data:\npct_bii_2020 = (pixels_2020 / total_2020) * 100\n\n# Print output\npct_text = \"The percentage of area in Phoenix County with a BII over 0.75 in\"\nprint(pct_text, \"2017 is: \", round(pct_bii_2017, 2), \"%\")\n\nprint(pct_text, \"2020 is: \", round(pct_bii_2020, 2), \"%\")\n\nThe percentage of area in Phoenix County with a BII over 0.75 in 2017 is:  7.13 %\nThe percentage of area in Phoenix County with a BII over 0.75 in 2020 is:  6.49 %"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#visualization-biodiversity-loss",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#visualization-biodiversity-loss",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Visualization: Biodiversity Loss",
    "text": "Visualization: Biodiversity Loss\nNow that we have calculated the differences in biodiversity density between the two years, we can visualize this difference with a map.\n\n# Find areas where there was a loss in BII from 2017 to 2020\nphx_bii_diff = phx_bii_2017 - phx_bii_2020\n\n# Make a mask - like in 223!\nphx_mask = phx_bii_diff == 1\n\n# Convert to int type\nphx_mask = phx_mask.astype(int)\n\n# Make a color gradient\ncolor_id = [\"none\", \"red\"]\n\n# Make a color map object\ncmap = plt.cm.colors.ListedColormap(color_id)\n\n# Plot!\nphx_mask.plot(cmap = cmap, add_colorbar = False)\n\n\n\n\n\n\n\n\nNow that we have applied a color map visualization to our biodiversity loss, we can overlay this with our Planetary Computer raster, and our Phoenix subdivision polygon, to create a final visualization that shows us the area, in red, where Phoenix county experienced significant biodiversity losee."
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#final-visualization",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#final-visualization",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Final Visualization:",
    "text": "Final Visualization:\n\n# Initialize plot\nfig, ax = plt.subplots(figsize = (8, 7))\n\n# Remove axes\nax.axis(\"off\")\n\n# Begin with clipped raster layer\nphx_clip_2020.plot(ax = ax,\n                  cbar_kwargs = {\"location\": \"bottom\",\n                                \"label\": \"BII in 2020\"})\n# Add mask layer\nphx_mask.plot(ax = ax,\n             cmap = cmap,\n             add_colorbar = False)\nphx_mask_patch = mpatches.Patch(color = \"red\",\n                      label = \"Area with 75% Biodiversity Loss between 2017 and 2020\")\n# Add polygon layer\nphoenix.plot(ax = ax, \n            color = \"none\",\n            edgecolor = \"black\",\n            linewidth = 2)\n\n# update legend, add in BII loss patch\nax.legend(handles = [phx_mask_patch],\n          frameon = False,\n          bbox_to_anchor = (0.85, -0.07))\nax.set_title(\"Biodiversity Intactness Index (BII) and Biodiversity Loss in Phoenix, AZ\")\n\nplt.show()"
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#conclusion",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#conclusion",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Conclusion",
    "text": "Conclusion\nPython and Jupyter Notebooks, as we can see, are powerful tools that allow us to conduct high level analyis with large amounts of spatial data.\nIn this analysis, we can use these tools to visualize the sharp decrease in biodiversity in this area, perhaps due to urbanization and human impact. Such an analysis can be used to further understand the overall biodiversity crisis, and can help inform decision makers about the needs and priorities of conservation efforts."
  },
  {
    "objectID": "posts/2024-09-29-BII/BII_analysis (1).html#citations-and-data-access",
    "href": "posts/2024-09-29-BII/BII_analysis (1).html#citations-and-data-access",
    "title": "Biodiversity Intactness Index (BII) change in Phoenix, AZ",
    "section": "Citations and Data Access:",
    "text": "Citations and Data Access:\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nMicrosoft Planetary Computer, STAC Catalog. Biodiversity Intactness (‘io-biodiversity’). [Dataset]. https://planetarycomputer.microsoft.com/dataset/io-biodiversity Accessed 2 December 2024.\nUnited States Census Bureau. (2022). Arizona County Subdivision 2022 TIGER/Line Shapefiles. [Data File]. U.S. Census Bureau, Geography Division. https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2022&layergroup=County+Subdivisions Accessed 2 December 2024."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Bailey Jørgensen",
    "section": "",
    "text": "Nice to e-meet you, I’m Bailey.\n\n\nData management and mapping is my day-passion. Marine secrets and wonders are my life-passion. When I’m not deepening my technical knowledge of data and coding at UC Santa Barbara Bren, caring for an active research collection and exhibits hall at the Alf Museum of Paleontology takes up most of my professional time. Sailing and preserving historic wooden ships for maritime museums and institutes ignited my interest in the heritage sector. Paleontological fieldwork and data and collection in the deserts of Utah, California, Wyoming, Montana and Mongolia solidified it. Have a project in the biodiversity heritage sector? (Bonus points if its for marine environments.) Let’s chat!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bailey Jørgensen",
    "section": "",
    "text": "Experimenting with a DEEP SEA CORAL interactions model\n\n\nAnalyzing the relationship between deep sea corals, latitude, and ocean depth\n\n\n\nBAILEY JØRGENSEN\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R Analysis of Landcover Classification using Decision Trees\n\n\nUsing multi-spectral imagery on the location of 4 land cover types, with a machine learning twist\n\n\n\nBailey Jørgensen\n\n\nNov 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire Analysis\n\n\nAQI and spatial analysis walkthrough of the 2017 natural disaster from the perspective of a Masters of Environmental Data Science student\n\n\n\nBailey Jørgensen\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiodiversity Intactness Index (BII) change in Phoenix, AZ\n\n\nA geospatial analysis in Python\n\n\n\nBailey Jørgensen\n\n\nSep 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post Title\n\n\na short catchy description of the blog post\n\n\n\nBailey Jørgensen\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html",
    "href": "posts/2024-10-18-my-first-post/index.html",
    "title": "Blog Post Title",
    "section": "",
    "text": "I’m going to insert a footnote here1"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#this-is-my-first-section",
    "href": "posts/2024-10-18-my-first-post/index.html#this-is-my-first-section",
    "title": "Blog Post Title",
    "section": "",
    "text": "I’m going to insert a footnote here1"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#this-is-my-second",
    "href": "posts/2024-10-18-my-first-post/index.html#this-is-my-second",
    "title": "Blog Post Title",
    "section": "This is my second",
    "text": "This is my second\nHere’s my next paragraph2\nhere is more random text. im going to cite a journal article now.(Gaynor et al. 2022)"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#footnotes",
    "href": "posts/2024-10-18-my-first-post/index.html#footnotes",
    "title": "Blog Post Title",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is a new footnote↩︎\nHere is my second footnote↩︎"
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-context",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-context",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "THE CONTEXT:",
    "text": "THE CONTEXT:\nIt is estimated that two-thirds of all known coral species can be classified as “deep-sea corals”. These corals exhibit a diversity similar to those of their shallow-water relatives, but they do not form symbiotic relationships with algae, do not obtain energy from sunlight (instead feeding from microorganisms in the water), and are much more resilient to cold temperatures.\nAlso similar to their shallow water counterparts, deep sea corals are hosts to many other forms of biodiversity. They are also extremely slow growing, and as a result, slow to recover from ecological or anthropogenic disturbance[2].\nThis analysis will seek to more deeply understand the environments in which deep sea corals are more likely to appear."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-data",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-data",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "THE DATA:",
    "text": "THE DATA:\nThe data used for this analysis comes from NOAA’s Deep Sea Coral Research and Technology Program (DSCRTP), and is housed at the Smithsonian National Museum of Natural History, Invertebrate Zoology Collection. It contains decades worth of data regarding deep sea corals, their taxa, spatial data, and collecting data. It is an open source data set, that is consistently being updated. (See link in the citations section, to take a look for yourself!)[3]\nHere are some details from the metadata of the dataset:\n\nNumber of records: 30,850\nNumber of coral records: 24,768\nNumber of sponge records: 6,082\nRecords with images: 245\n\nSeems like we have a lot of data points to work with here. Let’s begin by loading our libraries and reading in the data!\nThe deep sea corals dataset I chose also includes deep sea sponges. Since this analysis wants to focus on corals, I’ll have to filter that data out. In addition, there are several columns on the dataframe that I know won’t be helpful for my analysis (for example, collector data, institution data, or columns that contains a lot of NAs.) In addition to filtering out the sponge data, I will filter out those unwanted columns as well.\n\n\nCode\n# Filter data to only include columns I want\ncorals &lt;- corals %&gt;% \n  clean_names(case = \"snake\") %&gt;% # Change column names to snake_case\n  select(scientific_name, phylum, genus, species, individual_count, latitude, longitude, depth_in_meters, country) %&gt;% # Choose columns I want\n  filter(phylum %in% c(\"Cnidaria\")) # Filter to exclude sponges and NA row\n\n\nI know that in order to properly visualize my data, I’m going to want to make some maps. However, I also want the latitude and longitude data to be easily handled during any linear model calculations, so I decide to make a separate dataset variable, that transforms the lat long data into geometric data that I can easily add to tmaps for visualizations.\n\n\nCode\ncorals_transformed &lt;- corals %&gt;% \n  sf::st_as_sf(coords = c(\"longitude\", \"latitude\"),\n               crs = st_crs(4326)) %&gt;% \n              filter(st_is_valid(.))\n\n\nLet’s take a look at the result of our well filtered data! This data cleaning will enable us to run effective and easy to understand data analysis in the next few steps.\n\n\nCode\n# Create a table showing the head of our filtered dataframe\nkable(head(corals), format = \"html\", caption = \"Preview of Filtered Global Coral Data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\nPreview of Filtered Global Coral Data\n\n\nscientific_name\nphylum\ngenus\nspecies\nindividual_count\nlatitude\nlongitude\ndepth_in_meters\ncountry\n\n\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n2\n34.95839\n-75.32464\n146\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n6\n34.95839\n-75.32464\n146\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n6\n29.28357\n-88.26665\n84\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n1\n24.48375\n-80.88314\n191\nUSA\n\n\nDesmophyllum pertusum\nCnidaria\nDesmophyllum\npertusum\n1\n30.96684\n-79.69976\n396\nUSA\n\n\nDesmophyllum pertusum\nCnidaria\nDesmophyllum\npertusum\n1\n27.98361\n-79.3331\n577\nBahamas"
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-exploration",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-exploration",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "THE EXPLORATION:",
    "text": "THE EXPLORATION:\n\nVISUALIZATION\nI want to take a look at the whole scale of my data set. To do this, I decide to make a global map.\n\n\nCode\nworld &lt;- spData::world\n\ntm_shape(world) +\n  tm_fill(col = \"continent\",\n          palette = c(\"slategray\", \"snow3\", \"slategray2\", \"slategray3\", \"slategray4\", \"lightslategrey\", \"lightsteelblue4\", \"snow4\"),\n          title = \"Continents\") +\n  tm_shape(corals_transformed) +\n  tm_dots(col = \"phylum\",\n          palette = \"pink2\",\n          size = 0.05,\n          border.col = \"black\",\n          title = \"Data Point\") +\n  # tm_compass(type = \"4star\",\n  #            size = .05,\n  #            position = c(\"left\", \"top\")) +\n  tm_layout(main.title = \"Global Coral Observations\",\n            title.position = c(\"center\", \"top\"),\n            title.snap.to.legend = FALSE,\n            #frame = TRUE, \n            #legend.frame = TRUE,\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\nFrom this global map, we can see the huge volume to coral observations present in our dataset. In order to get a little more focus in my model, I decide filter the datset to a specific country, and run my analysis on that.\nIt could be interesting to, in the future, run the same analysis on each continent, and then cross-compare them. This would reveal interesting insights about different continents and species. Politically, it could also be an interesting view into how collecting bias might affect the results of such an analysis, since we can see that some continents contain more sample points than others. Unfortunately, this could be due to the fact that certain countries and regions are more studied than others, due to data collecting bias in the environmental sector.[1]\nFor the purpose of this study, however, I will filter the coral data to the Philippines. The Philippines is an area known for its coral diversity and it’s popularity in the diving community. I am curious to see how DEEP SEA corals are represented in the region, rather than the easily dive-able shallow coral systems.\n\n\nCode\nphil_coral &lt;- corals %&gt;% \n  filter(country %in% \"Philippines\") %&gt;% \n  mutate(\n    latitude = as.numeric(latitude),\n    longitude = as.numeric(longitude),\n    depth_in_meters = as.numeric(depth_in_meters),\n    latitude_rounded = round(latitude, 1)\n  ) %&gt;% \n  add_count(latitude_rounded, name = \"obs_count\")\n\n# Transformed dataset for mapping\nphil_coral_transformed &lt;- corals_transformed %&gt;% \n  filter(country %in% \"Philippines\")\n\n# Create a table showing the head of our filtered dataframe\nkable(head(phil_coral), format = \"html\", caption = \"Preview of Filtered Philippines Coral Data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\nPreview of Filtered Philippines Coral Data\n\n\nscientific_name\nphylum\ngenus\nspecies\nindividual_count\nlatitude\nlongitude\ndepth_in_meters\ncountry\nlatitude_rounded\nobs_count\n\n\n\n\nCoralliidae\nCnidaria\nNA\nNA\n1\n15.9708\n119.672\n1719\nPhilippines\n16.0\n3\n\n\nStenohelia tiliata\nCnidaria\nStenohelia\ntiliata\n1\n6.1333\n121.317\n275\nPhilippines\n6.1\n25\n\n\nStylaster multiplex\nCnidaria\nStylaster\nmultiplex\n1\n5.1867\n119.590\n450\nPhilippines\n5.2\n16\n\n\nDistichopora irregularis\nCnidaria\nDistichopora\nirregularis\n1\n7.0853\n125.662\n42\nPhilippines\n7.1\n8\n\n\nDistichopora irregularis\nCnidaria\nDistichopora\nirregularis\n4\n7.0950\n125.662\n38\nPhilippines\n7.1\n8\n\n\nDistichopora\nCnidaria\nDistichopora\nNA\n1\n7.0867\n125.660\n37\nPhilippines\n7.1\n8\n\n\n\n\n\n\n\n\n\nCode\n# Get data for base map of Philippines\nph &lt;- ne_countries(scale = 10, country = \"Philippines\", returnclass = \"sf\")\n\n# Make a plot of corals in the Philippines\ntm_shape(ph) +\n  tm_polygons(col = \"name\",\n          palette = c(\"slategray\", \"snow3\", \"slategray2\", \"slategray3\", \"slategray4\", \"lightslategrey\", \"lightsteelblue4\", \"snow4\"),\n          title = \"Philippines\") +\n  tm_shape(phil_coral_transformed) +\n  tm_dots(col = \"phylum\",\n          palette = \"pink2\",\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Data Point\") +\n   tm_compass(type = \"8star\",\n              size = 2,\n              position = c(\"left\", \"top\")) +\n  tm_layout(main.title = \"Coral Observations \\nin the Philippines\",\n            title.position = c(\"center\", \"top\"),\n            title.snap.to.legend = FALSE,\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nTHE SUMMARIZATION:\nAs we can see from the map above, there have been deep sea coral collections happening all throughout the country of the Philippines. This gives me a lot of data to run analysis on. I decided to make some additional visualizations to summarize the contents of the data, and guide me toward making a null and alternate hypothesis.\nFirst, I want to take a look at the distribution of coral observations along the different latitudes of the Philippines. In the data cleaning I performed above, I decided to create another column on the Philippines coral dataframe that rounded the latitudes to just two decimal places. This will allow me to work with latitiude data more easily, without having too many latitude points obscuring the visualizations of my data. I also added a column that summed the total number of coral observations at each of these rounded latitudes. This number will allow me to analyze the abundance of corals at each rounded latitude, enabling analysis of coral distributions.\nIt is using the rounded latitudes column(latitude_rounded), and the the observations per latitude column(obs_count) that I made the following data visualizations.\nFirst up, I wanted to take an initial look at these two columns:\n\n\nCode\n# Make a plot of coral obs and latitude\nggplot() +\n geom_line(data = phil_coral,\n            aes(x = latitude_rounded,\n                y = obs_count),\n           color = \"pink2\",\n           size = 1) +\n  labs(x = \"Latitude\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Deep Sea Corals at Latitude\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows the general distribution of the corals along the different latitudes, and visualizes a significant spike in abundance around the 14 degrees latitude line.\nI wanted a way to visualize depth fluctuations along these latitude lines, in order to help visualize general depth patters in the area of interest. I decided to do another line graph, this one working almost like a cross section of depth along latitude, though of course only including depths available on our dataframe, so not actually having spatial representation, just data representation:\n\n\nCode\n# Make a plot\nggplot() +\n geom_line(data = phil_coral,\n            aes(x = latitude_rounded,\n                y = depth_in_meters),\n           color = \"slategray\",\n           size = 1) +\n  labs(x = \"Latitude\",\n       y = \"Depth in Meters\",\n       title = \"Depth Fluctuations in the Philippines\") +\n  scale_y_reverse() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis visualization showed some interesting spikes in depths collected from, with another large spike around the 14 degree mark, but several other degrees showing some spikes as well. This helps visualize the depths along latitudes that our corals dwell on, but I want to find a better way of visualizing how these depths might effect coral abundance.\nI decide to create scatterplots that incorporates all three variables, and see how these different visualizations can help me form my hypothesis.\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = latitude_rounded,\n                 y = depth_in_meters,\n                 color = obs_count)) +\n  scale_color_gradient(low = \"black\", high = \"pink2\") +\n  labs(x = \"Latitude\",\n       y = \"Depth in Meters\",\n       title = \"Colored by # of Observations\") +\n    geom_smooth(data = phil_coral,\n              aes(x = latitude_rounded, y = depth_in_meters), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot, with a geom_smooth line with method “lm”, or linear model, shows us that as latitude increases, the depths that corals are found in decreases slightly. This visualization makes it difficult to interpret the number of corals observed, however, though it does show how there are more observations made at the 14 degree latitude.\nI decide to try rearranging what variables go on what axes:\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = latitude_rounded,\n                 y = obs_count,\n                 color = depth_in_meters)) +\n  scale_color_gradient(low = \"skyblue\", high = \"black\") +\n  labs(x = \"Latitude\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Depth, Latitude, and Observation Count\") +\n    geom_smooth(data = phil_coral,\n              aes(x = latitude_rounded, y = obs_count), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows me that as latitude increaases, the number of corals observed also increases. There is an outlier high up around latitude 14 degrees, and I suspect that that outlier is increasing the slope of our lm line by quite a bit. This plot makes it easy to observe the number of observations made, compared to the first plot.\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = depth_in_meters,\n                 y = obs_count,\n                 color = latitude)) +\n  scale_color_gradient(low = \"skyblue\", high = \"black\") +\n  labs(x = \"Depth in Meters\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Depth, Latitude, and Observation Count\") +\n  geom_smooth(data = phil_coral,\n              aes(x = depth_in_meters, y = obs_count), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nFinally, this plot shows us that as depth increases, the number of deep sea corals observed also decreases, with the colors of the points showing us the distribution of the points along latitude, with more observations occurring at higher latitudes.\nAll of these plots seem to show that there is some sort of relationship between all of these variables. Let’s see if we can quantify that relationship!\n\n\nTHE HYPOTHESIS:\nNull: There is no relationship between number of deep sea corals observed, latitude, and depth in the Philippines.\nAlternate: There IS a relationship between number of deep sea corals observed, latitude, and depth in the Philippines."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-analysis",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-analysis",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "THE ANALYSIS:",
    "text": "THE ANALYSIS:\n\nMETHOD:\nSince I have multiple variables to contend with, I decide on using an INTERACTIONS MODEL to understand the relationship between my variables (corals observed, latitude, and depth). My interactions model will take on this format:\n\\[\n\\text{number of deep sea corals observed} \\sim \\text{latitude} + \\text{depth} + \\text{latitude} \\times \\text{depth}\n\\]\nOr, more technically:\n\\[\n\\text{coral observed} = \\beta_0 + \\beta_1 \\cdot \\text{latitude} + \\beta_2 \\cdot \\text{depth} + \\beta_3 \\cdot (\\text{latitude} \\times \\text{depth}) + \\epsilon\n\\]\n\n\nIMPLEMENTATION:\nIn R code, I will create this linear model and then run the summary:\n\n\nCode\n# Create an interactions model\ninteractions &lt;- lm(obs_count ~ latitude_rounded + depth_in_meters + latitude_rounded:depth_in_meters,\n                   data = phil_coral)\nsummary(interactions)\n\n\n\nCall:\nlm(formula = obs_count ~ latitude_rounded + depth_in_meters + \n    latitude_rounded:depth_in_meters, data = phil_coral)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-116.30  -36.38  -11.13   18.18  137.55 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      -81.583936   9.804587  -8.321  2.6e-16 ***\nlatitude_rounded                  12.426692   0.827484  15.017  &lt; 2e-16 ***\ndepth_in_meters                    0.056945   0.024205   2.353 0.018821 *  \nlatitude_rounded:depth_in_meters  -0.007709   0.002110  -3.653 0.000272 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.38 on 1082 degrees of freedom\nMultiple R-squared:  0.2766,    Adjusted R-squared:  0.2746 \nF-statistic: 137.9 on 3 and 1082 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nDIAGNOSTICS AND INTERPRETATION:\nLooking at the p values on the far right of the summary chart, I can see that using latitude and depth together is significantly better than using depth alone to predict coral counts.\nThe p-value of the F-statistic tells me that it is below our threshold of .05, meaning that the relationship between our variables is statistically significant and we can REJECT the null hypothesis. So, there IS a relationship between coral counts, depth, and latitude!\nHowever, our adjusted R squared tells me that that only 27.46% of the variation of coral counts is explained by depth and latitude. Since this isn’t a very high percentage, I am lead to believe that there may be another variable in play that also effects coral counts.\nIf I were to take this test further, I would explore the possibility of omitted variable bias.\nIt is also important to consider how collecting practices might be influencing how and where corals are collected. I would also recommend further analysis of these variables, should I choose to continue this DEEP SEA CORALS analysis."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#citations",
    "href": "posts/2024-12-12-coral-interactions/index.html#citations",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "Citations",
    "text": "Citations\n[1]Konno, K., Gibbons, J., Lewis, R. and Pullin, A.S., 2024. Potential types of bias when estimating causal effects in environmental research and how to interpret them. Environmental Evidence, 13(1), p.1. (https://link.springer.com/article/10.1186/s13750-024-00324-7)\n[2]Roberts, S. and Hirshfield, M. (2004), Deep-sea corals: out of sight, but no longer out of mind. Frontiers in Ecology and the Environment, 2: 123-130. https://doi.org/10.1890/1540-9295(2004)002[0123:DCOOSB]2.0.CO;2\n[3]Smithsonian Institution, National Museum of Natural History. Observation date range: 1860 to 2022. Coral or sponge occurrence observations submitted to the NOAA National Database for Deep Sea Corals and Sponges (www.deepseacoraldata.noaa.gov). DSCRTP Dataset ID: NMNH_IZ. Database version: 20241022-1.\n[4] IMAGE CREDITS: Woods Hole Oceanographic Institution. “Deep-sea Corals.” Know Your Ocean. Accessed December 10, 2024. https://www.whoi.edu/know-your-ocean/ocean-topics/ocean-life/coral/deep-sea-corals/."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#github-repository",
    "href": "posts/2024-12-12-coral-interactions/index.html#github-repository",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "GITHUB REPOSITORY:",
    "text": "GITHUB REPOSITORY:\nhttps://github.com/jorb1/coral-interactions"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Bailey Jørgensen",
    "section": "",
    "text": "Experimenting with a DEEP SEA CORAL interactions model\n\n\nAnalyzing the relationship between deep sea corals, latitude, and ocean depth\n\n\n\nBAILEY JØRGENSEN\n\n\nDec 12, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn R Analysis of Landcover Classification using Decision Trees\n\n\nUsing multi-spectral imagery on the location of 4 land cover types, with a machine learning twist\n\n\n\nBailey Jørgensen\n\n\nNov 8, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire Analysis\n\n\nAQI and spatial analysis walkthrough of the 2017 natural disaster from the perspective of a Masters of Environmental Data Science student\n\n\n\nBailey Jørgensen\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiodiversity Intactness Index (BII) change in Phoenix, AZ\n\n\nA geospatial analysis in Python\n\n\n\nBailey Jørgensen\n\n\nSep 29, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post Title\n\n\na short catchy description of the blog post\n\n\n\nBailey Jørgensen\n\n\nSep 18, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]