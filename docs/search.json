[
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis_JORGENSEN.html",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis_JORGENSEN.html",
    "title": "Data Analysis of the Thomas Fire in Ventura and Santa Barbara Counties, 2017",
    "section": "",
    "text": "Repository Link: https://github.com/jorb1/eds220-hwk4\n\n\n\nImage of Fire"
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis_JORGENSEN.html#background",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis_JORGENSEN.html#background",
    "title": "Data Analysis of the Thomas Fire in Ventura and Santa Barbara Counties, 2017",
    "section": "Background:",
    "text": "Background:\nThe Thomas Fire, which burned over 280,000 acres in Ventura and Santa Barbara counties in December 2017, was one of California’s largest wildfires at the time. It caused widespread ecological damage, displaced communities, and left lasting environmental impacts.\nIn this analysis, I will find the perimeter of the fire, analyze spatial data of the lasting fire scars through false color images, and visualize the effects that this fire had on air quality in the Santa Barbara area.\nAbout the data\nIn this task I will use historical open-access data about fire perimeters in California. There are several datasets with this information online. The dataset that I found is from data.gov at this link: https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436. It was a particularly useful site, as there were multiple filetypes to choose from.\nThe next dataset I will use is a simplified collection of bands (red, green, blue, near-infrared and shortwave infrared) from the Landsat Collection 2 Level-2 atmosperically corrected surface reflectance data, collected by the Landsat 8 satellite. These data were retrieved from the Microsof Planetary Computer data catalogue and pre-processed to remove data outside land and coarsen the spatial resolution. (This data should be used for visualization and educational purposes only.)\nFinally, I will use Air Quality Index (AQI) data from the US Environmental Protection Agency to visualize the impact on the AQI of the 2017 Thomas Fire in Santa Barbara County.\n\nFirst up in my analysis:\nFire perimeter data retrieval and selection\nI will find and isolate the perimeter of the Thomas Fire, using open source data. I will then be able to use the Thomas Fire perimeter data in further analysis of the effects of the fire on Santa Barbara ecology. I will save this perimeter data as a file in my repository, for independent use in other notebooks.\nTo begin, I will do some exploratory data analysis to get a sense of the dataset I am using. I will ensure that I know the CRS of the data, for use in further spatial data joining and analysis.\n\n# Load libraries\n\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport geopandas as gpd\nimport rioxarray as rioxr\n\n# Read in data \n\nfp_perimeter = os.path.join('data', 'California_Fire_Perimeters_(all).shp')\nperimeter = gpd.read_file(fp_perimeter)\n\n\nperimeter.head(3)\n\n\n\n\n\n\n\n\nYEAR_\nSTATE\nAGENCY\nUNIT_ID\nFIRE_NAME\nINC_NUM\nALARM_DATE\nCONT_DATE\nCAUSE\nC_METHOD\nOBJECTIVE\nGIS_ACRES\nCOMMENTS\nCOMPLEX_NA\nIRWINID\nFIRE_NUM\nCOMPLEX_ID\nDECADES\ngeometry\n\n\n\n\n0\n2023\nCA\nCDF\nSKU\nWHITWORTH\n00004808\n2023-06-17\n2023-06-17\n5\n1\n1\n5.72913\nNone\nNone\n{7985848C-0AC2-4BA4-8F0E-29F778652E61}\nNone\nNone\n2020\nPOLYGON ((-13682443.000 5091132.739, -13682445...\n\n\n1\n2023\nCA\nLRA\nBTU\nKAISER\n00010225\n2023-06-02\n2023-06-02\n5\n1\n1\n13.60240\nNone\nNone\n{43EBCC88-B3AC-48EB-8EF5-417FE0939CCF}\nNone\nNone\n2020\nPOLYGON ((-13576727.142 4841226.161, -13576726...\n\n\n2\n2023\nCA\nCDF\nAEU\nJACKSON\n00017640\n2023-07-01\n2023-07-02\n2\n1\n1\n27.81450\nNone\nNone\n{B64E1355-BF1D-441A-95D0-BC1FBB93483B}\nNone\nNone\n2020\nPOLYGON ((-13459243.000 4621236.000, -13458968...\n\n\n\n\n\n\n\n\n# Figure out the dimensions of the dataframe\nprint(\"Shape of the data:\", perimeter.shape)\n\n# Figure out if the columns are the expected datatypes\nprint(\"Data types:\", perimeter.dtypes)\n\nShape of the data: (22261, 19)\nData types: YEAR_            int64\nSTATE           object\nAGENCY          object\nUNIT_ID         object\nFIRE_NAME       object\nINC_NUM         object\nALARM_DATE      object\nCONT_DATE       object\nCAUSE            int64\nC_METHOD         int64\nOBJECTIVE        int64\nGIS_ACRES      float64\nCOMMENTS        object\nCOMPLEX_NA      object\nIRWINID         object\nFIRE_NUM        object\nCOMPLEX_ID      object\nDECADES          int64\ngeometry      geometry\ndtype: object\n\n\n\n# Explore data CRS\nperimeter.crs\n\n&lt;Projected CRS: EPSG:3857&gt;\nName: WGS 84 / Pseudo-Mercator\nAxis Info [cartesian]:\n- X[east]: Easting (metre)\n- Y[north]: Northing (metre)\nArea of Use:\n- name: World between 85.06°S and 85.06°N.\n- bounds: (-180.0, -85.06, 180.0, 85.06)\nCoordinate Operation:\n- name: Popular Visualisation Pseudo-Mercator\n- method: Popular Visualisation Pseudo Mercator\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n\n# Find out if the CRS is projected\nperimeter.crs.is_projected\n\nTrue\n\n\nFrom this data exploration, I learned that the dataset is much larger than I need, but does contain useful information in addition to the geometries, such as acres burned, cause, etc. I learned that the names of the fires are in all capitals, and that the year numbers are int64, so I can treat them as numeric values. Finally, I leaned that the CRS is WGS 84, and that is is projected data, rather than geographic.\nFrom this fire perimeter data, I will select the Thomas Fire boundary. The fire occurred in 2017.\n\n# Make the column names lower case\nperimeter.columns = perimeter.columns.str.lower()\n\n# Filter data to only include the Thomas Fire boudnary in 2017\nthomas = perimeter[(perimeter['fire_name'] == \"THOMAS\") & (perimeter['year_'] == 2017)]\n\nthomas\n\n\n\n\n\n\n\n\nyear_\nstate\nagency\nunit_id\nfire_name\ninc_num\nalarm_date\ncont_date\ncause\nc_method\nobjective\ngis_acres\ncomments\ncomplex_na\nirwinid\nfire_num\ncomplex_id\ndecades\ngeometry\n\n\n\n\n2654\n2017\nCA\nUSF\nVNC\nTHOMAS\n00003583\n2017-12-04\n2018-01-12\n9\n7\n1\n281791.0\nCONT_DATE based on Inciweb\nNone\nNone\nNone\nNone\n2010\nMULTIPOLYGON (((-13316089.016 4088553.040, -13...\n\n\n\n\n\n\n\nNow I will save only the 2017 Thomas Fire boundary as a GeoJSON file. The file should go into the data/ directory in my repository.\n\n# Save the fire boundary as a file that can go into my repository\n# Save the filtered GeoDataFrame as a GeoJSON file\npath = 'data/thomas.geojson'\nthomas.to_file(path, driver='GeoJSON')\n\nI chose to use a GeoJSON file format for my perimeter boundary because it is a common and useful “open format for encoding vector points and their attributes”. It comes in one file, as compared to .shp files, which have many dependencies. It requires the data be in WGS84, and since I already verified that this data is in that CRS, it seems like the best possible option for this analysis."
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis_JORGENSEN.html#next-up",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis_JORGENSEN.html#next-up",
    "title": "Data Analysis of the Thomas Fire in Ventura and Santa Barbara Counties, 2017",
    "section": "Next up:",
    "text": "Next up:\n\nTrue Color Image\nAs I import the raster file to make this true color image, I add the parameter decode_coords=\"all\" to the code. This parameter controls how the coordinate metadata in the NetCDF file are processed. Specifically, the all distinction decodes all coordinates in the file to easily useable xarray coordinate variables.\n\n# Construct a file path to the Landsat data using os and import it\nfp = os.path.join('data', 'landsat8-2018-01-26-sb-simplified.nc')\nsb_rast = rioxr.open_rasterio(fp, decode_coords=\"all\")\nsb_rast\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 25MB\nDimensions:      (band: 1, x: 870, y: 731)\nCoordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0\nData variables:\n    red          (band, y, x) float64 5MB ...\n    green        (band, y, x) float64 5MB ...\n    blue         (band, y, x) float64 5MB ...\n    nir08        (band, y, x) float64 5MB ...\n    swir22       (band, y, x) float64 5MB ...xarray.DatasetDimensions:band: 1x: 870y: 731Coordinates: (4)band(band)int641array([1])x(x)float641.213e+05 1.216e+05 ... 3.559e+05axis :Xcrs :EPSG:32611long_name :x coordinate of projectionresolution :30standard_name :projection_x_coordinateunits :metre_FillValue :nanarray([121305., 121575., 121845., ..., 355395., 355665., 355935.])y(y)float643.952e+06 3.952e+06 ... 3.755e+06axis :Ycrs :EPSG:32611long_name :y coordinate of projectionresolution :-30standard_name :projection_y_coordinateunits :metre_FillValue :nanarray([3952395., 3952125., 3951855., ..., 3755835., 3755565., 3755295.])spatial_ref()int640crs_wkt :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984projected_crs_name :WGS 84 / UTM zone 11Ngrid_mapping_name :transverse_mercatorlatitude_of_projection_origin :0.0longitude_of_central_meridian :-117.0false_easting :500000.0false_northing :0.0scale_factor_at_central_meridian :0.9996spatial_ref :PROJCS[\"WGS 84 / UTM zone 11N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-117],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32611\"]]GeoTransform :121170.0 270.0 0.0 3952530.0 0.0 -270.0array(0)Data variables: (5)red(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]green(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]blue(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]nir08(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]swir22(band, y, x)float64...add_offset :0.0coordinates :timescale_factor :1.0_FillValue :0.0[635970 values with dtype=float64]Indexes: (3)bandPandasIndexPandasIndex(Index([1], dtype='int64', name='band'))xPandasIndexPandasIndex(Index([121305.0, 121575.0, 121845.0, 122115.0, 122385.0, 122655.0, 122925.0,\n       123195.0, 123465.0, 123735.0,\n       ...\n       353505.0, 353775.0, 354045.0, 354315.0, 354585.0, 354855.0, 355125.0,\n       355395.0, 355665.0, 355935.0],\n      dtype='float64', name='x', length=870))yPandasIndexPandasIndex(Index([3952395.0, 3952125.0, 3951855.0, 3951585.0, 3951315.0, 3951045.0,\n       3950775.0, 3950505.0, 3950235.0, 3949965.0,\n       ...\n       3757725.0, 3757455.0, 3757185.0, 3756915.0, 3756645.0, 3756375.0,\n       3756105.0, 3755835.0, 3755565.0, 3755295.0],\n      dtype='float64', name='y', length=731))Attributes: (0)\n\n\n\n# Explore the data\nprint('Shape: ', dict(sb_rast.sizes))\nprint(sb_rast.data_vars, '\\n')\n\nShape:  {'band': 1, 'x': 870, 'y': 731}\nData variables:\n    red      (band, y, x) float64 5MB ...\n    green    (band, y, x) float64 5MB ...\n    blue     (band, y, x) float64 5MB ...\n    nir08    (band, y, x) float64 5MB ...\n    swir22   (band, y, x) float64 5MB ... \n\n\n\nThis data exploration shows me that there is only one band on this raster. This means that including it is redundant, and the band will need to be removed. I also learned that each color band is a float64 integer, which is good to know when doing analysis.\n\n# Drop the band dimension of the data\n# Original dimensions and coordinates\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\n# Remove length 1 dimension (band)\nsb_rast = sb_rast.squeeze()\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\n# Drop the coordinates associated to band\nsb_rast = sb_rast.drop_vars('band')\nprint(sb_rast.dims, sb_rast.coords, '\\n')\n\nFrozenMappingWarningOnValuesAccess({'band': 1, 'x': 870, 'y': 731}) Coordinates:\n  * band         (band) int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\nFrozenMappingWarningOnValuesAccess({'x': 870, 'y': 731}) Coordinates:\n    band         int64 8B 1\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\nFrozenMappingWarningOnValuesAccess({'x': 870, 'y': 731}) Coordinates:\n  * x            (x) float64 7kB 1.213e+05 1.216e+05 ... 3.557e+05 3.559e+05\n  * y            (y) float64 6kB 3.952e+06 3.952e+06 ... 3.756e+06 3.755e+06\n    spatial_ref  int64 8B 0 \n\n\n\nTo get a good look at this raster data, without creating any new variables:\nI will select the red, green, and blue variables (in that order) of the xarray.Dataset holding the Landsat data, convert it to an xarray.DataArray using the to_array() method, and then use .plot.imshow() to create an RGB image with the data. There will be a warning, that’s ok. I will adjust the scale used for plotting the bands to get a true color image.\nThe first plot will have the parameter set to be robust=False.\n\n# Select red, green, and blue variables, stack them, and plot as an RGB image\nsb_rast[['red', 'green', 'blue']].to_array().plot.imshow(robust=False)\n\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n\n\n\n\n\n\n\n\n\nAs we can see, this doesn’t turn out quite as we’d hoped. That’s because, by setting that parameter to False, we are not accounting for cloud cover. Their RGB values are outliers and cause the other values to be squished when plotting.\nTo account for this, I will use the robust = True parameter at the end of my code, in order to deal with the clouds:\n\n# Select red, green, and blue variables, stack them, and plot as an RGB image\nsb_rast[['red', 'green', 'blue']].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nThis true color image gives us a visual that mostly resembles what we would expect to see with our human eyes looking down on Santa Barbara. The colors are what we would expect to see, and this can be useful for identifying landmarks. However, sometimes it is important to get a new perspective. For example, using this map, it is almost impossible to see the area in which the Thomas Fire burned. And that’s when we bring in…\n\n\n4. False color image\nTo continue my analysis, and without creating any new variables, I will create a false color image by plotting the short-wave infrared (swir22), near-infrared, and red variables (in that order).\n\n# Select the swir22, near-infrared, and red variables, stack them, and plot as a false color image\nsb_rast[['swir22', 'nir08', 'red']].to_array().plot.imshow(robust=True)\n\n\n\n\n\n\n\n\nFalse color imagery, created using satellite data from instruments like Landsat, is a useful tool for monitoring wildfire impacts. By assigning infrared bands to visible colors, these images highlight vegetation health, burn severity, and the extent of fire scars. This approach helps researchers and land managers assess recovery efforts, identify high-risk areas, and plan restoration strategies.\n\n\n5. Map\nFinally, I will create a map showing the shortwave infrared/near-infrared/red false color image together with the Thomas Fire perimeter.\n\n# Read in the Thomas fire perimeter we created\nfp2 = os.path.join('data', 'thomas.geojson')\nthomas_perim = gpd.read_file(fp2)\nthomas_perim.plot()\n\n\n\n\n\n\n\n\nThis initial plot shows us that our perimeter file is looking good. It also shows us the perimeter that we will want to clip our raster file to, in order to create our map analyzing the burn area of the Thomas Fire.\nFirst, we will have to ensure that our CRS’s match for these datasets…\n\n# Examine CRss\nprint('Santa Barbara Raster CRS: ', sb_rast.rio.crs)\nprint('Thomas Fire Perimeter CRS: ', thomas_perim.crs)\n\nSanta Barbara Raster CRS:  EPSG:32611\nThomas Fire Perimeter CRS:  EPSG:3857\n\n\n\n# Reproject CRS of the Santa Barbara Raster\nsb_rast = sb_rast.rio.reproject(\"EPSG:3857\")\nprint('Matched CRS?', sb_rast.rio.crs == thomas_perim.crs)\n\nMatched CRS? True\n\n\n\n# Clip the sb_rast map to match the dimensions of the Thomas Fire Perimeter\nsb_fire = sb_rast.rio.clip_box(*thomas_perim.total_bounds)\n\n\n# Map our false color image with the fire boundary overlaid\nfig, ax = plt.subplots(figsize=(10, 10))\nsb_fire[['swir22', 'nir08', 'red']].to_array().plot.imshow(ax=ax, robust=True)\nthomas_perim.boundary.plot(ax=ax, edgecolor=\"firebrick\", linewidth = 2, label=\"Thomas Fire Perimeter\")\nax.set_title(\"Thomas Fire: Burn Scars in False Color Imagery\", fontsize=16)\nax.legend(loc='upper right', fontsize=12)\n\nplt.show()\n\n\n\n\n\n\n\n\nOur true color image was insufficient to reveal plainly the path of the Thomas Fire. However, by simply utilizing a false color composit, the path of the fire and the effect it had on the landscape are much more clear.\nThis final map clearly shows the burn scars from the fire, contained within the perimeter boundary we utilized to help us further identify the exact area effected."
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis_JORGENSEN.html#conclusion",
    "href": "posts/2024-12-4-thomas-fire-post/thomas_fire_analysis_JORGENSEN.html#conclusion",
    "title": "Data Analysis of the Thomas Fire in Ventura and Santa Barbara Counties, 2017",
    "section": "Conclusion",
    "text": "Conclusion\nUsing Python in Jupyter Notebooks, I have successfully executed a series of analysis analyzing a fire that had clear and lasting effects on the regions of Santa Barbara and Ventura. This analysis also showcases the variety of data types that Python can handle, from spatial data to data visualization.\n\nCitations:\nAirNow. “Air Quality Index (AQI) Basics.” Accessed December 4, 2024. https://www.airnow.gov/aqi/aqi-basics/.\nC. Galaz García, EDS 220 - Working with Environmental Datasets, Course Notes. 2024. [Online]. Available: https://meds-eds-220.github.io/MEDS-eds-220-course/book/preface.html\nCAL Fire. “California Fire Perimeters (All).” Data.gov. Metadata created March 30, 2024, updated May 14, 2024. https://catalog.data.gov/dataset/california-fire-perimeters-all-b3436.\nHamm, Keith. “Closing Schools and Moving Finals Due to Thomas Fire: A Look at Our Education System’s Response to the Wildfire.” Santa Barbara Independent, December 13, 2017. https://www.independent.com/2017/12/13/closing-schools-and-moving-finals-due-thomas-fire/.\nMicrosoft Planetary Computer. Landsat Collection 2 Level-2 Atmospherically Corrected Surface Reflectance Data from Landsat 8 [Dataset]. Simplified for visualization and educational purposes. Accessed November 20, 2024. https://planetarycomputer.microsoft.com.\nU.S. Environmental Protection Agency. “Air Data: Air Quality Data Collected at Outdoor Monitors Across the US.” Accessed December 4, 2024. https://www.epa.gov/outdoor-air-quality-data."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-context",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-context",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "THE CONTEXT:",
    "text": "THE CONTEXT:\nIt is estimated that two-thirds of all known coral species can be classified as “deep-sea corals”. These corals exhibit a diversity similar to those of their shallow-water relatives, but they do not form symbiotic relationships with algae, do not obtain energy from sunlight (instead feeding from microorganisms in the water), and are much more resilient to cold temperatures.\nAlso similar to their shallow water counterparts, deep sea corals are hosts to many other forms of biodiversity. They are also extremely slow growing, and as a result, slow to recover from ecological or anthropogenic disturbance[2].\nThis analysis will seek to more deeply understand the environments in which deep sea corals are more likely to appear."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-data",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-data",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "THE DATA:",
    "text": "THE DATA:\nThe data used for this analysis comes from NOAA’s Deep Sea Coral Research and Technology Program (DSCRTP), and is housed at the Smithsonian National Museum of Natural History, Invertebrate Zoology Collection. It contains decades worth of data regarding deep sea corals, their taxa, spatial data, and collecting data. It is an open source data set, that is consistently being updated. (See link in the citations section, to take a look for yourself!)[3]\nHere are some details from the metadata of the dataset:\n\nNumber of records: 30,850\nNumber of coral records: 24,768\nNumber of sponge records: 6,082\nRecords with images: 245\n\nSeems like we have a lot of data points to work with here. Let’s begin by loading our libraries and reading in the data!\nThe deep sea corals dataset I chose also includes deep sea sponges. Since this analysis wants to focus on corals, I’ll have to filter that data out. In addition, there are several columns on the dataframe that I know won’t be helpful for my analysis (for example, collector data, institution data, or columns that contains a lot of NAs.) In addition to filtering out the sponge data, I will filter out those unwanted columns as well.\n\n\nCode\n# Filter data to only include columns I want\ncorals &lt;- corals %&gt;% \n  clean_names(case = \"snake\") %&gt;% # Change column names to snake_case\n  select(scientific_name, phylum, genus, species, individual_count, latitude, longitude, depth_in_meters, country) %&gt;% # Choose columns I want\n  filter(phylum %in% c(\"Cnidaria\")) # Filter to exclude sponges and NA row\n\n\nI know that in order to properly visualize my data, I’m going to want to make some maps. However, I also want the latitude and longitude data to be easily handled during any linear model calculations, so I decide to make a separate dataset variable, that transforms the lat long data into geometric data that I can easily add to tmaps for visualizations.\n\n\nCode\ncorals_transformed &lt;- corals %&gt;% \n  sf::st_as_sf(coords = c(\"longitude\", \"latitude\"),\n               crs = st_crs(4326)) %&gt;% \n              filter(st_is_valid(.))\n\n\nLet’s take a look at the result of our well filtered data! This data cleaning will enable us to run effective and easy to understand data analysis in the next few steps.\n\n\nCode\n# Create a table showing the head of our filtered dataframe\nkable(head(corals), format = \"html\", caption = \"Preview of Filtered Global Coral Data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\nPreview of Filtered Global Coral Data\n\n\nscientific_name\nphylum\ngenus\nspecies\nindividual_count\nlatitude\nlongitude\ndepth_in_meters\ncountry\n\n\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n2\n34.95839\n-75.32464\n146\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n6\n34.95839\n-75.32464\n146\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n6\n29.28357\n-88.26665\n84\nUSA\n\n\nPourtalosmilia conferta\nCnidaria\nPourtalosmilia\nconferta\n1\n24.48375\n-80.88314\n191\nUSA\n\n\nDesmophyllum pertusum\nCnidaria\nDesmophyllum\npertusum\n1\n30.96684\n-79.69976\n396\nUSA\n\n\nDesmophyllum pertusum\nCnidaria\nDesmophyllum\npertusum\n1\n27.98361\n-79.3331\n577\nBahamas"
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-exploration",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-exploration",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "THE EXPLORATION:",
    "text": "THE EXPLORATION:\n\nVISUALIZATION\nI want to take a look at the whole scale of my data set. To do this, I decide to make a global map.\n\n\nCode\nworld &lt;- spData::world\n\ntm_shape(world) +\n  tm_fill(col = \"continent\",\n          palette = c(\"slategray\", \"snow3\", \"slategray2\", \"slategray3\", \"slategray4\", \"lightslategrey\", \"lightsteelblue4\", \"snow4\"),\n          title = \"Continents\") +\n  tm_shape(corals_transformed) +\n  tm_dots(col = \"phylum\",\n          palette = \"pink2\",\n          size = 0.05,\n          border.col = \"black\",\n          title = \"Data Point\") +\n  # tm_compass(type = \"4star\",\n  #            size = .05,\n  #            position = c(\"left\", \"top\")) +\n  tm_layout(main.title = \"Global Coral Observations\",\n            title.position = c(\"center\", \"top\"),\n            title.snap.to.legend = FALSE,\n            #frame = TRUE, \n            #legend.frame = TRUE,\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\nFrom this global map, we can see the huge volume to coral observations present in our dataset. In order to get a little more focus in my model, I decide filter the datset to a specific country, and run my analysis on that.\nIt could be interesting to, in the future, run the same analysis on each continent, and then cross-compare them. This would reveal interesting insights about different continents and species. Politically, it could also be an interesting view into how collecting bias might affect the results of such an analysis, since we can see that some continents contain more sample points than others. Unfortunately, this could be due to the fact that certain countries and regions are more studied than others, due to data collecting bias in the environmental sector.[1]\nFor the purpose of this study, however, I will filter the coral data to the Philippines. The Philippines is an area known for its coral diversity and it’s popularity in the diving community. I am curious to see how DEEP SEA corals are represented in the region, rather than the easily dive-able shallow coral systems.\n\n\nCode\nphil_coral &lt;- corals %&gt;% \n  filter(country %in% \"Philippines\") %&gt;% \n  mutate(\n    latitude = as.numeric(latitude),\n    longitude = as.numeric(longitude),\n    depth_in_meters = as.numeric(depth_in_meters),\n    latitude_rounded = round(latitude, 1)\n  ) %&gt;% \n  add_count(latitude_rounded, name = \"obs_count\")\n\n# Transformed dataset for mapping\nphil_coral_transformed &lt;- corals_transformed %&gt;% \n  filter(country %in% \"Philippines\")\n\n# Create a table showing the head of our filtered dataframe\nkable(head(phil_coral), format = \"html\", caption = \"Preview of Filtered Philippines Coral Data\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\"))\n\n\n\nPreview of Filtered Philippines Coral Data\n\n\nscientific_name\nphylum\ngenus\nspecies\nindividual_count\nlatitude\nlongitude\ndepth_in_meters\ncountry\nlatitude_rounded\nobs_count\n\n\n\n\nCoralliidae\nCnidaria\nNA\nNA\n1\n15.9708\n119.672\n1719\nPhilippines\n16.0\n3\n\n\nStenohelia tiliata\nCnidaria\nStenohelia\ntiliata\n1\n6.1333\n121.317\n275\nPhilippines\n6.1\n25\n\n\nStylaster multiplex\nCnidaria\nStylaster\nmultiplex\n1\n5.1867\n119.590\n450\nPhilippines\n5.2\n16\n\n\nDistichopora irregularis\nCnidaria\nDistichopora\nirregularis\n1\n7.0853\n125.662\n42\nPhilippines\n7.1\n8\n\n\nDistichopora irregularis\nCnidaria\nDistichopora\nirregularis\n4\n7.0950\n125.662\n38\nPhilippines\n7.1\n8\n\n\nDistichopora\nCnidaria\nDistichopora\nNA\n1\n7.0867\n125.660\n37\nPhilippines\n7.1\n8\n\n\n\n\n\n\n\n\n\nCode\n# Get data for base map of Philippines\nph &lt;- ne_countries(scale = 10, country = \"Philippines\", returnclass = \"sf\")\n\n# Make a plot of corals in the Philippines\ntm_shape(ph) +\n  tm_polygons(col = \"name\",\n          palette = c(\"slategray\", \"snow3\", \"slategray2\", \"slategray3\", \"slategray4\", \"lightslategrey\", \"lightsteelblue4\", \"snow4\"),\n          title = \"Philippines\") +\n  tm_shape(phil_coral_transformed) +\n  tm_dots(col = \"phylum\",\n          palette = \"pink2\",\n          size = 0.1,\n          border.col = \"black\",\n          title = \"Data Point\") +\n   tm_compass(type = \"8star\",\n              size = 2,\n              position = c(\"left\", \"top\")) +\n  tm_layout(main.title = \"Coral Observations \\nin the Philippines\",\n            title.position = c(\"center\", \"top\"),\n            title.snap.to.legend = FALSE,\n            legend.outside = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nTHE SUMMARIZATION:\nAs we can see from the map above, there have been deep sea coral collections happening all throughout the country of the Philippines. This gives me a lot of data to run analysis on. I decided to make some additional visualizations to summarize the contents of the data, and guide me toward making a null and alternate hypothesis.\nFirst, I want to take a look at the distribution of coral observations along the different latitudes of the Philippines. In the data cleaning I performed above, I decided to create another column on the Philippines coral dataframe that rounded the latitudes to just two decimal places. This will allow me to work with latitiude data more easily, without having too many latitude points obscuring the visualizations of my data. I also added a column that summed the total number of coral observations at each of these rounded latitudes. This number will allow me to analyze the abundance of corals at each rounded latitude, enabling analysis of coral distributions.\nIt is using the rounded latitudes column(latitude_rounded), and the the observations per latitude column(obs_count) that I made the following data visualizations.\nFirst up, I wanted to take an initial look at these two columns:\n\n\nCode\n# Make a plot of coral obs and latitude\nggplot() +\n geom_line(data = phil_coral,\n            aes(x = latitude_rounded,\n                y = obs_count),\n           color = \"pink2\",\n           size = 1) +\n  labs(x = \"Latitude\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Deep Sea Corals at Latitude\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows the general distribution of the corals along the different latitudes, and visualizes a significant spike in abundance around the 14 degrees latitude line.\nI wanted a way to visualize depth fluctuations along these latitude lines, in order to help visualize general depth patters in the area of interest. I decided to do another line graph, this one working almost like a cross section of depth along latitude, though of course only including depths available on our dataframe, so not actually having spatial representation, just data representation:\n\n\nCode\n# Make a plot\nggplot() +\n geom_line(data = phil_coral,\n            aes(x = latitude_rounded,\n                y = depth_in_meters),\n           color = \"slategray\",\n           size = 1) +\n  labs(x = \"Latitude\",\n       y = \"Depth in Meters\",\n       title = \"Depth Fluctuations in the Philippines\") +\n  scale_y_reverse() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis visualization showed some interesting spikes in depths collected from, with another large spike around the 14 degree mark, but several other degrees showing some spikes as well. This helps visualize the depths along latitudes that our corals dwell on, but I want to find a better way of visualizing how these depths might effect coral abundance.\nI decide to create scatterplots that incorporates all three variables, and see how these different visualizations can help me form my hypothesis.\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = latitude_rounded,\n                 y = depth_in_meters,\n                 color = obs_count)) +\n  scale_color_gradient(low = \"black\", high = \"pink2\") +\n  labs(x = \"Latitude\",\n       y = \"Depth in Meters\",\n       title = \"Colored by # of Observations\") +\n    geom_smooth(data = phil_coral,\n              aes(x = latitude_rounded, y = depth_in_meters), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot, with a geom_smooth line with method “lm”, or linear model, shows us that as latitude increases, the depths that corals are found in decreases slightly. This visualization makes it difficult to interpret the number of corals observed, however, though it does show how there are more observations made at the 14 degree latitude.\nI decide to try rearranging what variables go on what axes:\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = latitude_rounded,\n                 y = obs_count,\n                 color = depth_in_meters)) +\n  scale_color_gradient(low = \"skyblue\", high = \"black\") +\n  labs(x = \"Latitude\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Depth, Latitude, and Observation Count\") +\n    geom_smooth(data = phil_coral,\n              aes(x = latitude_rounded, y = obs_count), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nThis plot shows me that as latitude increaases, the number of corals observed also increases. There is an outlier high up around latitude 14 degrees, and I suspect that that outlier is increasing the slope of our lm line by quite a bit. This plot makes it easy to observe the number of observations made, compared to the first plot.\n\n\nCode\nggplot() +\n  geom_point(data = phil_coral,\n             aes(x = depth_in_meters,\n                 y = obs_count,\n                 color = latitude)) +\n  scale_color_gradient(low = \"skyblue\", high = \"black\") +\n  labs(x = \"Depth in Meters\",\n       y = \"Number of Deep Sea Corals Observed\",\n       title = \"Depth, Latitude, and Observation Count\") +\n  geom_smooth(data = phil_coral,\n              aes(x = depth_in_meters, y = obs_count), \n              method = \"lm\", se = FALSE, linewidth = 1.5, color = \"seagreen\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nFinally, this plot shows us that as depth increases, the number of deep sea corals observed also decreases, with the colors of the points showing us the distribution of the points along latitude, with more observations occurring at higher latitudes.\nAll of these plots seem to show that there is some sort of relationship between all of these variables. Let’s see if we can quantify that relationship!\n\n\nTHE HYPOTHESIS:\nNull: There is no relationship between number of deep sea corals observed, latitude, and depth in the Philippines.\nAlternate: There IS a relationship between number of deep sea corals observed, latitude, and depth in the Philippines."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#the-analysis",
    "href": "posts/2024-12-12-coral-interactions/index.html#the-analysis",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "THE ANALYSIS:",
    "text": "THE ANALYSIS:\n\nMETHOD:\nSince I have multiple variables to contend with, I decide on using an INTERACTIONS MODEL to understand the relationship between my variables (corals observed, latitude, and depth). My interactions model will take on this format:\n\\[\n\\text{number of deep sea corals observed} \\sim \\text{latitude} + \\text{depth} + \\text{latitude} \\times \\text{depth}\n\\]\nOr, more technically:\n\\[\n\\text{coral observed} = \\beta_0 + \\beta_1 \\cdot \\text{latitude} + \\beta_2 \\cdot \\text{depth} + \\beta_3 \\cdot (\\text{latitude} \\times \\text{depth}) + \\epsilon\n\\]\n\n\nIMPLEMENTATION:\nIn R code, I will create this linear model and then run the summary:\n\n\nCode\n# Create an interactions model\ninteractions &lt;- lm(obs_count ~ latitude_rounded + depth_in_meters + latitude_rounded:depth_in_meters,\n                   data = phil_coral)\nsummary(interactions)\n\n\n\nCall:\nlm(formula = obs_count ~ latitude_rounded + depth_in_meters + \n    latitude_rounded:depth_in_meters, data = phil_coral)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-116.30  -36.38  -11.13   18.18  137.55 \n\nCoefficients:\n                                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      -81.583936   9.804587  -8.321  2.6e-16 ***\nlatitude_rounded                  12.426692   0.827484  15.017  &lt; 2e-16 ***\ndepth_in_meters                    0.056945   0.024205   2.353 0.018821 *  \nlatitude_rounded:depth_in_meters  -0.007709   0.002110  -3.653 0.000272 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 51.38 on 1082 degrees of freedom\nMultiple R-squared:  0.2766,    Adjusted R-squared:  0.2746 \nF-statistic: 137.9 on 3 and 1082 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nDIAGNOSTICS AND INTERPRETATION:\nLooking at the p values on the far right of the summary chart, I can see that using latitude and depth together is significantly better than using depth alone to predict coral counts.\nThe p-value of the F-statistic tells me that it is below our threshold of .05, meaning that the relationship between our variables is statistically significant and we can REJECT the null hypothesis. So, there IS a relationship between coral counts, depth, and latitude!\nHowever, our adjusted R squared tells me that that only 27.46% of the variation of coral counts is explained by depth and latitude. Since this isn’t a very high percentage, I am lead to believe that there may be another variable in play that also effects coral counts.\nIf I were to take this test further, I would explore the possibility of omitted variable bias.\nIt is also important to consider how collecting practices might be influencing how and where corals are collected. I would also recommend further analysis of these variables, should I choose to continue this DEEP SEA CORALS analysis."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#citations",
    "href": "posts/2024-12-12-coral-interactions/index.html#citations",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "Citations",
    "text": "Citations\n[1]Konno, K., Gibbons, J., Lewis, R. and Pullin, A.S., 2024. Potential types of bias when estimating causal effects in environmental research and how to interpret them. Environmental Evidence, 13(1), p.1. (https://link.springer.com/article/10.1186/s13750-024-00324-7)\n[2]Roberts, S. and Hirshfield, M. (2004), Deep-sea corals: out of sight, but no longer out of mind. Frontiers in Ecology and the Environment, 2: 123-130. https://doi.org/10.1890/1540-9295(2004)002[0123:DCOOSB]2.0.CO;2\n[3]Smithsonian Institution, National Museum of Natural History. Observation date range: 1860 to 2022. Coral or sponge occurrence observations submitted to the NOAA National Database for Deep Sea Corals and Sponges (www.deepseacoraldata.noaa.gov). DSCRTP Dataset ID: NMNH_IZ. Database version: 20241022-1.\n[4] IMAGE CREDITS: Woods Hole Oceanographic Institution. “Deep-sea Corals.” Know Your Ocean. Accessed December 10, 2024. https://www.whoi.edu/know-your-ocean/ocean-topics/ocean-life/coral/deep-sea-corals/."
  },
  {
    "objectID": "posts/2024-12-12-coral-interactions/index.html#github-repository",
    "href": "posts/2024-12-12-coral-interactions/index.html#github-repository",
    "title": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH",
    "section": "GITHUB REPOSITORY:",
    "text": "GITHUB REPOSITORY:\nhttps://github.com/jorb1/coral-interactions"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bailey Jørgensen",
    "section": "",
    "text": "Bailey Jørgensen grew up in Montana and got her B.S. in Archaeology at Montana State University. When not at Bren, she works as the Collections Manager at the Alf Museum of Paleontology in Claremont, California. At the museum, she cares for an active research collection and exhibits hall and coleads cohorts of students in paleontological fieldwork and data collection. Her fieldwork areas include backcountry quarries in Grand Staircase Escalante National Monument, sites in the Mojave Desert, Manti-la-Sal National Forest, Montana’s Gravelly Range, Bighorn Basin in Wyoming, and Bayanzag in Mongolia.\nBefore finding the Alf Museum, she worked sailing and preserving historic wooden ships for maritime museums and institutes. She helped run programs such as the Topsail STEM Adventure Program and the Beyond the Bell Oceanography Middle School Program at the Los Angeles Maritime Institute.\nHer experiences in data management have allowed her to participate in active research and data management. She joined the MEDS program at the Bren School to fully maximize these opportunities, and to deepen the technical knowledge required for more advanced projects. She hopes to continue work in the biodiversity heritage sector, especially for marine environments."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "What I do for work\nsome test goes here"
  },
  {
    "objectID": "about.html#what-i-do-for-fun",
    "href": "about.html#what-i-do-for-fun",
    "title": "About",
    "section": "What I do for fun",
    "text": "What I do for fun\n\ngive the people what they want\n\nhere is some more text\n\nhere is a line\nhere is another line\n\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "DELETE-LATER/practice.html",
    "href": "DELETE-LATER/practice.html",
    "title": "Here is my level one header",
    "section": "",
    "text": "Here is my level one header\nHere is my first paragraph\nHere is my second paragraph, where you can read more about MEDS.\nThis is very important text!"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html",
    "href": "posts/2024-10-18-my-first-post/index.html",
    "title": "Blog Post Title",
    "section": "",
    "text": "I’m going to insert a footnote here1"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#this-is-my-first-section",
    "href": "posts/2024-10-18-my-first-post/index.html#this-is-my-first-section",
    "title": "Blog Post Title",
    "section": "",
    "text": "I’m going to insert a footnote here1"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#this-is-my-second",
    "href": "posts/2024-10-18-my-first-post/index.html#this-is-my-second",
    "title": "Blog Post Title",
    "section": "This is my second",
    "text": "This is my second\nHere’s my next paragraph2\nhere is more random text. im going to cite a journal article now.(Gaynor et al. 2022)"
  },
  {
    "objectID": "posts/2024-10-18-my-first-post/index.html#footnotes",
    "href": "posts/2024-10-18-my-first-post/index.html#footnotes",
    "title": "Blog Post Title",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHere is a new footnote↩︎\nHere is my second footnote↩︎"
  },
  {
    "objectID": "posts/2024-12-4-thomas-fire-post/index.html",
    "href": "posts/2024-12-4-thomas-fire-post/index.html",
    "title": "Thomas Fire Analysis",
    "section": "",
    "text": "CitationBibTeX citation:@online{jørgensen2024,\n  author = {Jørgensen, Bailey},\n  title = {Thomas {Fire} {Analysis}},\n  date = {2024-10-18},\n  url = {https://jorb1.github.io/posts/2024-12-4-thomas-fire-post/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJørgensen, Bailey. 2024. “Thomas Fire Analysis.” October\n18, 2024. https://jorb1.github.io/posts/2024-12-4-thomas-fire-post/."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "USING AN INTERACTIONS MODEL TO ANALYZE THE RELATIONSHIP BETWEEN DEEP SEA CORALS, LATITUDE, AND OCEAN DEPTH\n\n\n\n\n\n\nBAILEY JØRGENSEN\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis of the Thomas Fire in Ventura and Santa Barbara Counties, 2017\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBlog Post Title\n\n\n\nEDS\n\n\nRobots\n\n\nBoats\n\n\nPaleontology\n\n\n\na short catchy description of the blog post\n\n\n\nBailey Jørgensen\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThomas Fire Analysis\n\n\n\nEDS\n\n\nRobots\n\n\nBoats\n\n\nPaleontology\n\n\n\nAQI and spatial analysis walkthrough\n\n\n\nBailey Jørgensen\n\n\nOct 18, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  }
]