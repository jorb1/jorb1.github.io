{
  "hash": "a1e35eb43d4c36c0dae1996dc4db68a1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"An R Analysis of Landcover Classification using Decision Trees\"\ndescription: \"Using multi-spectral imagery on the location of 4 land cover types, with a machine learning twist\"\nauthor: \"Bailey Jørgensen\"\ndate: 2024-11-8\ncategories: [EDS, Geospatial, Machine Learning]\nimage: tree.jpg\nexecute:\n  warning: false\n  message: false\nformat:\n  html:\n    code-fold: true\n    toc: true\neditor_options: \n  chunk_output_type: console\ncitation:\n  url: https://jorb1.github.io/posts/2024-11-8-landcover-classification/\n---\n\n\n\n\n## Background\n\nMonitoring the distribution and change in land cover types can help us understand the impacts of phenomena like climate change, natural disasters, deforestation, and urbanization. Determining land cover types over large areas is a major application of remote sensing because we are able to distinguish different materials based on their spectral reflectance.\n\nClassifying remotely sensed imagery into land cover classes enables us to understand the distribution and change in land cover types over large areas.\n\nThere are many approaches for performing land cover classification:\n\n- **Supervised** approaches use training data labeled by the user\n- **Unsupervised** approaches use algorithms to create groups which are identified by the user afterward\n\n## Game Plan\n\nIn this exercise, I am using a form of supervised classification – a decision tree classifier.\n\n*Decision trees* classify pixels using a series of conditions based on values in spectral bands. These conditions (or decisions) are developed based on training data.\n\nI will create a land cover classification for southern Santa Barbara County based on multi-spectral imagery and data on the location of 4 land cover types:\n\n- green vegetation\n- dry grass or soil\n- urban\n- water\n\nTo do so, I will need to:\n\n- Load and process Landsat scene\n- Crop and mask Landsat data to study area\n- Extract spectral data at training sites\n- Train and apply decision tree classifier\n- Plot results\n\n## Data Details\n\nTo conduct this analysis, I will use the Landsat 5 Thematic Mapper data. More information about these data can be found at this link: https://www.usgs.gov/landsat-missions/landsat-5. \n\nSpecifically, I will be using 1 scene from September 25, 2007 (my birthday!), on bands 1,2, 3, 4, 5, 7, with collection 2 surface reflectance product. \n\n*Data files:*\n\n- landsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B1.tif\n- landsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B2.tif\n- landsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B3.tif\n- landsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B4.tif\n- landsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B5.tif\n- landsat-data/LT05_L2SP_042036_20070925_20200829_02_T1_SR_B7.tif\n\n**Study area:**\n\nI will be using a polygon representing southern Santa Barbara county, the county in which I am currently attending school. \n\n*Data file:*\n\n- SB_county_south.shp\n\n**Training data:**\n\nAnd finally, I will be using a data file with polygons representing sites with training data. Specifically, I will be using the data character string with land cover type.\n\n*Data file:* \n\n- trainingdata.shp\n\nAll of the data used in this study were accessed on November 25th, 2024.\n\n## Workflow\n\n### 1. Set up\n\nTo train our classification algorithm and plot the results, I will use the rpart and rpart.plot packages.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# install.packages(\"rpart\")\n# install.packages(\"rpart.plot\")\n```\n:::\n\n\n\n\nLet’s load all necessary packages:\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n### 2. Load Landsat data\n\nLet’s create a raster stack. Each file name ends with the band number (e.g. B1.tif).\n\n*I am missing a file for band 6, but, this is intentional. Band 6 corresponds to thermal data, which I will not be working with during this exercise.*\n\nTo create a raster stack, I will create a list of the files that I would like to work with and read them all in at once using the terra::rast() function. I will then update the names of the layers to match the spectral bands and plot a true color image to see what we’re working with.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# list files for each band, including the full file path\nfilelist <- list.files(here::here(\"posts\", \"2024-11-8-landcover-classification\", \"data\", \"landsat-data\"), full.names = TRUE)\n\n# read in and store as a raster stack\nlandsat <- rast(filelist)\n\n# update layer names to match band\nnames(landsat) <- c(\"blue\", \"green\", \"red\", \"NIR\", \"SWIR1\", \"SWIR2\")\n\n# plot true color image\nplotRGB(landsat, r = 3, g = 2, b = 1, stretch = \"lin\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n### 3. Load study area\n\nI want to constrain our analysis to the southern portion of the county where we have training data, so I’ll read in a file that defines the area I would like to study.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in shapefile for southern portion of SB county\nSB_county_south <- st_read(here::here(\"posts\", \"2024-11-8-landcover-classification\", \"data\", \"SB_county_south.shp\")) %>%\n      st_transform(SB_county_south, crs = crs(landsat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `SB_county_south' from data source \n  `C:\\MEDS\\jorb1.github.io\\posts\\2024-11-8-landcover-classification\\data\\SB_county_south.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -120.2327 ymin: 34.33603 xmax: -119.5757 ymax: 34.53716\nGeodetic CRS:  NAD83\n```\n\n\n:::\n\n```{.r .cell-code}\n# Plot the shapefile\ntm_shape(SB_county_south) +\n  tm_borders()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n### 4. Crop and mask Landsat data to study area\n\nNow, I can crop and mask the Landsat data to the study area.\n\n- **Why?** This reduces the amount of data we’ll be working with and therefore saves computational time\n- **Bonus:** We can also remove any objects we’re no longer working with to save space\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# crop Landsat scene to the extent of the SB county shapefile\nlandsat_cropped <- crop(landsat, SB_county_south)\n\n# mask the raster to southern portion of SB county\nlandsat_masked <- mask(landsat_cropped, SB_county_south)\n\n# remove unnecessary object from environment\nrm(landsat, SB_county_south, landsat_cropped)\n\n# Plot!\nplotRGB(landsat_masked, r = 3, g = 2, b = 1, stretch = \"lin\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n### 5. Convert Landsat values to reflectance\n\nNow I need to convert the values in our raster stack to correspond to reflectance values. To do so, we need to remove erroneous values and apply any scaling factors to convert to reflectance.\n\nIn this case, I are working with Landsat Collection 2.\n\nThe valid range of pixel values for this collection goes from 7,273 to 43,636…\n- with a multiplicative scale factor of 0.0000275\n- with an additive scale factor of -0.2\n\nLet’s reclassify any erroneous values as NA and update the values for each pixel based on the scaling factors. Now the pixel values should range from 0-100%!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reclassify erroneous values as NA\nrcl <- matrix(c(-Inf, 7273, NA,\n                 43636, Inf, NA), ncol = 3, byrow = TRUE)\n\nlandsat <- classify(landsat_masked, rcl = rcl)\n\n# adjust values based on scaling factor\nlandsat <- (landsat * 0.0000275 - 0.2) * 100\n\n# check values are 0 - 100\nsummary(landsat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      blue           green            red             NIR       \n Min.   : 1.11   Min.   : 0.74   Min.   : 0.00   Min.   : 0.23  \n 1st Qu.: 2.49   1st Qu.: 2.17   1st Qu.: 1.08   1st Qu.: 0.75  \n Median : 3.06   Median : 4.59   Median : 4.45   Median :14.39  \n Mean   : 3.83   Mean   : 5.02   Mean   : 4.92   Mean   :11.52  \n 3rd Qu.: 4.63   3rd Qu.: 6.76   3rd Qu.: 7.40   3rd Qu.:19.34  \n Max.   :39.42   Max.   :53.32   Max.   :56.68   Max.   :57.08  \n NA's   :39856   NA's   :39855   NA's   :39855   NA's   :39856  \n     SWIR1           SWIR2      \n Min.   : 0.10   Min.   : 0.20  \n 1st Qu.: 0.41   1st Qu.: 0.60  \n Median :13.43   Median : 8.15  \n Mean   :11.88   Mean   : 8.52  \n 3rd Qu.:18.70   3rd Qu.:13.07  \n Max.   :49.13   Max.   :48.07  \n NA's   :42892   NA's   :46809  \n```\n\n\n:::\n:::\n\n\n\n\n### 6. Training classifier\n\nLet’s begin by extracting reflectance values for training data!\n\nWe will load the shapefile identifying locations within our study area as containing one of our 4 land cover types.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# read in and transform training data\ntraining_data <- st_read(here::here( \"posts\", \"2024-11-8-landcover-classification\", \"data\", \"trainingdata.shp\")) %>%\n  st_transform(., crs = crs(landsat))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `trainingdata' from data source \n  `C:\\MEDS\\jorb1.github.io\\posts\\2024-11-8-landcover-classification\\data\\trainingdata.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 40 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 215539.2 ymin: 3808948 xmax: 259927.3 ymax: 3823134\nProjected CRS: WGS 84 / UTM zone 11N\n```\n\n\n:::\n:::\n\n\n\n\nNow, we can extract the spectral reflectance values at each site to create a data frame that relates land cover types to their spectral reflectance.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# extract reflectance values at training sites\ntraining_data_values <- terra::extract(landsat, training_data, df = TRUE)\n\n# convert training data to data frame\ntraining_data_attributes <- training_data %>%\n  st_drop_geometry()\n\n# join training data attributes and extracted reflectance values\nSB_training_data <- left_join(training_data_values, training_data_attributes,\n                              by = c(\"ID\" = \"id\")) %>%\n                    mutate(type = as.factor(type)) # convert landcover type to factor\n```\n:::\n\n\n\n\nNext, let’s train the decision tree classifier!\n\nTo train our decision tree, we first need to establish our model formula (i.e. what our response and predictor variables are).\n\n- The rpart() function implements the CART algorithm\n- The rpart() function needs to know the model formula and training data you would like to use\n- Because we are performing a classification, we set method = \"class\"\n- We also set na.action = na.omit to remove any pixels with NAs from the analysis.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Establish model formula\nSB_formula <- type ~ red + green + blue + NIR + SWIR1 + SWIR2\n\n# Train decision tree\nSB_decision_tree <- rpart(formula = SB_formula,\n                          data = SB_training_data, \n                          method = \"class\",\n                          na.action = na.omit)\n```\n:::\n\n\n\n\nTo understand how the decision tree will classify pixels, I can plot the results!\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot decision tree\nprp(SB_decision_tree)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\n\n\n\n### 7. Classify image\n\nNow that I have a rule set for classifying spectral reflectance values into landcover types, I can apply the classifier to identify the landcover type in each pixel.\n\nThe terra package includes a predict() function that allows us to apply a model to our data. In order for this to work properly, the names of the layers need to match the column names of the predictors we used to train our decision tree. The predict() function will return a raster layer with integer values. These integer values correspond to the factor levels in the training data. To figure out what category each integer corresponds to, we can inspect the levels of our training data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# classify image based on decision tree\nSB_classification <- terra::predict(landsat, SB_decision_tree, type = \"class\", na.rm = TRUE)\n\n# inspect level to understand the order of classes in prediction\nlevels(SB_training_data$type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"green_vegetation\" \"soil_dead_grass\"  \"urban\"            \"water\"           \n```\n\n\n:::\n:::\n\n\n\n\n### 8. Plot results\n\nFinally, I can plot the results and check out our land cover map!\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Plot results\ntm_shape(SB_classification) + \n  tm_raster(palette = c(\"#8DB580\", \"#F2DDA4\", \"grey\", \"cornflowerblue\"),\n            labels = c(\"green vegetation\",\n                       \"soil/dead grass\",\n                       \"urban\",\n                       \"water\"),\n            title = \"Land cover type\") +\n  tm_layout(legend.position = c(\"left\", \"bottom\"),\n            main.title = \"Santa Barbara Land Cover\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n\n## Conclusion:\n\nWorking with Landsat data is fun! And it allows us to run analysis regarding landcover types, with the magic of R.\n\n## Acknowledements: \n\nMaterial for this exercise was taken from Ruth Oliver's EDS 223: Geospatial Analysis Course at the University of Santa Barbara's Masters of Environmental Data Science program. Thank you, Ruth! \n\n## Citations:\n\nR. Oliver, EDS 223 - Geospatial Analysis and Remote Sensing, Course Notes. 2024. [Online]. Available: https://eds-223-geospatial.github.io/\n\nU.S. Geological Survey. (n.d.). Landsat 5. Retrieved December 7, 2024, from https://www.usgs.gov/landsat-missions/landsat-5\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}